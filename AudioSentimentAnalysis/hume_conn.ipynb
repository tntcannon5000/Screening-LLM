{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This File is a Test File for Sentiment Analysis using Hume\n",
    "### Requirements for running this file :\n",
    "    1. pip install \"hume[stream]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hume import HumeBatchClient\n",
    "from hume.models.config import LanguageConfig\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job(id=\"4fcc0646-8f2c-4e70-96e4-24a5ef4cf344\")\n",
      "Running...\n",
      "Predictions downloaded to predictions.json\n"
     ]
    }
   ],
   "source": [
    "client = HumeBatchClient(\"8JxAevcGH7SMYv2fa7EFa1ljRgBJftxhjelB77EvTu0wTK8m\")\n",
    "filepaths = [\n",
    "    \"input/RecordingHappy.wav\",\n",
    "]\n",
    "# Accepted values are `word`, `sentence`, `utterance`, or `conversational_turn`.\n",
    "#             The default is `utterance`.\n",
    "#             `utterance` corresponds to a natural pause or break in conversation\n",
    "#             `conversational_turn` corresponds to a change in speaker.\n",
    "config = LanguageConfig(\n",
    "    granularity=\"conversational_turn\",\n",
    "    sentiment={},\n",
    "    toxicity={}\n",
    ")\n",
    "\n",
    "job = client.submit_job(None, [config], files=filepaths)\n",
    "\n",
    "print(job)\n",
    "print(\"Running...\")\n",
    "\n",
    "details = job.await_complete()\n",
    "predictions = job.get_predictions()\n",
    "job.download_predictions(\"predictions.json\")\n",
    "print(\"Predictions downloaded to predictions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through the json file to extract the relevant score sentiment score and the transcription. Sentiment analysis is done word by word. Average for each sentiment is calculated for the entire audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Scores:\n",
      "  Admiration: 0.029698949307203293\n",
      "  Adoration: 0.04302431270480156\n",
      "  Aesthetic Appreciation: 0.01405085064470768\n",
      "  Amusement: 0.046523310244083405\n",
      "  Anger: 0.004918431863188744\n",
      "  Annoyance: 0.003085097298026085\n",
      "  Anxiety: 0.0031014648266136646\n",
      "  Awe: 0.04468310624361038\n",
      "  Awkwardness: 0.0035788188688457012\n",
      "  Boredom: 0.000661490426864475\n",
      "  Calmness: 0.03959153592586517\n",
      "  Concentration: 0.0022455144207924604\n",
      "  Confusion: 0.004444075282663107\n",
      "  Contemplation: 0.010204021818935871\n",
      "  Contempt: 0.021374398842453957\n",
      "  Contentment: 0.2674786150455475\n",
      "  Craving: 0.002267987234517932\n",
      "  Desire: 0.0012277523055672646\n",
      "  Determination: 0.009590454399585724\n",
      "  Disappointment: 0.0007131476886570454\n",
      "  Disapproval: 0.0014754937728866935\n",
      "  Disgust: 0.0011475816136226058\n",
      "  Distress: 0.002752037486061454\n",
      "  Doubt: 0.000799593806732446\n",
      "  Ecstasy: 0.34356117248535156\n",
      "  Embarrassment: 0.000743867305573076\n",
      "  Empathic Pain: 0.002465164987370372\n",
      "  Enthusiasm: 0.3080814778804779\n",
      "  Entrancement: 0.019871320575475693\n",
      "  Envy: 0.0008098236285150051\n",
      "  Excitement: 0.3121441602706909\n",
      "  Fear: 0.0009849846828728914\n",
      "  Gratitude: 0.1208457499742508\n",
      "  Guilt: 0.0005904586869291961\n",
      "  Horror: 0.0005830202717334032\n",
      "  Interest: 0.01758081093430519\n",
      "  Joy: 0.8937432765960693\n",
      "  Love: 0.06204117834568024\n",
      "  Nostalgia: 0.0017802549991756678\n",
      "  Pain: 0.001951383426785469\n",
      "  Pride: 0.06803938001394272\n",
      "  Realization: 0.026851702481508255\n",
      "  Relief: 0.06699783354997635\n",
      "  Romance: 0.005580543074756861\n",
      "  Sadness: 0.001141536282375455\n",
      "  Sarcasm: 0.010973036289215088\n",
      "  Satisfaction: 0.2749807834625244\n",
      "  Shame: 0.0010663117282092571\n",
      "  Surprise (negative): 0.002550811041146517\n",
      "  Surprise (positive): 0.09627598524093628\n",
      "  Sympathy: 0.002557111205533147\n",
      "  Tiredness: 0.0013367693172767758\n",
      "  Triumph: 0.17714570462703705\n",
      "Sentimen Scores:\n",
      "  1: 0.0010329830693081021\n",
      "  2: 0.0010178626980632544\n",
      "  3: 0.0013630579924210906\n",
      "  4: 0.0026854900643229485\n",
      "  5: 0.014838673174381256\n",
      "  6: 0.01902943290770054\n",
      "  7: 0.05759797617793083\n",
      "  8: 0.1589507907629013\n",
      "  9: 0.7589902877807617\n",
      "Toxicity Scores:\n",
      "  identity_hate: 0.003196313977241516\n",
      "  insult: 0.0029195230454206467\n",
      "  obscene: 0.0017942077247425914\n",
      "  severe_toxic: 0.0021684677340090275\n",
      "  threat: 0.0032580411061644554\n",
      "  toxic: 0.006401388440281153\n",
      "\n",
      "Transcription:\n",
      " I am very very very. Very happy. So Happy. So happy very very, very, very happy.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store total scores and counts\n",
    "emotion_totals = defaultdict(float)\n",
    "emotion_counts = defaultdict(int)\n",
    "\n",
    "# Extract and accumulate sentiment scores\n",
    "# for prediction in predictions:\n",
    "#     for result in prediction['results']['predictions']:\n",
    "#         for model in result['models']['language']['grouped_predictions']:\n",
    "#             for pred in model['predictions']:\n",
    "#                 text = pred['text']\n",
    "#                 transcription.append(text)\n",
    "#                 emotions = pred['emotions']\n",
    "#                 for emotion in emotions:\n",
    "#                     name = emotion['name']\n",
    "#                     score = emotion['score']\n",
    "#                     emotion_totals[name] += score\n",
    "#                     emotion_counts[name] += 1\n",
    "\n",
    "\n",
    "# Calculate and print average sentiment scores\n",
    "# print(\"Average Sentiment Scores:\")\n",
    "# for emotion, total_score in emotion_totals.items():\n",
    "#     average_score = total_score / emotion_counts[emotion]\n",
    "#     print(f\"  {emotion}: {average_score:.4f}\")\n",
    "\n",
    "# Just the prediction scores in case of conversational_turn\n",
    "pred = predictions[0]['results']['predictions'][0]['models']['language']['grouped_predictions'][0]['predictions'][0]\n",
    "emotions = pred['emotions']\n",
    "sentiments = pred['sentiment']\n",
    "toxicity_traits = pred['toxicity']\n",
    "transcription = pred['text']\n",
    "\n",
    "# Print emotion names and scores\n",
    "print(\"Emotion Scores:\")\n",
    "for emotion in emotions:\n",
    "    name = emotion['name']\n",
    "    score = emotion['score']\n",
    "    print(f\"  {name}: {score}\")\n",
    "    \n",
    "# Print emotion names and scores\n",
    "#name corresponds to a sentiment level, with 1 being negative and 9 being positive.\n",
    "# Unlike a single sentiment estimate, this distribution-style approach allows for a more nuanced analysis. For example, both a neutral \n",
    "# text and a text with mixed extreme sentiments (very positive and very negative) could have an average sentiment rating of 5. \n",
    "# Therefore, the distribution offers more insight than a single average, and this API returns a value for each sentiment level.\n",
    "print(\"Sentimen Scores:\")\n",
    "for sentiment in sentiments:\n",
    "    name = sentiment['name']\n",
    "    score = sentiment['score']\n",
    "    print(f\"  {name}: {score}\")\n",
    "    \n",
    "# Print emotion names and scores\n",
    "print(\"Toxicity Scores:\")\n",
    "for toxicity in toxicity_traits:\n",
    "    name = toxicity['name']\n",
    "    score = toxicity['score']\n",
    "    print(f\"  {name}: {score}\")\n",
    "\n",
    "#To-Do : Error handling Should be done here. Print within error portion within the json.\n",
    "\n",
    "# Print the transcription\n",
    "print(\"\\nTranscription:\")\n",
    "print(\" \" + transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
