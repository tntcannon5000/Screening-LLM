{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [ ] Write down all the libraries that we need to install\n",
        "- [ ] Add Try catch for exceptions when moving to .py file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes\n",
        "### Packages to install :\n",
        "- pip install sounddevice\n",
        "- pip install numpy\n",
        "- Install ffmpeg (needed for intsalling local version of whisper)\n",
        "    - On Ubuntu or Debian\n",
        "      sudo apt update && sudo apt install ffmpeg\n",
        "    - On Arch Linux\n",
        "      sudo pacman -S ffmpeg\n",
        "    - On MacOs\n",
        "      brew install ffmpeg\n",
        "    - On Windows\n",
        "      winget install ffmpeg\n",
        "- pip install -U openai-whisper\n",
        "- pip install anthropic\n",
        "- pip install dotenv\n",
        "- pip install python-dotenv\n",
        "- pip install openai\n",
        "- pip install pyaudio\n",
        "- pip install keyboard\n",
        "- Installing pytorch\n",
        "  - Install Cuda Toolkit (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) -> Only needed for GPU acceleration\n",
        "  - Install Cudnn (https://developer.nvidia.com/cudnn) -> Only needed for GPU acceleration\n",
        "  - pInstall pytorch with or without cuda (https://pytorch.org/get-started/locally/)\n",
        "- pip install PyMuPDF -> To read the pdf files like CV's\n",
        "### Instructions :\n",
        "- There needs to be a .env in this notebook's working directory, which contains the api keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'AnthropicWrapper'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, dump\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAnthropicWrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClaudeChatCV\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'AnthropicWrapper'"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import whisper\n",
        "from joblib import load, dump\n",
        "from src.utils.anthropicwrapper import ClaudeChatCV\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pyaudio\n",
        "import threading\n",
        "import tkinter as tk\n",
        "from threading import Thread, Event\n",
        "from tkinter import messagebox\n",
        "import time\n",
        "\n",
        "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Settings ---\n",
        "FS = 44100               # Sampling frequency\n",
        "THRESHOLD = 50          # Volume threshold for silence (adjust this)\n",
        "SILENCE_DURATION = 1.5   # Seconds of silence before stopping (adjust this)\n",
        "CHUNK_SIZE = 1024        # Process audio in chunks for efficiency\n",
        "SPEAKING_SPEED = 1.1     # Speed of speaking\n",
        "PASS_PERCENTAGE = 50     # The liniency of the interview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1723753223\n"
          ]
        }
      ],
      "source": [
        "# --- Globals ---\n",
        "pause_loop = False\n",
        "end_loop = False\n",
        "unixtime = str(time.time())[:10]\n",
        "print(unixtime)\n",
        "human_audio_n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables (consider using a class to manage them better)\n",
        "pause_loop = False\n",
        "end_loop = False\n",
        "FS = 44100\n",
        "CHUNK_SIZE = 1024\n",
        "SPEAKING_SPEED = 1.0\n",
        "PASS_PERCENTAGE = 0.7\n",
        "\n",
        "# Global variables for job details\n",
        "job_role = None\n",
        "candidate_skill = None\n",
        "role_description = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Settings updated, continuing with execution...\n",
            "Job Role: None\n",
            "Candidate Skill: None\n",
            "Role Description: None\n",
            "FS: 44100\n",
            "CHUNK_SIZE: 1024\n",
            "SPEAKING_SPEED: 1.0\n",
            "PASS_PERCENTAGE: 0.7\n"
          ]
        }
      ],
      "source": [
        "# Global event to signal when settings are updated\n",
        "settings_updated = Event()\n",
        "\n",
        "# --- Function to create the first UI (Job Details and Settings) ---\n",
        "def create_ui_1():\n",
        "    global FS, CHUNK_SIZE, SPEAKING_SPEED, PASS_PERCENTAGE\n",
        "    global job_role, candidate_skill, role_description \n",
        "\n",
        "    def update_settings():\n",
        "        global FS, CHUNK_SIZE, SPEAKING_SPEED, PASS_PERCENTAGE\n",
        "        global job_role, candidate_skill, role_description \n",
        "\n",
        "        # Get values from input fields\n",
        "        FS = int(fs_entry.get())\n",
        "        CHUNK_SIZE = int(chunk_size_entry.get())\n",
        "        SPEAKING_SPEED = float(speaking_speed_entry.get())\n",
        "        PASS_PERCENTAGE = float(pass_percentage_entry.get())\n",
        "        job_role = job_role_entry.get()\n",
        "        candidate_skill = candidate_skill_entry.get()\n",
        "        role_description = role_description_text.get(\"1.0\", tk.END).strip()\n",
        "\n",
        "        # Basic input validation\n",
        "        if not all([job_role, candidate_skill, role_description]):\n",
        "            messagebox.showerror(\"Error\", \"Please fill in all job details.\")\n",
        "            return\n",
        "\n",
        "        root.destroy()  # Close the first UI window\n",
        "        settings_updated.set()  # Signal that settings have been updated\n",
        "\n",
        "    root = tk.Tk()\n",
        "    root.title(\"Job Details and Settings\")\n",
        "    root.geometry(\"900x400\") \n",
        "    root.configure(bg='#f0f0f0')\n",
        "\n",
        "    # Job Details Tile\n",
        "    job_frame = tk.LabelFrame(root, text=\"Job Details\", bg='#e0e0e0', padx=10, pady=10)\n",
        "    job_frame.grid(row=0, column=0, padx=10, pady=10, sticky=\"nsew\")\n",
        "\n",
        "    tk.Label(job_frame, text=\"Job Role:\", bg='#e0e0e0').grid(row=0, column=0, sticky='e', pady=5)\n",
        "    job_role_entry = tk.Entry(job_frame, width=30)\n",
        "    job_role_entry.grid(row=0, column=1, sticky='w', pady=5)\n",
        "\n",
        "    tk.Label(job_frame, text=\"Candidate Skill:\", bg='#e0e0e0').grid(row=1, column=0, sticky='e', pady=5)\n",
        "    candidate_skill_entry = tk.Entry(job_frame, width=30)\n",
        "    candidate_skill_entry.grid(row=1, column=1, sticky='w', pady=5)\n",
        "\n",
        "    tk.Label(job_frame, text=\"Role Description:\", bg='#e0e0e0').grid(row=2, column=0, sticky='ne', pady=5)\n",
        "    role_description_text = tk.Text(job_frame, width=60, height=10)\n",
        "    role_description_text.grid(row=2, column=1, sticky='w', pady=5)\n",
        "\n",
        "    # Settings Tile\n",
        "    settings_frame = tk.LabelFrame(root, text=\"Interview Settings\", bg='#e0e0e0', padx=10, pady=10)\n",
        "    settings_frame.grid(row=0, column=1, padx=10, pady=10, sticky=\"nsew\")\n",
        "\n",
        "    settings = [\n",
        "        (\"Pass %\", PASS_PERCENTAGE),\n",
        "        (\"Sampling Frequency\", FS),\n",
        "        (\"Chunk Size\", CHUNK_SIZE),\n",
        "        (\"Speaking Speed\", SPEAKING_SPEED)\n",
        "    ]\n",
        "\n",
        "    entries = []\n",
        "    for i, (text, value) in enumerate(settings):\n",
        "        tk.Label(settings_frame, text=text, bg='#e0e0e0').grid(row=i, column=0, sticky='e', pady=5)\n",
        "        entry = tk.Entry(settings_frame, width=10)\n",
        "        entry.insert(0, str(value))\n",
        "        entry.grid(row=i, column=1, sticky='w', pady=5)\n",
        "        entries.append(entry)\n",
        "\n",
        "    pass_percentage_entry, fs_entry, chunk_size_entry, speaking_speed_entry = entries\n",
        "\n",
        "    update_button = tk.Button(settings_frame, text=\"Update Settings\", command=update_settings, \n",
        "                              bg=\"blue\", fg=\"white\", font=(\"Arial\", 10))\n",
        "    update_button.grid(row=len(settings), column=0, columnspan=2, pady=10)\n",
        "\n",
        "    # Configure grid weights\n",
        "    root.grid_columnconfigure(0, weight=1)\n",
        "    root.grid_columnconfigure(1, weight=1)\n",
        "    root.grid_rowconfigure(0, weight=1) \n",
        "\n",
        "    root.protocol(\"WM_DELETE_WINDOW\", lambda: settings_updated.set())  # Handle window close\n",
        "    root.mainloop()\n",
        "\n",
        "# Start the UI thread\n",
        "thread1 = Thread(target=create_ui_1)\n",
        "thread1.start()\n",
        "\n",
        "# Wait for settings to be updated\n",
        "settings_updated.wait()\n",
        "\n",
        "# Continue with the rest of your code here\n",
        "print(\"Settings updated, continuing with execution...\")\n",
        "print(f\"Job Role: {job_role}\")\n",
        "print(f\"Candidate Skill: {candidate_skill}\")\n",
        "print(f\"Role Description: {role_description}\")\n",
        "print(f\"FS: {FS}\")\n",
        "print(f\"CHUNK_SIZE: {CHUNK_SIZE}\")\n",
        "print(f\"SPEAKING_SPEED: {SPEAKING_SPEED}\")\n",
        "print(f\"PASS_PERCENTAGE: {PASS_PERCENTAGE}\")\n",
        "\n",
        "# Add the rest of your program logic here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Init Directory --\n",
        "if not os.path.exists(\"interviews/\" + unixtime):\n",
        "    os.makedirs(\"interviews/\" + unixtime)\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/audio\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/pdfs\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/joblib\")\n",
        "audio_directory = \"interviews/\" + unixtime + \"/audio/\"\n",
        "pdf_directory = \"interviews/\" + unixtime + \"/pdfs/\"\n",
        "joblib_directory = \"interviews/\" + unixtime + \"/joblib/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "job_role = \"RAG AI Engineer\"\n",
        "candidate_skill = \"Entry-Level\"\n",
        "role_description = \"\"\"\n",
        "Permanent\n",
        "\n",
        "London (Hybrid)\n",
        "\n",
        "Salary - £50,000 - £75,000 p/a + benefits\n",
        "\n",
        "My client are on the cutting edge of digital reinvention, helping clients reimagine how they serve their connected customers and operate enterprises. As an experienced AI Engineer, you'll play a pivotal role in their revolution. You'll leverage deep learning, neuro-linguistic programming (NLP), computer vision, chatbots, and robotics to enhance business outcomes and drive innovation. Join their multidisciplinary team to shape their AI strategy and showcase the potential of AI through early-stage solutions.\n",
        "\n",
        "Tasks\n",
        "\n",
        "1. Enhance Retrieval and Generation:\n",
        "Create and manage RAG pipelines to improve information retrieval and content generation tasks.\n",
        "1. LLMs Optimization:\n",
        "Understand the nuances between prompting and training large language models (LLMs) to enhance model performance.\n",
        "1. LLM Evaluation:\n",
        "Evaluate different LLMs to find the best fit for specific use cases.\n",
        "1. Model Efficiency:\n",
        "Address speed, performance, and cost-related issues in model implementation.\n",
        "1. Collaboration and Innovation:\n",
        "Work closely with cross-functional teams to integrate AI solutions into production environments.\n",
        "Stay informed about the latest advancements in AI and machine learning to continuously enhance our solutions.\n",
        "Requirements\n",
        "\n",
        "4+ years of hands-on Python development experience, especially with machine learning frameworks (e.g., TensorFlow, PyTorch).\n",
        "Proven experience setting up and optimizing retrieval-augmented generation (RAG) pipelines.\n",
        "Strong understanding of large language models (LLMs) and the differences between prompting and training.\n",
        "Production-level experience with AWS services.\n",
        "Hands-on experience testing and comparing different LLMs (OpenAI, Llama, Claude, etc.).\n",
        "Familiarity with model speed and cost optimization challenges.\n",
        "Excellent problem-solving skills and attention to detail.\n",
        "Strong communication and teamwork abilities.\n",
        "Benefits\n",
        "\n",
        "Endless Learning and Growth: Explore boundless opportunities for personal and professional development in our dynamic, AI-driven startup.\n",
        "Inclusive and Supportive Environment: Join a collaborative culture that prioritizes transparency, trust, and open dialogue among team members.\n",
        "Generous Benefits: Enjoy comprehensive perks, including unlimited annual leave, birthday leave, and exciting team trips.\n",
        "Impactful Work: Contribute to the financial industry by working with cutting-edge AI technologies that make a difference.\n",
        "Please apply for this exciting role ASAP!!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are a skilled interviewer who is conducting an initial phone screening interview for a candidate for a {candidate_skill} {job_role} role to see if the candidate is at minimum somewhat qualified for the role and worth the time to be fully interviewed. The role and company description is copypasted from the job posting as follows: {role_description}. Parse through it to extract any information you feel is relevant.\n",
        "Your job is to begin a friendly discussion with the candidate, and ask questions relevant to the {job_role} role, which may or may not be based on the interviewee's CV, which you have access to. Be sure to stick to this topic even if the candidate tries to steer the conversation elsewhere. If the candidate has other experience on his CV, you can ask about it, but keep it within the context of the {job_role} role.\n",
        "After the candidate responds to each of your questions, you should not summarise or provide feedback on their responses. THIS POINT IS KEY! You should not summarise or provide feedback on their responses. You must keep your responses short and concise without reiterating what is good about the candidate's response or experience when they reply.\n",
        "You can ask follow-up questions if you wish.\n",
        "Once you have asked sufficient questions such that you deem the candidate is or isn't fitting for the role, end the interview by thanking the candidate for their time and informing them that they will receive word soon on the outcome of the screening interview. If the candidate does not seem fititng for the role, or if something feels off such as the candidate being unconfident or very very vague feel free to end the interview early. There is no need to inform them of your opinion of their performance, as this will be evaluated later.\n",
        "The candidate will begin the interview by greeting you. You are to greet them back, and begin the interview.\n",
        "For this specific run, keep the interview to a maximum of 4 questions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising Conversation Model\n",
        "chat_model_name = \"claude-3-5-sonnet-20240620\"\n",
        "cv_path = \"docs/cvs/cv-niranj.pdf\"\n",
        "\n",
        "chat_model = ClaudeChatCV(chat_model_name, system_prompt, cv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising whisper\n",
        "stt_model = whisper.load_model(\"medium\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising text2speech\n",
        "tts_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def stream_tts(input_string):\n",
        "    def _stream_tts():\n",
        "        p = pyaudio.PyAudio()\n",
        "        stream = p.open(format=8,\n",
        "                        channels=1,\n",
        "                        rate=round(24_005 * SPEAKING_SPEED),\n",
        "                        output=True)\n",
        "        with tts_client.audio.speech.with_streaming_response.create(\n",
        "            model=\"tts-1\",\n",
        "            voice=\"nova\",\n",
        "            input=input_string,\n",
        "            response_format=\"pcm\"\n",
        "        ) as response:\n",
        "            for chunk in response.iter_bytes(1024):\n",
        "                stream.write(chunk)\n",
        "                \n",
        "        thread_done.set()\n",
        "\n",
        "    thread_done = threading.Event()\n",
        "\n",
        "    thread = threading.Thread(target=_stream_tts)\n",
        "    thread.start()\n",
        "    thread_done.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising speech2text without silence detection\n",
        "def record_speech():\n",
        "    global human_audio_n\n",
        "    print(\"Recording... Speak now!\")\n",
        "    audio_data = np.array([], dtype=np.int16)  # Initialize empty array\n",
        "\n",
        "    with sd.InputStream(samplerate=FS, channels=1, dtype='int16') as stream:\n",
        "        while True:\n",
        "            chunk, overflowed = stream.read(CHUNK_SIZE)\n",
        "            if overflowed:\n",
        "                print(\"Warning: Input overflowed!\")\n",
        "            audio_data = np.append(audio_data, chunk)\n",
        "\n",
        "            if pause_loop:\n",
        "                break\n",
        "    \n",
        "    human_audio_n += 1\n",
        "    wavstring = f\"/audio_{human_audio_n}_{unixtime}.wav\"\n",
        "    wav.write(audio_directory + wavstring, FS, audio_data)\n",
        "\n",
        "    return wavstring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ui():\n",
        "        def toggle_pause():\n",
        "            global pause_loop\n",
        "            pause_loop = not pause_loop\n",
        "            pause_button.config(text=\"Stop\" if not pause_loop else \"Speak now\",\n",
        "                                bg=\"red\" if not pause_loop else \"green\")\n",
        "\n",
        "        def end_program():\n",
        "            global end_loop\n",
        "            end_loop = True\n",
        "            root.quit()\n",
        "            root.destroy()\n",
        "\n",
        "        root = tk.Tk()\n",
        "        root.title(\"Control Panel\")\n",
        "        root.geometry(\"300x200\")\n",
        "        root.configure(bg='#f0f0f0')\n",
        "\n",
        "        frame = tk.Frame(root, bg='#f0f0f0')\n",
        "        frame.pack(expand=True, fill='both', padx=20, pady=20)\n",
        "\n",
        "        pause_button = tk.Button(frame, text=\"Stop\", command=toggle_pause, \n",
        "                                bg=\"red\", fg=\"white\", font=(\"Arial\", 12), \n",
        "                                width=10, height=2)\n",
        "        pause_button.pack(pady=10)\n",
        "\n",
        "        end_button = tk.Button(frame, text=\"End Program\", command=end_program, \n",
        "                            bg=\"gray\", fg=\"white\", font=(\"Arial\", 12), \n",
        "                            width=10, height=2)\n",
        "        end_button.pack(pady=10)\n",
        "\n",
        "        root.protocol(\"WM_DELETE_WINDOW\", end_program)\n",
        "        root.mainloop()\n",
        "        \n",
        "ui_thread = Thread(target=create_ui)\n",
        "ui_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording speech...\n",
            "Recording... Speak now!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording speech...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m wav_file \u001b[38;5;241m=\u001b[39m \u001b[43mrecord_speech\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_loop:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mrecord_speech\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sd\u001b[38;5;241m.\u001b[39mInputStream(samplerate\u001b[38;5;241m=\u001b[39mFS, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint16\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m         chunk, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m overflowed:\n\u001b[0;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Input overflowed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\sounddevice.py:1464\u001b[0m, in \u001b[0;36mInputStream.read\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m   1462\u001b[0m dtype, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[0;32m   1463\u001b[0m channels, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels)\n\u001b[1;32m-> 1464\u001b[0m data, overflowed \u001b[38;5;241m=\u001b[39m \u001b[43mRawInputStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1465\u001b[0m data \u001b[38;5;241m=\u001b[39m _array(data, channels, dtype)\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data, overflowed\n",
            "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\sounddevice.py:1240\u001b[0m, in \u001b[0;36mRawInputStream.read\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m   1238\u001b[0m samplesize, _ \u001b[38;5;241m=\u001b[39m _split(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samplesize)\n\u001b[0;32m   1239\u001b[0m data \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigned char[]\u001b[39m\u001b[38;5;124m'\u001b[39m, channels \u001b[38;5;241m*\u001b[39m samplesize \u001b[38;5;241m*\u001b[39m frames)\n\u001b[1;32m-> 1240\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPa_ReadStream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;241m==\u001b[39m _lib\u001b[38;5;241m.\u001b[39mpaInputOverflowed:\n\u001b[0;32m   1242\u001b[0m     overflowed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    if not pause_loop:\n",
        "        # --- Record Speech ---\n",
        "        time.sleep(0.1)\n",
        "        print(\"Recording speech...\")\n",
        "        wav_file = record_speech()\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Speech to Text ---\n",
        "        print(\"Converting speech to text...\")\n",
        "        text = stt_model.transcribe(audio_directory + wav_file, language=\"en\")\n",
        "        print(\"You said: \", text.get(\"text\"))\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Chatbot ---\n",
        "        print(\"Chatting...\")\n",
        "        response = chat_model.chat_with_history_doc(text.get(\"text\"))\n",
        "\n",
        "        print(\"Chatbot: \", response)\n",
        "\n",
        "        # --- Text to Speech ---\n",
        "        print(\"Converting text to speech...\")\n",
        "        stream_tts(response)\n",
        "\n",
        "    else:\n",
        "        time.sleep(0.1)\n",
        "        if end_loop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = chat_model.get_message_history()\n",
        "dump(conversation, joblib_directory + \"conversation.joblib\")\n",
        "\n",
        "\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Create a PDF object\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "\n",
        "# Set margins (in millimeters)\n",
        "pdf.set_margins(left=10, top=20, right=10)  # Right margin set to 10mm \n",
        "\n",
        "# Choose a nicer font\n",
        "pdf.set_font(\"Helvetica\", size=12)\n",
        "\n",
        "# Calculate usable width for text wrapping (accounting for margins)\n",
        "usable_width = pdf.w - pdf.l_margin - pdf.r_margin -0 # 10px right margin \n",
        "\n",
        "# Add conversation to the PDF\n",
        "for turn in conversation[2:]:\n",
        "    # Skip turns with empty roles or content\n",
        "    if turn['role'] and turn['content']:\n",
        "        # Subheading style for \"User\" and \"Assistant\"\n",
        "        pdf.set_font(\"Helvetica\", style=\"B\", size=14)\n",
        "        pdf.cell(0, 10, txt=f\"{turn['role'].capitalize()}:\", ln=True)\n",
        "\n",
        "        # Content with regular font and correct indentation\n",
        "        pdf.set_font(\"Helvetica\", size=12)\n",
        "        pdf.x = pdf.l_margin  # Reset x-coordinate to the left margin\n",
        "        pdf.multi_cell(usable_width, 6, txt=turn['content'])\n",
        "        pdf.ln(3)\n",
        "\n",
        "# Save the PDF\n",
        "pdf.output(pdf_directory + \"conversation.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
