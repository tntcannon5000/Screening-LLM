{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AnthropicWrapper import ClaudeChat, ClaudeChatCV, ClaudeChatHistory\n",
    "from HumeWrapper import HumeSentimentAnalyzer\n",
    "import conversationVerifier\n",
    "from dotenv import load_dotenv\n",
    "from joblib import load\n",
    "import os\n",
    "\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Global Variables --\n",
    "timestamp = 1723670854\n",
    "chatlog = load(f'interviews/{timestamp}/joblib/conversation.joblib') # Loading in the list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_chatlog(chatlog):\n",
    "    dropped_context = chatlog[3:]\n",
    "    outputchatlog = []\n",
    "\n",
    "    for i in range(0, len(dropped_context), 2):\n",
    "        if i + 1 < len(dropped_context):\n",
    "            tempdict = {\n",
    "                'interviewer': dropped_context[i]['content'],\n",
    "                'candidate': dropped_context[i+1]['content']\n",
    "            }\n",
    "\n",
    "            outputchatlog.append(tempdict)\n",
    "        else:\n",
    "            tempdict = {\n",
    "                'interviewer': dropped_context[i]['content'],\n",
    "                'candidate': 'System - The candidate Ended the Interview'  \n",
    "            }\n",
    "            outputchatlog.append(tempdict)\n",
    "            break \n",
    "\n",
    "    return outputchatlog\n",
    "\n",
    "chatlog_chat = reformat_chatlog(chatlog)\n",
    "# for item in chatlog_chat:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the Sentiment Analyser\n",
    "sentiment_analyser = HumeSentimentAnalyzer(api_key=os.getenv(\"HUME_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Sentiment Summariser\n",
    "chat_model_name = \"claude-3-5-sonnet-20240620\"\n",
    "\n",
    "system_prompt = \"\"\"You are a skilled emotions analyst provided with a detailed breakdown of sentiment analysis scores from Hume.ai, for a single response in an interview to a question from the interviewer. The scores are split into 3 sections. All numbers are from 0 to 1, linearly scaling, with 1 being a very strong representation of the indicator in question.\n",
    "\n",
    "First, Emotions. This contains several human emotions with a numerical value indicating the strength of the corresponding emotion.\n",
    "Second, Sentiments. This contains a scale from 1 to 9, each containing a numerical value indicating the magnitude of the sentiment of the topic of the conversation. A negative topic such as murder will have a high value lower in the scale, such as 1 or 2, and a positive topic will have a high value from 0 to 1 higher in the scale such as 8 or 9.\n",
    "Third, Toxicity. This contains several toxic representations such as hate, insult, etc, with a value from 0 to 1 for each representation identified in the audio.\n",
    "\n",
    "Your job is to provide a concise detailed one sentence breakdown of how the individual was feeling for the particular scores provided. You must be highly objective as your job is to discern whether or not a candidate was exhibiting traits which would or would not be fitting for a successful interview. \n",
    "Model your answer beginning with something along the lines of \"For this particular response, the candidate...\".\n",
    "\"\"\"\n",
    "\n",
    "sentiment_summariser = ClaudeChat(chat_model_name, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Evaluation LLM\n",
    "chat_model_name = \"claude-3-5-sonnet-20240620\"\n",
    "system_prompt = \"You are a highly skilled .\"\n",
    "\n",
    "candidate_evaluator = ClaudeChatHistory(chat_model_name, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 1\n",
      "Breakpoint 2\n",
      "Breakpoint 3\n",
      "Breakpoint 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'interviewer': \"Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'm looking forward to learning more about your experience and skills. To start off, could you tell me about any experience you have with setting up and optimizing retrieval-augmented generation (RAG) pipelines?\",\n",
       "  'candidate': \" I'm sure. So I'm currently working on my final year dissertation which is to build a LLM screening system that will screen candidates and assess whether they are fit to move on to the next round. So to achieve this one of the steps involved checking the accuracy of the candidates answer. To check the accuracy of the answer what I basically did was set up a rag pipeline. That pipeline involved getting each, getting the transcript of each question and answer and then dividing the answer into searchable queries using an LLM, using those searchable queries to web script Google links and put it in the context window of a vector store. Now I created another rag chain which analyzes the accuracy of each answer keeping the training data of the LLM that was already there and the web script data that was in the vector store that it searches the most relevant information from the vector store and doesn't get fetch everything from the vector store and puts it in the context of the LLM and then it judges the accuracy of the answer. Now that we have the accuracy of the answer and we can assess the how right or wrong the candidates per answer. This is just one part of the sentiment analysis. I can go on with the entire sentiment analysis if you want later on.\",\n",
       "  'feedback': 'The accuracy of the answer is 85%.\\n\\nThe candidate has a good understanding of the RAG pipeline and its application in their project. They correctly explained how they used the pipeline to assess the accuracy of candidates\\' answers in their LLM screening system. They also correctly mentioned the use of web scraping to gather data for the context window of a vector store, which is a key part of the RAG pipeline.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that they created another RAG chain to analyze the accuracy of each answer. This is not entirely accurate. In a RAG pipeline, there is typically only one RAG model that is used to generate responses based on the context provided. It\\'s not clear what the candidate means by a \"RAG chain\".\\n\\n2. The candidate mentioned that the RAG model searches the most relevant information from the vector store and doesn\\'t fetch everything from the vector store. This is not entirely accurate. In a RAG pipeline, the retrieval model is used to fetch relevant documents from the document store (not a vector store) based on the query. The generator model then uses these documents to generate a response.\\n\\nFeedback:\\n\\n1. Please clarify what you mean by a \"RAG chain\". In a typical RAG pipeline, there is only one RAG model that is used to generate responses.\\n\\n2. Please ensure to use the correct terminology when describing the RAG pipeline. The retrieval model fetches relevant documents from a document store (not a vector store) based on the query.'},\n",
       " {'interviewer': \"Thank you for sharing that experience. It's certainly relevant to the role. Could you elaborate on your experience with different large language models? Have you worked with or compared various LLMs like OpenAI, Llama, or Claude in your projects?\",\n",
       "  'candidate': \" Yes, sure. So in my current project with the Screening LLM, I'm currently using Claude and OpenAI. What I found is Claude tends to be more accurate and comes to a more descriptive answer with a single prompt rather than OpenAI. An example would be, so if I tell Claude to write a bare bones frontend for a UI, Claude can do it in a single go, whereas OpenAI struggles to do it in a single go and sometimes have to give multiple prompts. So Claude is better at understanding code, better than OpenAI. However, OpenAI does have a mini model called GPT-4.0 mini that uses less, that I think uses less parameters than the normal GPT-4.0, so it's quicker. So I can also some of the tasks to GPT-4.0 when I require real-time analysis. An example of this would be if I had required, if I required that each answer from the candidate would have to be processed in real time rather than post the interview, I would use GPT-4.0 in that case.\",\n",
       "  'feedback': \"The accuracy of the answer is 90%.\\n\\nThe candidate has a good understanding of the differences between the Claude and OpenAI models. They correctly explained how Claude tends to be more accurate and can generate more descriptive answers with a single prompt compared to OpenAI. They also correctly mentioned that OpenAI's GPT-4.0 mini model uses fewer parameters than the normal GPT-4.0, making it quicker and more suitable for tasks requiring real-time analysis.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that Claude is better at understanding code than OpenAI. While Claude may be more accurate in some cases, it's not accurate to say that it's universally better at understanding code. The performance of these models can vary depending on the specific task and the way they're used.\\n\\n2. The candidate mentioned that they use GPT-4.0 for real-time analysis. While it's true that GPT-4.0 can be used for real-time analysis, it's important to note that this would require significant computational resources. The candidate didn't mention this, which could give the impression that GPT-4.0 is easily used for real-time analysis in any situation.\\n\\nFeedback:\\n\\n1. Please be careful when making broad statements about the performance of different models. While Claude may perform better in some cases, it's not accurate to say that it's universally better at understanding code than OpenAI.\\n\\n2. When discussing the use of models for real-time analysis, it's important to mention the computational resources required. This can help give a more accurate picture of the practical considerations involved in using these models.\"},\n",
       " {'interviewer': \"That's an interesting comparison of Claude and OpenAI models. You've clearly gained some practical insights. Now, could you tell me about your experience with AWS services, particularly in a production environment?\",\n",
       "  'candidate': \" Yes, so for AWS, I have previously deployed applications when I was working for AMEX into AWS, and also in private servers. So for AWS, some of the experiences I have in deploying are generating a Docker file that can be uploaded directly to an AWS server. So whenever we require anything for the project, it can be downloaded specifically from the company's repository in the AWS server. Also, we can deploy entire applications into AWS servers, and depending on the load balancing settings, it can be upscaled as and when traffic to the web application increases or decreases. This is done automatically through configuration parameters. I am yet to learn all the configuration parameters. That is the duty of Debugs, but I am keen to learn more on that topic.\",\n",
       "  'feedback': \"The accuracy of the answer is 95%.\\n\\nThe candidate has a good understanding of AWS services and their application in a production environment. They correctly explained how they have used AWS for deploying applications, generating Docker files, and managing load balancing. They also correctly mentioned that they have worked with AWS in a previous role at AMEX.\\n\\nHowever, there is a minor inaccuracy in the answer:\\n\\n1. The candidate mentioned that they are yet to learn all the configuration parameters and that this is the duty of Debugs. While it's true that understanding all configuration parameters can be complex and typically falls under the responsibilities of a DevOps engineer, it's important for all team members to have a basic understanding of these parameters to ensure smooth deployment and operation of applications.\\n\\nFeedback:\\n\\n1. While it's great that you're keen to learn more about AWS configuration parameters, it's important to note that having a basic understanding of these parameters can be beneficial for all team members, not just DevOps engineers. This can help ensure smooth deployment and operation of applications.\"},\n",
       " {'interviewer': \"Thank you for sharing your experience with AWS. Lastly, could you describe a challenging problem you've encountered in your AI or machine learning projects and how you approached solving it?\",\n",
       "  'candidate': \" I'm sure so we had a group project for developing Sorry, we had a group project for beating a game called jumping using neat which is neural evolution of augmented topologies So what it does is basically? Start with a few basic generations of neural networks that are randomly generated and it sees the neural networks that perform best for solving the task and those neural networks reproduced among themselves to produce the next generation so basically that the best genes of the best from each of the parents are Move forward in the next generation as well as the top performing network in the first generation and this continues on and on and on until We get the most optimum solution also I forgot to mention that we have a fitness function defined for this Which calculates how good a neural network performs so the better the neural network performs the more Chances of it reaching the next generation or the more chances of it More chances of it Producing with another neural network to move on to the next generation if that makes sense So we had to we had used this to beat the game jumping One of the challenges was to come up with a fitness function that is how the Individual king in the game would perceive the environment around it so it needs to jump to the highest level possible right so one method of the one method of getting the fitness function was to calculate the height that the King that the Kings reached at the end of each level however we had to feed in the Input to the neural network in such a way that it recognized the geometry of the game that was pretty hard to overcome We finally came up with a solution of distance from the The shortest distance from the King that is the shortest diagonal distance the shortest vertical distance the shortest horizontal distance And we also rewarded it for reaching a higher height and also penalized it if It fell down from a certain platform Yes, so that is how we were working the challenges and the After Incorporating the network into incorporating no incorporating is not the term I'm looking for after Feeding the Inputs in the form of the XY coordinates of the platform it was able to significantly Increase the performance of the network by increase the performance. I mean it was able to Increase the fitness function generation after generation until the King reached the final stage\",\n",
       "  'feedback': 'The accuracy of the answer is 80%.\\n\\nThe candidate has a good understanding of the NEAT (NeuroEvolution of Augmenting Topologies) algorithm and its application in their project. They correctly explained how they used NEAT to evolve neural networks for a game, and how they defined a fitness function to evaluate the performance of these networks. They also correctly mentioned the concept of generations in NEAT, where the best-performing networks are selected to reproduce and create the next generation.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that they used NEAT to \"beat the game jumping\". However, they did not provide any details about the game or how they specifically applied NEAT to solve it. This makes it difficult to assess the accuracy of their application of NEAT.\\n\\n2. The candidate mentioned that they fed the inputs to the neural network in the form of the XY coordinates of the platform. While this is a valid approach, they did not explain how they used these inputs to train the neural network or how they related to the fitness function.\\n\\nFeedback:\\n\\n1. Please provide more details about the specific game or problem you were trying to solve with NEAT. This will help to better understand your application of the algorithm.\\n\\n2. Please explain how you used the inputs (XY coordinates of the platform) to train the neural network and how they related to the fitness function. This will help to better understand your approach to solving the problem.'},\n",
       " {'interviewer': \"Thank you for sharing that detailed example. It's clear you have experience with complex AI problems and innovative approaches. Thank you for your time today. We'll be in touch soon regarding the next steps in the interview process.\",\n",
       "  'candidate': 'System - The candidate Ended the Interview',\n",
       "  'feedback': \"The accuracy of the answer is 100%.\\n\\nThe answer accurately reflects the conclusion of the interview. The interviewer thanks the candidate for their time and sharing their experiences, and informs them that they will be in touch regarding the next steps in the interview process. The candidate's response indicates that the interview has ended. There are no inaccuracies in the answer.\\n\\nFeedback: None. The answer is accurate.\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversationVerifier.process_qa_pair(chatlog_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Analysing the post-conversation to make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interviews/1723670854/audio\\audio_1_1723670854.wav\n",
      "Analyzing audio...\n",
      "interviews/1723670854/audio\\audio_2_1723670854.wav\n",
      "Analyzing audio...\n",
      "interviews/1723670854/audio\\audio_3_1723670854.wav\n",
      "Analyzing audio...\n",
      "interviews/1723670854/audio\\audio_4_1723670854.wav\n",
      "Analyzing audio...\n",
      "interviews/1723670854/audio\\audio_5_1723670854.wav\n",
      "Analyzing audio...\n"
     ]
    }
   ],
   "source": [
    "# Specify the filepath\n",
    "filepath = f'interviews/{timestamp}/audio'\n",
    "\n",
    "# Get all files in the filepath\n",
    "files = [f for f in os.listdir(filepath) if os.path.isfile(os.path.join(filepath, f))]\n",
    "\n",
    "# Loop through each file\n",
    "sentiments = []\n",
    "for count, file in enumerate(files, 1):\n",
    "    # Print the file name\n",
    "    print(os.path.join(filepath, str(file)))\n",
    "    result = sentiment_analyser.analyze_audio(os.path.join(filepath, str(file)))\n",
    "    sentiment_summary = sentiment_summariser.chat(str(result))\n",
    "    sentiments.append((result, sentiment_summary))\n",
    "    chatlog_chat[count-1]['sentiment'] = sentiment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'interviewer': \"Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'm looking forward to learning more about your experience and skills. To start off, could you tell me about any experience you have with setting up and optimizing retrieval-augmented generation (RAG) pipelines?\",\n",
       "  'candidate': \" I'm sure. So I'm currently working on my final year dissertation which is to build a LLM screening system that will screen candidates and assess whether they are fit to move on to the next round. So to achieve this one of the steps involved checking the accuracy of the candidates answer. To check the accuracy of the answer what I basically did was set up a rag pipeline. That pipeline involved getting each, getting the transcript of each question and answer and then dividing the answer into searchable queries using an LLM, using those searchable queries to web script Google links and put it in the context window of a vector store. Now I created another rag chain which analyzes the accuracy of each answer keeping the training data of the LLM that was already there and the web script data that was in the vector store that it searches the most relevant information from the vector store and doesn't get fetch everything from the vector store and puts it in the context of the LLM and then it judges the accuracy of the answer. Now that we have the accuracy of the answer and we can assess the how right or wrong the candidates per answer. This is just one part of the sentiment analysis. I can go on with the entire sentiment analysis if you want later on.\",\n",
       "  'feedback': 'The accuracy of the answer is 85%.\\n\\nThe candidate has a good understanding of the RAG pipeline and its application in their project. They correctly explained how they used the pipeline to assess the accuracy of candidates\\' answers in their LLM screening system. They also correctly mentioned the use of web scraping to gather data for the context window of a vector store, which is a key part of the RAG pipeline.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that they created another RAG chain to analyze the accuracy of each answer. This is not entirely accurate. In a RAG pipeline, there is typically only one RAG model that is used to generate responses based on the context provided. It\\'s not clear what the candidate means by a \"RAG chain\".\\n\\n2. The candidate mentioned that the RAG model searches the most relevant information from the vector store and doesn\\'t fetch everything from the vector store. This is not entirely accurate. In a RAG pipeline, the retrieval model is used to fetch relevant documents from the document store (not a vector store) based on the query. The generator model then uses these documents to generate a response.\\n\\nFeedback:\\n\\n1. Please clarify what you mean by a \"RAG chain\". In a typical RAG pipeline, there is only one RAG model that is used to generate responses.\\n\\n2. Please ensure to use the correct terminology when describing the RAG pipeline. The retrieval model fetches relevant documents from a document store (not a vector store) based on the query.',\n",
       "  'sentiment': 'For this particular response, the candidate exhibited a high level of interest (0.51) and confusion (0.33), along with notable levels of awkwardness (0.16) and doubt (0.14), suggesting they were engaged but struggling to fully comprehend or articulate their thoughts, which could indicate a lack of preparation or nervousness during the interview.'},\n",
       " {'interviewer': \"Thank you for sharing that experience. It's certainly relevant to the role. Could you elaborate on your experience with different large language models? Have you worked with or compared various LLMs like OpenAI, Llama, or Claude in your projects?\",\n",
       "  'candidate': \" Yes, sure. So in my current project with the Screening LLM, I'm currently using Claude and OpenAI. What I found is Claude tends to be more accurate and comes to a more descriptive answer with a single prompt rather than OpenAI. An example would be, so if I tell Claude to write a bare bones frontend for a UI, Claude can do it in a single go, whereas OpenAI struggles to do it in a single go and sometimes have to give multiple prompts. So Claude is better at understanding code, better than OpenAI. However, OpenAI does have a mini model called GPT-4.0 mini that uses less, that I think uses less parameters than the normal GPT-4.0, so it's quicker. So I can also some of the tasks to GPT-4.0 when I require real-time analysis. An example of this would be if I had required, if I required that each answer from the candidate would have to be processed in real time rather than post the interview, I would use GPT-4.0 in that case.\",\n",
       "  'feedback': \"The accuracy of the answer is 90%.\\n\\nThe candidate has a good understanding of the differences between the Claude and OpenAI models. They correctly explained how Claude tends to be more accurate and can generate more descriptive answers with a single prompt compared to OpenAI. They also correctly mentioned that OpenAI's GPT-4.0 mini model uses fewer parameters than the normal GPT-4.0, making it quicker and more suitable for tasks requiring real-time analysis.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that Claude is better at understanding code than OpenAI. While Claude may be more accurate in some cases, it's not accurate to say that it's universally better at understanding code. The performance of these models can vary depending on the specific task and the way they're used.\\n\\n2. The candidate mentioned that they use GPT-4.0 for real-time analysis. While it's true that GPT-4.0 can be used for real-time analysis, it's important to note that this would require significant computational resources. The candidate didn't mention this, which could give the impression that GPT-4.0 is easily used for real-time analysis in any situation.\\n\\nFeedback:\\n\\n1. Please be careful when making broad statements about the performance of different models. While Claude may perform better in some cases, it's not accurate to say that it's universally better at understanding code than OpenAI.\\n\\n2. When discussing the use of models for real-time analysis, it's important to mention the computational resources required. This can help give a more accurate picture of the practical considerations involved in using these models.\",\n",
       "  'sentiment': 'For this particular response, the candidate exhibited a high level of concentration and contemplation, with strong determination and interest, suggesting they were deeply engaged in the interview process and giving thoughtful, focused answers while maintaining a calm and composed demeanor.'},\n",
       " {'interviewer': \"That's an interesting comparison of Claude and OpenAI models. You've clearly gained some practical insights. Now, could you tell me about your experience with AWS services, particularly in a production environment?\",\n",
       "  'candidate': \" Yes, so for AWS, I have previously deployed applications when I was working for AMEX into AWS, and also in private servers. So for AWS, some of the experiences I have in deploying are generating a Docker file that can be uploaded directly to an AWS server. So whenever we require anything for the project, it can be downloaded specifically from the company's repository in the AWS server. Also, we can deploy entire applications into AWS servers, and depending on the load balancing settings, it can be upscaled as and when traffic to the web application increases or decreases. This is done automatically through configuration parameters. I am yet to learn all the configuration parameters. That is the duty of Debugs, but I am keen to learn more on that topic.\",\n",
       "  'feedback': \"The accuracy of the answer is 95%.\\n\\nThe candidate has a good understanding of AWS services and their application in a production environment. They correctly explained how they have used AWS for deploying applications, generating Docker files, and managing load balancing. They also correctly mentioned that they have worked with AWS in a previous role at AMEX.\\n\\nHowever, there is a minor inaccuracy in the answer:\\n\\n1. The candidate mentioned that they are yet to learn all the configuration parameters and that this is the duty of Debugs. While it's true that understanding all configuration parameters can be complex and typically falls under the responsibilities of a DevOps engineer, it's important for all team members to have a basic understanding of these parameters to ensure smooth deployment and operation of applications.\\n\\nFeedback:\\n\\n1. While it's great that you're keen to learn more about AWS configuration parameters, it's important to note that having a basic understanding of these parameters can be beneficial for all team members, not just DevOps engineers. This can help ensure smooth deployment and operation of applications.\",\n",
       "  'sentiment': 'For this particular response, the candidate exhibited a very high level of concentration (0.92) and contemplation (0.34), with moderate levels of calmness (0.30) and interest (0.25), suggesting they were deeply focused and engaged in the conversation while maintaining a composed demeanor, which are generally positive traits in an interview setting.'},\n",
       " {'interviewer': \"Thank you for sharing your experience with AWS. Lastly, could you describe a challenging problem you've encountered in your AI or machine learning projects and how you approached solving it?\",\n",
       "  'candidate': \" I'm sure so we had a group project for developing Sorry, we had a group project for beating a game called jumping using neat which is neural evolution of augmented topologies So what it does is basically? Start with a few basic generations of neural networks that are randomly generated and it sees the neural networks that perform best for solving the task and those neural networks reproduced among themselves to produce the next generation so basically that the best genes of the best from each of the parents are Move forward in the next generation as well as the top performing network in the first generation and this continues on and on and on until We get the most optimum solution also I forgot to mention that we have a fitness function defined for this Which calculates how good a neural network performs so the better the neural network performs the more Chances of it reaching the next generation or the more chances of it More chances of it Producing with another neural network to move on to the next generation if that makes sense So we had to we had used this to beat the game jumping One of the challenges was to come up with a fitness function that is how the Individual king in the game would perceive the environment around it so it needs to jump to the highest level possible right so one method of the one method of getting the fitness function was to calculate the height that the King that the Kings reached at the end of each level however we had to feed in the Input to the neural network in such a way that it recognized the geometry of the game that was pretty hard to overcome We finally came up with a solution of distance from the The shortest distance from the King that is the shortest diagonal distance the shortest vertical distance the shortest horizontal distance And we also rewarded it for reaching a higher height and also penalized it if It fell down from a certain platform Yes, so that is how we were working the challenges and the After Incorporating the network into incorporating no incorporating is not the term I'm looking for after Feeding the Inputs in the form of the XY coordinates of the platform it was able to significantly Increase the performance of the network by increase the performance. I mean it was able to Increase the fitness function generation after generation until the King reached the final stage\",\n",
       "  'feedback': 'The accuracy of the answer is 80%.\\n\\nThe candidate has a good understanding of the NEAT (NeuroEvolution of Augmenting Topologies) algorithm and its application in their project. They correctly explained how they used NEAT to evolve neural networks for a game, and how they defined a fitness function to evaluate the performance of these networks. They also correctly mentioned the concept of generations in NEAT, where the best-performing networks are selected to reproduce and create the next generation.\\n\\nHowever, there are a few inaccuracies in the answer:\\n\\n1. The candidate mentioned that they used NEAT to \"beat the game jumping\". However, they did not provide any details about the game or how they specifically applied NEAT to solve it. This makes it difficult to assess the accuracy of their application of NEAT.\\n\\n2. The candidate mentioned that they fed the inputs to the neural network in the form of the XY coordinates of the platform. While this is a valid approach, they did not explain how they used these inputs to train the neural network or how they related to the fitness function.\\n\\nFeedback:\\n\\n1. Please provide more details about the specific game or problem you were trying to solve with NEAT. This will help to better understand your application of the algorithm.\\n\\n2. Please explain how you used the inputs (XY coordinates of the platform) to train the neural network and how they related to the fitness function. This will help to better understand your approach to solving the problem.',\n",
       "  'sentiment': 'For this particular response, the candidate exhibited a high level of interest (0.70) and concentration (0.68), coupled with strong determination (0.58) and enthusiasm (0.33), suggesting they were deeply engaged and motivated during this part of the interview, while maintaining a generally positive and professional demeanor as indicated by the low toxicity scores and the balanced sentiment ratings centered around the neutral to slightly positive range.'},\n",
       " {'interviewer': \"Thank you for sharing that detailed example. It's clear you have experience with complex AI problems and innovative approaches. Thank you for your time today. We'll be in touch soon regarding the next steps in the interview process.\",\n",
       "  'candidate': 'System - The candidate Ended the Interview',\n",
       "  'feedback': \"The accuracy of the answer is 100%.\\n\\nThe answer accurately reflects the conclusion of the interview. The interviewer thanks the candidate for their time and sharing their experiences, and informs them that they will be in touch regarding the next steps in the interview process. The candidate's response indicates that the interview has ended. There are no inaccuracies in the answer.\\n\\nFeedback: None. The answer is accurate.\",\n",
       "  'sentiment': 'For this particular response, the candidate exhibited a high level of concentration (0.399) and satisfaction (0.382), coupled with notable levels of calmness (0.198), interest (0.209), and determination (0.184), suggesting a focused and positive demeanor that would be well-suited for a successful interview.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatlog_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
