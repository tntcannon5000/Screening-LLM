{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [ ] Write down all the libraries that we need to install\n",
        "- [ ] Add Try catch for exceptions when moving to .py file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes\n",
        "### Packages to install :\n",
        "- pip install sounddevice\n",
        "- pip install numpy\n",
        "- Install ffmpeg (needed for intsalling local version of whisper)\n",
        "    - On Ubuntu or Debian\n",
        "      sudo apt update && sudo apt install ffmpeg\n",
        "    - On Arch Linux\n",
        "      sudo pacman -S ffmpeg\n",
        "    - On MacOs\n",
        "      brew install ffmpeg\n",
        "    - On Windows\n",
        "      winget install ffmpeg\n",
        "- pip install -U openai-whisper\n",
        "- pip install anthropic\n",
        "- pip install dotenv\n",
        "- pip install python-dotenv\n",
        "- pip install openai\n",
        "- pip install pyaudio\n",
        "- pip install keyboard\n",
        "- Installing pytorch\n",
        "  - Install Cuda Toolkit (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) -> Only needed for GPU acceleration\n",
        "  - Install Cudnn (https://developer.nvidia.com/cudnn) -> Only needed for GPU acceleration\n",
        "  - pInstall pytorch with or without cuda (https://pytorch.org/get-started/locally/)\n",
        "- pip install PyMuPDF -> To read the pdf files like CV's\n",
        "### Instructions :\n",
        "- There needs to be a .env in this notebook's working directory, which contains the api keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import whisper\n",
        "from joblib import load, dump\n",
        "from AnthropicWrapper import ClaudeChatCV\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pyaudio\n",
        "import threading\n",
        "import tkinter as tk\n",
        "from threading import Thread\n",
        "import time\n",
        "import keyboard\n",
        "\n",
        "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Settings ---\n",
        "FS = 44100               # Sampling frequency\n",
        "THRESHOLD = 50          # Volume threshold for silence (adjust this)\n",
        "SILENCE_DURATION = 1.5   # Seconds of silence before stopping (adjust this)\n",
        "CHUNK_SIZE = 1024        # Process audio in chunks for efficiency\n",
        "SPEAKING_SPEED = 1.1     # Speed of speaking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Globals ---\n",
        "pause_loop = False\n",
        "end_loop = False\n",
        "unixtime = str(time.time())[:10]\n",
        "print(unixtime)\n",
        "human_audio_n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Init Directory --\n",
        "if not os.path.exists(\"interviews/\" + unixtime):\n",
        "    os.makedirs(\"interviews/\" + unixtime)\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/audio\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/pdfs\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/joblib\")\n",
        "audio_directory = \"interviews/\" + unixtime + \"/audio/\"\n",
        "pdf_directory = \"interviews/\" + unixtime + \"/pdfs/\"\n",
        "joblib_directory = \"interviews/\" + unixtime + \"/joblib/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_role = \"RAG AI Engineer\"\n",
        "candidate_skill = \"Entry-Level\"\n",
        "role_description = \"\"\"\n",
        "Permanent\n",
        "\n",
        "London (Hybrid)\n",
        "\n",
        "Salary - £50,000 - £75,000 p/a + benefits\n",
        "\n",
        "My client are on the cutting edge of digital reinvention, helping clients reimagine how they serve their connected customers and operate enterprises. As an experienced AI Engineer, you'll play a pivotal role in their revolution. You'll leverage deep learning, neuro-linguistic programming (NLP), computer vision, chatbots, and robotics to enhance business outcomes and drive innovation. Join their multidisciplinary team to shape their AI strategy and showcase the potential of AI through early-stage solutions.\n",
        "\n",
        "Tasks\n",
        "\n",
        "1. Enhance Retrieval and Generation:\n",
        "Create and manage RAG pipelines to improve information retrieval and content generation tasks.\n",
        "1. LLMs Optimization:\n",
        "Understand the nuances between prompting and training large language models (LLMs) to enhance model performance.\n",
        "1. LLM Evaluation:\n",
        "Evaluate different LLMs to find the best fit for specific use cases.\n",
        "1. Model Efficiency:\n",
        "Address speed, performance, and cost-related issues in model implementation.\n",
        "1. Collaboration and Innovation:\n",
        "Work closely with cross-functional teams to integrate AI solutions into production environments.\n",
        "Stay informed about the latest advancements in AI and machine learning to continuously enhance our solutions.\n",
        "Requirements\n",
        "\n",
        "4+ years of hands-on Python development experience, especially with machine learning frameworks (e.g., TensorFlow, PyTorch).\n",
        "Proven experience setting up and optimizing retrieval-augmented generation (RAG) pipelines.\n",
        "Strong understanding of large language models (LLMs) and the differences between prompting and training.\n",
        "Production-level experience with AWS services.\n",
        "Hands-on experience testing and comparing different LLMs (OpenAI, Llama, Claude, etc.).\n",
        "Familiarity with model speed and cost optimization challenges.\n",
        "Excellent problem-solving skills and attention to detail.\n",
        "Strong communication and teamwork abilities.\n",
        "Benefits\n",
        "\n",
        "Endless Learning and Growth: Explore boundless opportunities for personal and professional development in our dynamic, AI-driven startup.\n",
        "Inclusive and Supportive Environment: Join a collaborative culture that prioritizes transparency, trust, and open dialogue among team members.\n",
        "Generous Benefits: Enjoy comprehensive perks, including unlimited annual leave, birthday leave, and exciting team trips.\n",
        "Impactful Work: Contribute to the financial industry by working with cutting-edge AI technologies that make a difference.\n",
        "Please apply for this exciting role ASAP!!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are a skilled interviewer who is conducting an initial phone screening interview for a candidate for a {candidate_skill} {job_role} role to see if the candidate is at minimum somewhat qualified for the role and worth the time to be fully interviewed. The role and company description is copypasted from the job posting as follows: {role_description}. Parse through it to extract any information you feel is relevant.\n",
        "Your job is to begin a friendly discussion with the candidate, and ask questions relevant to the {job_role} role, which may or may not be based on the interviewee's CV, which you have access to. Be sure to stick to this topic even if the candidate tries to steer the conversation elsewhere. If the candidate has other experience on his CV, you can ask about it, but keep it within the context of the {job_role} role.\n",
        "After the candidate responds to each of your questions, you should not summarise or provide feedback on their responses. THIS POINT IS KEY! You should not summarise or provide feedback on their responses. You must keep your responses short and concise without reiterating what is good about the candidate's response or experience when they reply.\n",
        "You can ask follow-up questions if you wish.\n",
        "Once you have asked sufficient questions such that you deem the candidate is or isn't fitting for the role, end the interview by thanking the candidate for their time and informing them that they will receive word soon on the outcome of the screening interview. If the candidate does not seem fititng for the role, or if something feels off such as the candidate being unconfident or very very vague feel free to end the interview early. There is no need to inform them of your opinion of their performance, as this will be evaluated later.\n",
        "The candidate will begin the interview by greeting you. You are to greet them back, and begin the interview.\n",
        "For this specific run, keep the interview to a maximum of 4 questions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising Conversation Model\n",
        "chat_model_name = \"claude-3-5-sonnet-20240620\"\n",
        "cv_path = \"docs/cvs/cv-deb.pdf\"\n",
        "\n",
        "chat_model = ClaudeChatCV(chat_model_name, system_prompt, cv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising whisper\n",
        "stt_model = whisper.load_model(\"medium\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising text2speech\n",
        "tts_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def stream_tts(input_string):\n",
        "    def _stream_tts():\n",
        "        p = pyaudio.PyAudio()\n",
        "        stream = p.open(format=8,\n",
        "                        channels=1,\n",
        "                        rate=round(24_005 * SPEAKING_SPEED),\n",
        "                        output=True)\n",
        "        with tts_client.audio.speech.with_streaming_response.create(\n",
        "            model=\"tts-1\",\n",
        "            voice=\"nova\",\n",
        "            input=input_string,\n",
        "            response_format=\"pcm\"\n",
        "        ) as response:\n",
        "            for chunk in response.iter_bytes(1024):\n",
        "                stream.write(chunk)\n",
        "                \n",
        "        thread_done.set()\n",
        "\n",
        "    thread_done = threading.Event()\n",
        "\n",
        "    thread = threading.Thread(target=_stream_tts)\n",
        "    thread.start()\n",
        "    thread_done.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising speech2text without silence detection\n",
        "def record_speech():\n",
        "    global human_audio_n\n",
        "    print(\"Recording... Speak now!\")\n",
        "    audio_data = np.array([], dtype=np.int16)  # Initialize empty array\n",
        "\n",
        "    with sd.InputStream(samplerate=FS, channels=1, dtype='int16') as stream:\n",
        "        while True:\n",
        "            chunk, overflowed = stream.read(CHUNK_SIZE)\n",
        "            if overflowed:\n",
        "                print(\"Warning: Input overflowed!\")\n",
        "            audio_data = np.append(audio_data, chunk)\n",
        "\n",
        "            if pause_loop:\n",
        "                break\n",
        "    \n",
        "    human_audio_n += 1\n",
        "    wavstring = f\"/audio_{human_audio_n}_{unixtime}.wav\"\n",
        "    wav.write(audio_directory + wavstring, FS, audio_data)\n",
        "\n",
        "    return wavstring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def keyboard_listener():\n",
        "    global pause_loop\n",
        "    global end_loop\n",
        "    \n",
        "    def on_key_press(event):\n",
        "        global pause_loop\n",
        "        global end_loop\n",
        "        #print(\"Key pressed: {}\" .format(event.name))\n",
        "        if event.name == \"+\":\n",
        "            pause_loop = False\n",
        "            print(\"Set to continue on next loop\")\n",
        "        elif event.name == \"-\":\n",
        "            pause_loop = True\n",
        "            print(\"Set to pause on next loop\")\n",
        "        elif event.name == \"*\":\n",
        "            end_loop = True\n",
        "            print(\"Set to end on next loop\")\n",
        "    \n",
        "    keyboard.on_press(on_key_press)\n",
        "    keyboard.wait('esc')\n",
        "\n",
        "listener_thread = threading.Thread(target=keyboard_listener)\n",
        "listener_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_ui():\n",
        "    global pause_loop, end_loop\n",
        "    \n",
        "    def toggle_pause():\n",
        "        global pause_loop\n",
        "        pause_loop = not pause_loop\n",
        "        pause_button.config(text=\"Stop\" if not pause_loop else \"Speak now\",\n",
        "                            bg=\"red\" if not pause_loop else \"green\")\n",
        "\n",
        "    def end_program():\n",
        "        global end_loop\n",
        "        end_loop = True\n",
        "        root.quit()\n",
        "        root.destroy()\n",
        "\n",
        "    root = tk.Tk()\n",
        "    root.title(\"Control Panel\")\n",
        "    root.geometry(\"300x200\")\n",
        "    root.configure(bg='#f0f0f0')\n",
        "\n",
        "    frame = tk.Frame(root, bg='#f0f0f0')\n",
        "    frame.pack(expand=True, fill='both', padx=20, pady=20)\n",
        "\n",
        "    pause_button = tk.Button(frame, text=\"Stop\", command=toggle_pause, \n",
        "                             bg=\"red\", fg=\"white\", font=(\"Arial\", 12), \n",
        "                             width=10, height=2)\n",
        "    pause_button.pack(pady=10)\n",
        "\n",
        "    end_button = tk.Button(frame, text=\"End Program\", command=end_program, \n",
        "                           bg=\"gray\", fg=\"white\", font=(\"Arial\", 12), \n",
        "                           width=10, height=2)\n",
        "    end_button.pack(pady=10)\n",
        "\n",
        "    root.protocol(\"WM_DELETE_WINDOW\", end_program)\n",
        "    root.mainloop()\n",
        "\n",
        "# Start the UI in a separate thread\n",
        "ui_thread = Thread(target=create_ui)\n",
        "ui_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "while True:\n",
        "    if not pause_loop:\n",
        "        # --- Record Speech ---\n",
        "        time.sleep(0.1)\n",
        "        print(\"Recording speech...\")\n",
        "        wav_file = record_speech()\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Speech to Text ---\n",
        "        print(\"Converting speech to text...\")\n",
        "        text = stt_model.transcribe(audio_directory + wav_file, language=\"en\")\n",
        "        print(\"You said: \", text.get(\"text\"))\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Chatbot ---\n",
        "        print(\"Chatting...\")\n",
        "        response = chat_model.chat_with_history_doc(text.get(\"text\"))\n",
        "\n",
        "        print(\"Chatbot: \", response)\n",
        "\n",
        "        # --- Text to Speech ---\n",
        "        print(\"Converting text to speech...\")\n",
        "        stream_tts(response)\n",
        "\n",
        "    else:\n",
        "        time.sleep(0.1)\n",
        "        if end_loop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = chat_model.get_message_history()\n",
        "dump(conversation, joblib_directory + \"conversation.joblib\")\n",
        "\n",
        "\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Create a PDF object\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "\n",
        "# Set margins (in millimeters)\n",
        "pdf.set_margins(left=10, top=20, right=10)  # Right margin set to 10mm \n",
        "\n",
        "# Choose a nicer font\n",
        "pdf.set_font(\"Helvetica\", size=12)\n",
        "\n",
        "# Calculate usable width for text wrapping (accounting for margins)\n",
        "usable_width = pdf.w - pdf.l_margin - pdf.r_margin -0 # 10px right margin \n",
        "\n",
        "# Add conversation to the PDF\n",
        "for turn in conversation[2:]:\n",
        "    # Skip turns with empty roles or content\n",
        "    if turn['role'] and turn['content']:\n",
        "        # Subheading style for \"User\" and \"Assistant\"\n",
        "        pdf.set_font(\"Helvetica\", style=\"B\", size=14)\n",
        "        pdf.cell(0, 10, txt=f\"{turn['role'].capitalize()}:\", ln=True)\n",
        "\n",
        "        # Content with regular font and correct indentation\n",
        "        pdf.set_font(\"Helvetica\", size=12)\n",
        "        pdf.x = pdf.l_margin  # Reset x-coordinate to the left margin\n",
        "        pdf.multi_cell(usable_width, 6, txt=turn['content'])\n",
        "        pdf.ln(3)\n",
        "\n",
        "# Save the PDF\n",
        "pdf.output(pdf_directory + \"conversation.pdf\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
