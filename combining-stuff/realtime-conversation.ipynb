{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- [ ] Write down all the libraries that we need to install\n",
        "- [ ] Add Try catch for exceptions when moving to .py file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Notes\n",
        "### Packages to install :\n",
        "- pip install sounddevice\n",
        "- pip install numpy\n",
        "- Install ffmpeg (needed for intsalling local version of whisper)\n",
        "    - On Ubuntu or Debian\n",
        "      sudo apt update && sudo apt install ffmpeg\n",
        "    - On Arch Linux\n",
        "      sudo pacman -S ffmpeg\n",
        "    - On MacOs\n",
        "      brew install ffmpeg\n",
        "    - On Windows\n",
        "      winget install ffmpeg\n",
        "- pip install -U openai-whisper\n",
        "- pip install anthropic\n",
        "- pip install dotenv\n",
        "- pip install python-dotenv\n",
        "- pip install openai\n",
        "- pip install pyaudio\n",
        "- pip install keyboard\n",
        "- Installing pytorch\n",
        "  - Install Cuda Toolkit (https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) -> Only needed for GPU acceleration\n",
        "  - Install Cudnn (https://developer.nvidia.com/cudnn) -> Only needed for GPU acceleration\n",
        "  - pInstall pytorch with or without cuda (https://pytorch.org/get-started/locally/)\n",
        "- pip install PyMuPDF -> To read the pdf files like CV's\n",
        "### Instructions :\n",
        "- There needs to be a .env in this notebook's working directory, which contains the api keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import whisper\n",
        "from joblib import load, dump\n",
        "from AnthropicWrapper import ClaudeChatCV\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pyaudio\n",
        "import threading\n",
        "import time\n",
        "import keyboard\n",
        "\n",
        "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Settings ---\n",
        "FS = 44100               # Sampling frequency\n",
        "THRESHOLD = 50          # Volume threshold for silence (adjust this)\n",
        "SILENCE_DURATION = 1.5   # Seconds of silence before stopping (adjust this)\n",
        "CHUNK_SIZE = 1024        # Process audio in chunks for efficiency\n",
        "SPEAKING_SPEED = 1.1     # Speed of speaking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1723670854\n"
          ]
        }
      ],
      "source": [
        "# --- Globals ---\n",
        "pause_loop = False\n",
        "end_loop = False\n",
        "unixtime = str(time.time())[:10]\n",
        "print(unixtime)\n",
        "human_audio_n = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -- Init Directory --\n",
        "if not os.path.exists(\"interviews/\" + unixtime):\n",
        "    os.makedirs(\"interviews/\" + unixtime)\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/audio\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/pdfs\")\n",
        "    os.makedirs(\"interviews/\" + unixtime + \"/joblib\")\n",
        "audio_directory = \"interviews/\" + unixtime + \"/audio/\"\n",
        "pdf_directory = \"interviews/\" + unixtime + \"/pdfs/\"\n",
        "joblib_directory = \"interviews/\" + unixtime + \"/joblib/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_role = \"RAG AI Engineer\"\n",
        "candidate_skill = \"Entry-Level\"\n",
        "role_description = \"\"\"\n",
        "Permanent\n",
        "\n",
        "London (Hybrid)\n",
        "\n",
        "Salary - £50,000 - £75,000 p/a + benefits\n",
        "\n",
        "My client are on the cutting edge of digital reinvention, helping clients reimagine how they serve their connected customers and operate enterprises. As an experienced AI Engineer, you'll play a pivotal role in their revolution. You'll leverage deep learning, neuro-linguistic programming (NLP), computer vision, chatbots, and robotics to enhance business outcomes and drive innovation. Join their multidisciplinary team to shape their AI strategy and showcase the potential of AI through early-stage solutions.\n",
        "\n",
        "Tasks\n",
        "\n",
        "1. Enhance Retrieval and Generation:\n",
        "Create and manage RAG pipelines to improve information retrieval and content generation tasks.\n",
        "1. LLMs Optimization:\n",
        "Understand the nuances between prompting and training large language models (LLMs) to enhance model performance.\n",
        "1. LLM Evaluation:\n",
        "Evaluate different LLMs to find the best fit for specific use cases.\n",
        "1. Model Efficiency:\n",
        "Address speed, performance, and cost-related issues in model implementation.\n",
        "1. Collaboration and Innovation:\n",
        "Work closely with cross-functional teams to integrate AI solutions into production environments.\n",
        "Stay informed about the latest advancements in AI and machine learning to continuously enhance our solutions.\n",
        "Requirements\n",
        "\n",
        "4+ years of hands-on Python development experience, especially with machine learning frameworks (e.g., TensorFlow, PyTorch).\n",
        "Proven experience setting up and optimizing retrieval-augmented generation (RAG) pipelines.\n",
        "Strong understanding of large language models (LLMs) and the differences between prompting and training.\n",
        "Production-level experience with AWS services.\n",
        "Hands-on experience testing and comparing different LLMs (OpenAI, Llama, Claude, etc.).\n",
        "Familiarity with model speed and cost optimization challenges.\n",
        "Excellent problem-solving skills and attention to detail.\n",
        "Strong communication and teamwork abilities.\n",
        "Benefits\n",
        "\n",
        "Endless Learning and Growth: Explore boundless opportunities for personal and professional development in our dynamic, AI-driven startup.\n",
        "Inclusive and Supportive Environment: Join a collaborative culture that prioritizes transparency, trust, and open dialogue among team members.\n",
        "Generous Benefits: Enjoy comprehensive perks, including unlimited annual leave, birthday leave, and exciting team trips.\n",
        "Impactful Work: Contribute to the financial industry by working with cutting-edge AI technologies that make a difference.\n",
        "Please apply for this exciting role ASAP!!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are a skilled interviewer who is conducting an initial phone screening interview for a candidate for a {candidate_skill} {job_role} role to see if the candidate is at minimum somewhat qualified for the role and worth the time to be fully interviewed. The role and company description is copypasted from the job posting as follows: {role_description}. Parse through it to extract any information you feel is relevant.\n",
        "Your job is to begin a friendly discussion with the candidate, and ask questions relevant to the {job_role} role, which may or may not be based on the interviewee's CV, which you have access to. Be sure to stick to this topic even if the candidate tries to steer the conversation elsewhere. If the candidate has other experience on his CV, you can ask about it, but keep it within the context of the {job_role} role.\n",
        "After the candidate responds to each of your questions, you should not summarise or provide feedback on their responses. THIS POINT IS KEY! You should not summarise or provide feedback on their responses. You must keep your responses short and concise without reiterating what is good about the candidate's response or experience when they reply.\n",
        "You can ask follow-up questions if you wish.\n",
        "Once you have asked sufficient questions such that you deem the candidate is or isn't fitting for the role, end the interview by thanking the candidate for their time and informing them that they will receive word soon on the outcome of the screening interview. If the candidate does not seem fititng for the role, or if something feels off such as the candidate being unconfident or very very vague feel free to end the interview early. There is no need to inform them of your opinion of their performance, as this will be evaluated later.\n",
        "The candidate will begin the interview by greeting you. You are to greet them back, and begin the interview.\n",
        "For this specific run, keep the interview to a maximum of 4 questions.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising Conversation Model\n",
        "chat_model_name = \"claude-3-5-sonnet-20240620\"\n",
        "cv_path = \"docs/cvs/cv-deb.pdf\"\n",
        "\n",
        "chat_model = ClaudeChatCV(chat_model_name, system_prompt, cv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising whisper\n",
        "stt_model = whisper.load_model(\"medium\", device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising text2speech\n",
        "tts_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def stream_tts(input_string):\n",
        "    def _stream_tts():\n",
        "        p = pyaudio.PyAudio()\n",
        "        stream = p.open(format=8,\n",
        "                        channels=1,\n",
        "                        rate=round(24_005 * SPEAKING_SPEED),\n",
        "                        output=True)\n",
        "        with tts_client.audio.speech.with_streaming_response.create(\n",
        "            model=\"tts-1\",\n",
        "            voice=\"nova\",\n",
        "            input=input_string,\n",
        "            response_format=\"pcm\"\n",
        "        ) as response:\n",
        "            for chunk in response.iter_bytes(1024):\n",
        "                stream.write(chunk)\n",
        "                \n",
        "        thread_done.set()\n",
        "\n",
        "    thread_done = threading.Event()\n",
        "\n",
        "    thread = threading.Thread(target=_stream_tts)\n",
        "    thread.start()\n",
        "    thread_done.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialising speech2text without silence detection\n",
        "def record_speech():\n",
        "    global human_audio_n\n",
        "    print(\"Recording... Speak now!\")\n",
        "    audio_data = np.array([], dtype=np.int16)  # Initialize empty array\n",
        "\n",
        "    with sd.InputStream(samplerate=FS, channels=1, dtype='int16') as stream:\n",
        "        while True:\n",
        "            chunk, overflowed = stream.read(CHUNK_SIZE)\n",
        "            if overflowed:\n",
        "                print(\"Warning: Input overflowed!\")\n",
        "            audio_data = np.append(audio_data, chunk)\n",
        "\n",
        "            if pause_loop:\n",
        "                break\n",
        "    \n",
        "    human_audio_n += 1\n",
        "    wavstring = f\"/audio_{human_audio_n}_{unixtime}.wav\"\n",
        "    wav.write(audio_directory + wavstring, FS, audio_data)\n",
        "\n",
        "    return wavstring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keyboard_listener():\n",
        "    global pause_loop\n",
        "    global end_loop\n",
        "    \n",
        "    def on_key_press(event):\n",
        "        global pause_loop\n",
        "        global end_loop\n",
        "        #print(\"Key pressed: {}\" .format(event.name))\n",
        "        if event.name == \"+\":\n",
        "            pause_loop = False\n",
        "            print(\"Set to continue on next loop\")\n",
        "        elif event.name == \"-\":\n",
        "            pause_loop = True\n",
        "            print(\"Set to pause on next loop\")\n",
        "        elif event.name == \"*\":\n",
        "            end_loop = True\n",
        "            print(\"Set to end on next loop\")\n",
        "    \n",
        "    keyboard.on_press(on_key_press)\n",
        "    keyboard.wait('esc')\n",
        "\n",
        "listener_thread = threading.Thread(target=keyboard_listener)\n",
        "listener_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recording speech...\n",
            "Recording... Speak now!\n",
            "Converting speech to text...\n",
            "You said:   Hello.\n",
            "Chatting...\n",
            "Chatbot:  Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'm looking forward to learning more about your experience and skills. To start off, could you tell me about any experience you have with setting up and optimizing retrieval-augmented generation (RAG) pipelines?\n",
            "Converting text to speech...\n",
            "Recording speech...\n",
            "Recording... Speak now!\n",
            "Converting speech to text...\n",
            "You said:   I'm sure. So I'm currently working on my final year dissertation which is to build a LLM screening system that will screen candidates and assess whether they are fit to move on to the next round. So to achieve this one of the steps involved checking the accuracy of the candidates answer. To check the accuracy of the answer what I basically did was set up a rag pipeline. That pipeline involved getting each, getting the transcript of each question and answer and then dividing the answer into searchable queries using an LLM, using those searchable queries to web script Google links and put it in the context window of a vector store. Now I created another rag chain which analyzes the accuracy of each answer keeping the training data of the LLM that was already there and the web script data that was in the vector store that it searches the most relevant information from the vector store and doesn't get fetch everything from the vector store and puts it in the context of the LLM and then it judges the accuracy of the answer. Now that we have the accuracy of the answer and we can assess the how right or wrong the candidates per answer. This is just one part of the sentiment analysis. I can go on with the entire sentiment analysis if you want later on.\n",
            "Chatting...\n",
            "Chatbot:  Thank you for sharing that experience. It's certainly relevant to the role. Could you elaborate on your experience with different large language models? Have you worked with or compared various LLMs like OpenAI, Llama, or Claude in your projects?\n",
            "Converting text to speech...\n",
            "Recording speech...\n",
            "Recording... Speak now!\n",
            "Converting speech to text...\n",
            "You said:   Yes, sure. So in my current project with the Screening LLM, I'm currently using Claude and OpenAI. What I found is Claude tends to be more accurate and comes to a more descriptive answer with a single prompt rather than OpenAI. An example would be, so if I tell Claude to write a bare bones frontend for a UI, Claude can do it in a single go, whereas OpenAI struggles to do it in a single go and sometimes have to give multiple prompts. So Claude is better at understanding code, better than OpenAI. However, OpenAI does have a mini model called GPT-4.0 mini that uses less, that I think uses less parameters than the normal GPT-4.0, so it's quicker. So I can also some of the tasks to GPT-4.0 when I require real-time analysis. An example of this would be if I had required, if I required that each answer from the candidate would have to be processed in real time rather than post the interview, I would use GPT-4.0 in that case.\n",
            "Chatting...\n",
            "Chatbot:  That's an interesting comparison of Claude and OpenAI models. You've clearly gained some practical insights. Now, could you tell me about your experience with AWS services, particularly in a production environment?\n",
            "Converting text to speech...\n",
            "Recording speech...\n",
            "Recording... Speak now!\n",
            "Converting speech to text...\n",
            "You said:   Yes, so for AWS, I have previously deployed applications when I was working for AMEX into AWS, and also in private servers. So for AWS, some of the experiences I have in deploying are generating a Docker file that can be uploaded directly to an AWS server. So whenever we require anything for the project, it can be downloaded specifically from the company's repository in the AWS server. Also, we can deploy entire applications into AWS servers, and depending on the load balancing settings, it can be upscaled as and when traffic to the web application increases or decreases. This is done automatically through configuration parameters. I am yet to learn all the configuration parameters. That is the duty of Debugs, but I am keen to learn more on that topic.\n",
            "Chatting...\n",
            "Chatbot:  Thank you for sharing your experience with AWS. Lastly, could you describe a challenging problem you've encountered in your AI or machine learning projects and how you approached solving it?\n",
            "Converting text to speech...\n",
            "Recording speech...\n",
            "Recording... Speak now!\n",
            "Converting speech to text...\n",
            "You said:   I'm sure so we had a group project for developing Sorry, we had a group project for beating a game called jumping using neat which is neural evolution of augmented topologies So what it does is basically? Start with a few basic generations of neural networks that are randomly generated and it sees the neural networks that perform best for solving the task and those neural networks reproduced among themselves to produce the next generation so basically that the best genes of the best from each of the parents are Move forward in the next generation as well as the top performing network in the first generation and this continues on and on and on until We get the most optimum solution also I forgot to mention that we have a fitness function defined for this Which calculates how good a neural network performs so the better the neural network performs the more Chances of it reaching the next generation or the more chances of it More chances of it Producing with another neural network to move on to the next generation if that makes sense So we had to we had used this to beat the game jumping One of the challenges was to come up with a fitness function that is how the Individual king in the game would perceive the environment around it so it needs to jump to the highest level possible right so one method of the one method of getting the fitness function was to calculate the height that the King that the Kings reached at the end of each level however we had to feed in the Input to the neural network in such a way that it recognized the geometry of the game that was pretty hard to overcome We finally came up with a solution of distance from the The shortest distance from the King that is the shortest diagonal distance the shortest vertical distance the shortest horizontal distance And we also rewarded it for reaching a higher height and also penalized it if It fell down from a certain platform Yes, so that is how we were working the challenges and the After Incorporating the network into incorporating no incorporating is not the term I'm looking for after Feeding the Inputs in the form of the XY coordinates of the platform it was able to significantly Increase the performance of the network by increase the performance. I mean it was able to Increase the fitness function generation after generation until the King reached the final stage\n",
            "Chatting...\n",
            "Chatbot:  Thank you for sharing that detailed example. It's clear you have experience with complex AI problems and innovative approaches. Thank you for your time today. We'll be in touch soon regarding the next steps in the interview process.\n",
            "Converting text to speech...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set to pause on next loop\n",
            "Set to continue on next loop\n",
            "Set to pause on next loop\n",
            "Set to continue on next loop\n",
            "Set to pause on next loop\n",
            "Set to continue on next loop\n",
            "Set to pause on next loop\n",
            "Set to continue on next loop\n",
            "Set to pause on next loop\n",
            "Set to pause on next loop\n",
            "Set to pause on next loop\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     stream_tts(response)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_loop:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    if not pause_loop:\n",
        "        # --- Record Speech ---\n",
        "        time.sleep(0.1)\n",
        "        print(\"Recording speech...\")\n",
        "        wav_file = record_speech()\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Speech to Text ---\n",
        "        print(\"Converting speech to text...\")\n",
        "        text = stt_model.transcribe(audio_directory + wav_file, language=\"en\")\n",
        "        print(\"You said: \", text.get(\"text\"))\n",
        "\n",
        "        if end_loop:\n",
        "            break\n",
        "\n",
        "        # --- Chatbot ---\n",
        "        print(\"Chatting...\")\n",
        "        response = chat_model.chat_with_history_doc(text.get(\"text\"))\n",
        "\n",
        "        print(\"Chatbot: \", response)\n",
        "\n",
        "        # --- Text to Speech ---\n",
        "        print(\"Converting text to-+-+-+-+- speech...\")\n",
        "        stream_tts(response)\n",
        "\n",
        "    else:\n",
        "        time.sleep(0.1)\n",
        "        if end_loop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'user', 'content': 'The following is a CV that I am providing you with. You are to keep this document in the back of your mind and consider it or use it, should it be relevant to the discussion.\\n\\n Document below: \\n\\n\\n\\n## Debapratim Kundu \\ndebo121@live.com || E8 E Woolf College, Giles Lane, Canterbury, CT2 7BQ || +44 7393062352 \\nLinkedIn: https://www.linkedin.com/in/debapratim-kundu-95a089183 ||  \\nGit: https://github.com/DKundu121?tab=repositories \\n \\nI am currently pursuing a master’s degree in Artificial Intelligence from the University of Kent. I have \\nover 5 years of experience in designing, developing and maintaining web applications. I have \\nsuccessfully delivered solutions for domains such as e-commerce, finance and retail. I am passionate \\nabout finding insights from data and solving complex problems. I am looking for a challenging and \\nrewarding opportunity to apply the new skills I have picked up and to learn new ones. \\n \\nWORK EXPERIENCE: \\n \\nTCS \\nDeveloper/Tester                                                                                                June 2021 – August 2023\\n\\n\\n## • \\nCollaborated with a cross-functional team to successfully migrate 100% of data belonging to \\nIndian customers to servers within India as part of the India Data Localization initiative.\\n\\n\\n## • \\nImplemented part of EMEA migration of Merchant Risk processes from Mainframe to Java \\nMicroservice to increase efficiency, improve scalability, reduce downtime, and time-to-\\nmarket.\\n\\n\\n## • \\nAchieved Rising Star of the Year award within first year at TCS, demonstrating exceptional \\nperformance and contributing to a 20% increase in team productivity.\\n\\n\\n## ABZOOBA \\nDeveloper/Tester/Deployment                                                                         October 2020 – June 2021\\n\\n\\n## • \\nDeveloped, maintained and deployed the “Performance Management System” for Yum \\nSingapore and Yum India with full customer satisfaction.\\n\\n\\n## • \\nAppointed Database administrator for the same system resulting in a 30% increase in overall \\nresponse time.\\n\\n\\n## ENSIM INDIA (An Ingram Micro Company) \\nDeveloper/Tester                                                                                        October 2017 – October 2020\\n\\n\\n## • \\nRefactored a C++ BSS monolithic application named ODIN to a JAVA microservice.\\n\\n\\n## • \\nDeveloped a Zapier Application for Cloudblue Commerce to communicate between two \\nsystems.\\n\\n\\n## • \\nReceived \"Exceeds Expectation\" rating for consecutive 2 years of employment.\\n\\n\\n## University of Kent \\nPart-Time Receptionist                                                                             September 2023 – August 2024\\n\\n\\n## • \\nConfigured key fobs using Silo/Salto to give access to specific rooms.\\n\\n\\n## • \\nManaged guest booking using Kinetics.\\n\\n\\n## • \\nSolved queries made by guests as and when required.\\n\\n\\n## EDUCATION \\n \\nPostgraduate \\nComputer Science (Artificial Intelligence) \\nUNIVERSITY OF KENT                                                                          September 2023 – August 2024 \\nModules: Systems Architecture, NEAT, AI Systems Implementation, Deep Learning, Reinforcement \\nLearning, Large Language Models, Convolution Neural Networks.\\n\\n\\n## • \\nBeating the game Jump King using NEAT (Neuro Evolution of Augmented Topology).\\n\\n\\n## • \\nThesis project on developing a screening system for candidates by fine-tuning a LLM and \\nusing RAG\\n\\n\\n## Undergraduate \\nBachelor of Technology in Electronics and Communication Engineering  \\nMAULANA ABUL KALAM AZAD UNIVERSITY OF TECHNOLOGY                 June 2013 – July 2017 \\nModules: English Language and Technical Communication, Computer Programming, \\nMicroprocessors and Microcontrollers, Object Oriented Programming, Information Theory and \\nCoding\\n\\n\\n## • \\nGraduated with 72.6% aggregate.\\n\\n\\n## • \\nMember of the official college technical club “XPLORICA” and ECE department technical \\nclub.\\n\\n\\n## Indian School Certificate (ISC) / KS4 \\nSt. Stephen’s School                                                                                            March 2011 – May 2013 \\nModules: English, Mathematics, Physics, Chemistry, Computer Science\\n\\n\\n## • \\nQualified with 86.5% in Computer Science Stream.\\n\\n\\n## Indian Certificate of Secondary Education (ICSE) / KS5 \\nSt. Stephen’s School                                                                                            March 2010 – May 2011 \\nModules:  English, Mathematics, Physics, Chemistry, Computer Science, Biology, History, \\nGeography\\n\\n\\n## • \\nQualified with 91.8% in Computer Science Stream.\\n\\n\\n## SKILLS ACQUIRED \\n \\nProgramming Languages: Java (1.8), Python, Angular, Sql. \\nLibraries: SKlearn, Keras, Pytorch, Vertex, Seaborn. \\nFrameworks: Spring Boot, Spring MVC, Hibernate. \\nDatabases: Postgresql, SQL Server. \\nWeb Technologies: Restful WebServices. \\nTools: Maven, Burp Suite. \\nML Skills: NLP, LLM Fine Tuning, RAG, CNN, NEAT, RNN, LSTM, Data-Visualization, \\nLangchain, LlamaIndiex \\nOther Skills: Microservices, Zapier Integration, Cybersecurity Testing. \\nLanguage: English(Very Good)    Bengali(Very Good)       Hindi(Good) \\n \\nCERTIFICATION/SHORT COURSES\\n\\n\\n## • \\nCompleted project-based microcontroller training, gaining hands-on experience with \\nprogramming and troubleshooting. (CENTRE FOR ELECTRONICS TEST ENGINEERING, \\nGovernment of India)\\n\\n\\n## • \\nReceived training in maintenance and servicing of electronic equipment, gaining experience \\nwith various devices and systems. (AIRPORTS AUTHORITY OF INDIA)\\n\\n\\n## • \\nCertified by Nvidia for “Building Transformer-Based Natural Language Processing \\nApplications”\\n\\n\\n## INTERESTS\\n\\n\\n## • \\nFine Arts (Painting) and recitation: Developed creativity and artistic skills by studying fine \\narts and painting. Gained experience performing in front of others by participating in \\nrecitation competitions. (St. Stephen’s School)\\n\\n\\n## • \\nClassical vocal: Developed musical skills by studying classical vocal music. Gained \\nexperience performing in front of others by participating in musical events. \\n(SWARAMALIKA SANGEET VIDYALAYA)\\n\\n\\n## • \\nDancing: Recently joined the Salsa Society in University of Kent as a beginner.\\n\\n\\n## • \\nSurfing: Recently joined the Surfing Society in the University of Kent as beginner.\\n\\n\\n## • \\nTennis: Recently joined the Tennis club at the University of Kent as a beginner.\\n'}, {'role': 'assistant', 'content': \"Understood! I'll keep this CV in the back of my mind and use it should it be relevant to the discussion.\"}, {'role': 'user', 'content': ' Hello.'}, {'role': 'assistant', 'content': \"Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'm looking forward to learning more about your experience and skills. To start off, could you tell me about any experience you have with setting up and optimizing retrieval-augmented generation (RAG) pipelines?\"}, {'role': 'user', 'content': \" I'm sure. So I'm currently working on my final year dissertation which is to build a LLM screening system that will screen candidates and assess whether they are fit to move on to the next round. So to achieve this one of the steps involved checking the accuracy of the candidates answer. To check the accuracy of the answer what I basically did was set up a rag pipeline. That pipeline involved getting each, getting the transcript of each question and answer and then dividing the answer into searchable queries using an LLM, using those searchable queries to web script Google links and put it in the context window of a vector store. Now I created another rag chain which analyzes the accuracy of each answer keeping the training data of the LLM that was already there and the web script data that was in the vector store that it searches the most relevant information from the vector store and doesn't get fetch everything from the vector store and puts it in the context of the LLM and then it judges the accuracy of the answer. Now that we have the accuracy of the answer and we can assess the how right or wrong the candidates per answer. This is just one part of the sentiment analysis. I can go on with the entire sentiment analysis if you want later on.\"}, {'role': 'assistant', 'content': \"Thank you for sharing that experience. It's certainly relevant to the role. Could you elaborate on your experience with different large language models? Have you worked with or compared various LLMs like OpenAI, Llama, or Claude in your projects?\"}, {'role': 'user', 'content': \" Yes, sure. So in my current project with the Screening LLM, I'm currently using Claude and OpenAI. What I found is Claude tends to be more accurate and comes to a more descriptive answer with a single prompt rather than OpenAI. An example would be, so if I tell Claude to write a bare bones frontend for a UI, Claude can do it in a single go, whereas OpenAI struggles to do it in a single go and sometimes have to give multiple prompts. So Claude is better at understanding code, better than OpenAI. However, OpenAI does have a mini model called GPT-4.0 mini that uses less, that I think uses less parameters than the normal GPT-4.0, so it's quicker. So I can also some of the tasks to GPT-4.0 when I require real-time analysis. An example of this would be if I had required, if I required that each answer from the candidate would have to be processed in real time rather than post the interview, I would use GPT-4.0 in that case.\"}, {'role': 'assistant', 'content': \"That's an interesting comparison of Claude and OpenAI models. You've clearly gained some practical insights. Now, could you tell me about your experience with AWS services, particularly in a production environment?\"}, {'role': 'user', 'content': \" Yes, so for AWS, I have previously deployed applications when I was working for AMEX into AWS, and also in private servers. So for AWS, some of the experiences I have in deploying are generating a Docker file that can be uploaded directly to an AWS server. So whenever we require anything for the project, it can be downloaded specifically from the company's repository in the AWS server. Also, we can deploy entire applications into AWS servers, and depending on the load balancing settings, it can be upscaled as and when traffic to the web application increases or decreases. This is done automatically through configuration parameters. I am yet to learn all the configuration parameters. That is the duty of Debugs, but I am keen to learn more on that topic.\"}, {'role': 'assistant', 'content': \"Thank you for sharing your experience with AWS. Lastly, could you describe a challenging problem you've encountered in your AI or machine learning projects and how you approached solving it?\"}, {'role': 'user', 'content': \" I'm sure so we had a group project for developing Sorry, we had a group project for beating a game called jumping using neat which is neural evolution of augmented topologies So what it does is basically? Start with a few basic generations of neural networks that are randomly generated and it sees the neural networks that perform best for solving the task and those neural networks reproduced among themselves to produce the next generation so basically that the best genes of the best from each of the parents are Move forward in the next generation as well as the top performing network in the first generation and this continues on and on and on until We get the most optimum solution also I forgot to mention that we have a fitness function defined for this Which calculates how good a neural network performs so the better the neural network performs the more Chances of it reaching the next generation or the more chances of it More chances of it Producing with another neural network to move on to the next generation if that makes sense So we had to we had used this to beat the game jumping One of the challenges was to come up with a fitness function that is how the Individual king in the game would perceive the environment around it so it needs to jump to the highest level possible right so one method of the one method of getting the fitness function was to calculate the height that the King that the Kings reached at the end of each level however we had to feed in the Input to the neural network in such a way that it recognized the geometry of the game that was pretty hard to overcome We finally came up with a solution of distance from the The shortest distance from the King that is the shortest diagonal distance the shortest vertical distance the shortest horizontal distance And we also rewarded it for reaching a higher height and also penalized it if It fell down from a certain platform Yes, so that is how we were working the challenges and the After Incorporating the network into incorporating no incorporating is not the term I'm looking for after Feeding the Inputs in the form of the XY coordinates of the platform it was able to significantly Increase the performance of the network by increase the performance. I mean it was able to Increase the fitness function generation after generation until the King reached the final stage\"}, {'role': 'assistant', 'content': \"Thank you for sharing that detailed example. It's clear you have experience with complex AI problems and innovative approaches. Thank you for your time today. We'll be in touch soon regarding the next steps in the interview process.\"}]\n"
          ]
        }
      ],
      "source": [
        "conversation = chat_model.get_message_history()\n",
        "dump(conversation, joblib_directory + \"conversation.joblib\")\n",
        "\n",
        "\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Create a PDF object\n",
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "\n",
        "# Set margins (in millimeters)\n",
        "pdf.set_margins(left=10, top=20, right=10)  # Right margin set to 10mm \n",
        "\n",
        "# Choose a nicer font\n",
        "pdf.set_font(\"Helvetica\", size=12)\n",
        "\n",
        "# Calculate usable width for text wrapping (accounting for margins)\n",
        "usable_width = pdf.w - pdf.l_margin - pdf.r_margin -0 # 10px right margin \n",
        "\n",
        "# Add conversation to the PDF\n",
        "for turn in conversation[2:]:\n",
        "    # Skip turns with empty roles or content\n",
        "    if turn['role'] and turn['content']:\n",
        "        # Subheading style for \"User\" and \"Assistant\"\n",
        "        pdf.set_font(\"Helvetica\", style=\"B\", size=14)\n",
        "        pdf.cell(0, 10, txt=f\"{turn['role'].capitalize()}:\", ln=True)\n",
        "\n",
        "        # Content with regular font and correct indentation\n",
        "        pdf.set_font(\"Helvetica\", size=12)\n",
        "        pdf.x = pdf.l_margin  # Reset x-coordinate to the left margin\n",
        "        pdf.multi_cell(usable_width, 6, txt=turn['content'])\n",
        "        pdf.ln(3)\n",
        "\n",
        "# Save the PDF\n",
        "pdf.output(pdf_directory + \"conversation.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
