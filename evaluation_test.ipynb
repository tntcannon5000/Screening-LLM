{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages if you don't have them:\n",
    "# !pip install python-dotenv joblib anthropic hume-api\n",
    "\n",
    "# Import Statements\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from joblib import load\n",
    "from pprint import pprint\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# Custom Modules (Assuming these are in your project structure)\n",
    "from src.utils.anthropicwrapper import ClaudeChat, ClaudeChatAssess\n",
    "from src.utils.humewrapper import HumeSentimentAnalyzer\n",
    "from src.modules import ConversationVerifier\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Environment Variables ---\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))\n",
    "\n",
    "# --- Timestamp Input ---\n",
    "timestamp = 1724629705  # Example timestamp, replace with your input \n",
    "\n",
    "# --- Load Data ---\n",
    "directory = f'data/interviews/{timestamp}/'\n",
    "chatlog = load(os.path.join(directory, \"joblib/conversation.joblib\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF: no such file: 'data/cvs/cv-deb.pdf'\n"
     ]
    }
   ],
   "source": [
    "# --- Model Setup --- \n",
    "try:\n",
    "    sentiment_analyser = HumeSentimentAnalyzer(api_key=os.getenv(\"HUME_API_KEY\"))\n",
    "\n",
    "    sentiment_system_prompt = \"\"\"You are a skilled emotions analyst provided with a detailed breakdown of sentiment analysis scores from Hume.ai, for a single response in an interview to a question from the interviewer. The scores are split into 3 sections. All numbers are from 0 to 1, linearly scaling, with 1 being a very strong representation of the indicator in question.\n",
    "\n",
    "    First, Emotions. This contains several human emotions with a numerical value indicating the strength of the corresponding emotion.\n",
    "    Second, Sentiments. This contains a scale from 1 to 9, each containing a numerical value indicating the magnitude of the sentiment of the topic of the conversation. A negative topic such as murder will have a high value lower in the scale, such as 1 or 2, and a positive topic will have a high value from 0 to 1 higher in the scale such as 8 or 9.\n",
    "    Third, Toxicity. This contains several toxic representations such as hate, insult, etc, with a value from 0 to 1 for each representation identified in the audio.\n",
    "\n",
    "    Your job is to provide a concise detailed one sentence breakdown of how the individual was feeling for the particular scores provided. You must be highly objective as your job is to discern whether or not a candidate was exhibiting traits which would or would not be fitting for a successful interview. \n",
    "    Model your answer beginning with something along the lines of \"For this particular response, the candidate...\"\"\"\n",
    "    \n",
    "    sentiment_summariser = ClaudeChat(\"claude-3-5-sonnet-20240620\", sentiment_system_prompt)\n",
    "\n",
    "    evaluation_system_prompt = \"\"\"You are a highly skilled interviewer currently tasked with reviewing a phone screening interview candidate to decide whether they are to pass on to the next stage of the interview process. There is a high volume of candidates and as such only 47% of candidates should be allowed to pass.\n",
    "    You will be provided with the following information to aid your decision:\n",
    "    1. A copy of the job description.\n",
    "    2. Transcript of the phone interview between the interviewer and candidate.\n",
    "    3. Sentiment analysis summary to provide you with insight into the tone of the candidate in the call\n",
    "    4. A summary of any factual inaccuracies the candidate may have made during discussions in the call\n",
    "    5. A copy of the candidate's CV, which will be provided next.\n",
    "\n",
    "    You are to evaluate the candidate, primarily on the transcript, and use the additional information provided to identify any potential red-flags. Your response should include a detailed breakdown of why the candidate is chosen to continue onwards to further interviewing. You must end the breakdown with a simple one word response on a new line, \"pass\" or \"fail\".\"\"\"\n",
    "\n",
    "    candidate_evaluator = ClaudeChatAssess(\"claude-3-5-sonnet-20240620\", evaluation_system_prompt, \"data/cvs/cv-deb.pdf\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model setup: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reformat Chatlog ---\n",
    "def reformat_chatlog(chatlog):\n",
    "    dropped_context = chatlog[3:]\n",
    "    outputchatlog = []\n",
    "\n",
    "    for i in range(0, len(dropped_context), 2):\n",
    "        if i + 1 < len(dropped_context):\n",
    "            tempdict = {\n",
    "                'interviewer': dropped_context[i]['content'],\n",
    "                'candidate': dropped_context[i+1]['content']\n",
    "            }\n",
    "            outputchatlog.append(tempdict)\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    return outputchatlog\n",
    "\n",
    "# --- Process Sentiments ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiments(chatlog_chat, timestamp):\n",
    "    filepath = os.path.join('data', 'interviews', str(timestamp), 'audio')\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Full audio directory path: {os.path.abspath(filepath)}\")\n",
    "\n",
    "    files = [f for f in os.listdir(filepath) if os.path.isfile(os.path.join(filepath, f))]\n",
    "    \n",
    "    if len(chatlog_chat) < len(files):\n",
    "        files = files[:len(chatlog_chat)]\n",
    "\n",
    "    for f in files:\n",
    "        print(f\"File found: {f}\")\n",
    "\n",
    "    sentiments = []\n",
    "    \n",
    "    for count, file in enumerate(files, 1):\n",
    "        full_file_path = os.path.join(filepath, file)\n",
    "        print(f\"Processing file: {full_file_path}\")\n",
    "        \n",
    "        # Add a small delay and re-check file existence\n",
    "        time.sleep(0.1)\n",
    "        if not os.path.exists(full_file_path):\n",
    "            print(f\"File not found (after delay): {full_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = sentiment_analyser.analyze_audio(full_file_path)\n",
    "            sentiment_summary = sentiment_summariser.chat(str(result))\n",
    "            sentiments.append((result, sentiment_summary))\n",
    "            chatlog_chat[count-1]['sentiment'] = sentiment_summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {full_file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    return chatlog_chat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Candidate --- \n",
    "def evaluate_candidate(chatlog_chat):\n",
    "    ConversationVerifier.process_qa_pair(chatlog_chat)\n",
    "    print(\"The Feedback JSON from the sentiment analyser and accuracy verifier: \\n\")\n",
    "    pprint(chatlog_chat)\n",
    "    evaluation = candidate_evaluator.chat(str(chatlog_chat))\n",
    "    return evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------1724629705-0------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.68603516, 'detected_language': 'fr'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'AllÃ´', 'position': {'begin': 0, 'end': 4}, 'time': {'begin': 0.2525, 'end': 0.7525}, 'confidence': 0.51708984, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020738372579216957}, {'name': 'Adoration', 'score': 0.01181262917816639}, {'name': 'Aesthetic Appreciation', 'score': 0.025624649599194527}, {'name': 'Amusement', 'score': 0.09691035747528076}, {'name': 'Anger', 'score': 0.0017122863791882992}, {'name': 'Annoyance', 'score': 0.033468905836343765}, {'name': 'Anxiety', 'score': 0.001537588657811284}, {'name': 'Awe', 'score': 0.014678704552352428}, {'name': 'Awkwardness', 'score': 0.09391114115715027}, {'name': 'Boredom', 'score': 0.08684036880731583}, {'name': 'Calmness', 'score': 0.17578108608722687}, {'name': 'Concentration', 'score': 0.01743236742913723}, {'name': 'Confusion', 'score': 0.07936257123947144}, {'name': 'Contemplation', 'score': 0.03057972341775894}, {'name': 'Contempt', 'score': 0.026898130774497986}, {'name': 'Contentment', 'score': 0.05857750028371811}, {'name': 'Craving', 'score': 0.001543792779557407}, {'name': 'Desire', 'score': 0.002783130621537566}, {'name': 'Determination', 'score': 0.0045044259168207645}, {'name': 'Disappointment', 'score': 0.007620147429406643}, {'name': 'Disapproval', 'score': 0.01939733326435089}, {'name': 'Disgust', 'score': 0.003605820704251528}, {'name': 'Distress', 'score': 0.0011619292199611664}, {'name': 'Doubt', 'score': 0.014647692441940308}, {'name': 'Ecstasy', 'score': 0.004119873512536287}, {'name': 'Embarrassment', 'score': 0.00807468593120575}, {'name': 'Empathic Pain', 'score': 0.00083870196249336}, {'name': 'Enthusiasm', 'score': 0.09402378648519516}, {'name': 'Entrancement', 'score': 0.013106844387948513}, {'name': 'Envy', 'score': 0.0023139817640185356}, {'name': 'Excitement', 'score': 0.04988173767924309}, {'name': 'Fear', 'score': 0.0004378239973448217}, {'name': 'Gratitude', 'score': 0.010108529590070248}, {'name': 'Guilt', 'score': 0.001002092962153256}, {'name': 'Horror', 'score': 0.0003048056969419122}, {'name': 'Interest', 'score': 0.13673515617847443}, {'name': 'Joy', 'score': 0.08501555025577545}, {'name': 'Love', 'score': 0.005728898104280233}, {'name': 'Nostalgia', 'score': 0.005715981591492891}, {'name': 'Pain', 'score': 0.0002003698464250192}, {'name': 'Pride', 'score': 0.005743168294429779}, {'name': 'Realization', 'score': 0.10372363775968552}, {'name': 'Relief', 'score': 0.007595688104629517}, {'name': 'Romance', 'score': 0.00347414193674922}, {'name': 'Sadness', 'score': 0.0006772489286959171}, {'name': 'Sarcasm', 'score': 0.05803794041275978}, {'name': 'Satisfaction', 'score': 0.052822887897491455}, {'name': 'Shame', 'score': 0.0032640795689076185}, {'name': 'Surprise (negative)', 'score': 0.036121830344200134}, {'name': 'Surprise (positive)', 'score': 0.21053987741470337}, {'name': 'Sympathy', 'score': 0.001816560048609972}, {'name': 'Tiredness', 'score': 0.005596870556473732}, {'name': 'Triumph', 'score': 0.009085817262530327}], 'sentiment': [{'name': '1', 'score': 0.0006381617859005928}, {'name': '2', 'score': 0.0009034622926265001}, {'name': '3', 'score': 0.0016342836897820234}, {'name': '4', 'score': 0.004970110487192869}, {'name': '5', 'score': 0.37046676874160767}, {'name': '6', 'score': 0.18902303278446198}, {'name': '7', 'score': 0.14644967019557953}, {'name': '8', 'score': 0.13382212817668915}, {'name': '9', 'score': 0.13396164774894714}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036815230268985033}, {'name': 'insult', 'score': 0.005257337354123592}, {'name': 'obscene', 'score': 0.0027760271914303303}, {'name': 'severe_toxic', 'score': 0.0018583473283797503}, {'name': 'threat', 'score': 0.0033218057360500097}, {'name': 'toxic', 'score': 0.06039322167634964}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98831725, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making a automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 897}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9893276, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.722984}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98870856, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last, iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1238}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline for accuracy verification in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in natural language processing tasks\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"ChatGPD 4.0 Mini vs ChatGPD 4.0 comparison in terms of cost effectiveness and speed\"']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentioned they are studying artificial '\n",
      "              'intelligence at the University of Kent and are working on an '\n",
      "              'automated screening interview agent for their dissertation. '\n",
      "              'This is relevant and shows their engagement with AI projects.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of the RAG pipeline is mostly accurate, but '\n",
      "              'there are some areas that could be clearer. The candidate '\n",
      "              'states that the RAG pipeline is used as an \"accuracy verifier,\" '\n",
      "              'which could be misleading. While RAG can help improve the '\n",
      "              'accuracy of responses by retrieving relevant information, it is '\n",
      "              'not solely an accuracy verifier. It would be beneficial to '\n",
      "              'clarify that RAG enhances the context for the language model '\n",
      "              '(LM) to generate more accurate responses rather than directly '\n",
      "              'verifying accuracy.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate describes breaking down answers into '\n",
      "              '\"searchable strings\" and using Google to retrieve articles. '\n",
      "              'This is a practical application of RAG, but it would be helpful '\n",
      "              'to mention how the retrieved content is integrated into the '\n",
      "              \"LLM's context for generating responses. The explanation could \"\n",
      "              'benefit from more detail on how the retrieved information is '\n",
      "              'processed and utilized.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of providing an accuracy percentage based on the '\n",
      "              \"candidate's answers is a good point, but it would be more \"\n",
      "              'effective to explain how this percentage is calculated and what '\n",
      "              'criteria are used to determine accuracy.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG and its application in their project, but some '\n",
      "              'clarifications and additional details would enhance the '\n",
      "              'explanation.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's approach to improving context and retrieval \"\n",
      "              'quality by breaking down answers into searchable strings is a '\n",
      "              'practical method. However, the explanation could be clearer '\n",
      "              'regarding how these strings are generated and how they relate '\n",
      "              'to the original answer. The candidate mentions using Google to '\n",
      "              'retrieve articles, but it would be beneficial to clarify how '\n",
      "              'the information from these articles is integrated into the '\n",
      "              \"LLM's context for generating responses.\\n\"\n",
      "              '\\n'\n",
      "              '2. The candidate states that they retrieve content from the '\n",
      "              'first two articles found on Google. While this is a valid '\n",
      "              'approach, it may not always yield the most relevant or '\n",
      "              'high-quality information. It would be helpful to mention the '\n",
      "              'importance of evaluating the credibility and relevance of the '\n",
      "              'sources retrieved.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate discusses storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity for relevance. This is '\n",
      "              'a good point, but the explanation lacks detail on how the '\n",
      "              'cosine similarity is calculated and how it influences the '\n",
      "              \"selection of relevant portions for the LLM's response.\\n\"\n",
      "              '\\n'\n",
      "              '4. The mention of using a technique from a Google paper called '\n",
      "              '\"Query Decomposition\" is relevant, but the candidate could '\n",
      "              'elaborate on how this technique specifically addresses the '\n",
      "              'challenges faced in their implementation. Providing more '\n",
      "              'context on the benefits of this technique would strengthen the '\n",
      "              'explanation.\\n'\n",
      "              '\\n'\n",
      "              '5. The candidate\\'s assertion that this method \"vastly improves '\n",
      "              'the quality of the answers fetched\" is somewhat subjective. It '\n",
      "              'would be more effective to provide specific metrics or examples '\n",
      "              'demonstrating the improvement in answer quality as a result of '\n",
      "              'this approach.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and its application in their project, but '\n",
      "              'additional clarity and detail in certain areas would enhance '\n",
      "              'the explanation.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions selecting \"JATGBD 4.0 mini\" for model '\n",
      "              'selection, which appears to be a transcription error. It is '\n",
      "              'likely they meant \"GPT-4.0 mini.\" This should be clarified as '\n",
      "              'it may confuse the reader regarding the model used.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that they did not choose \"Claude SONET '\n",
      "              '3.5\" because it is considered the most intelligent LLM. While '\n",
      "              'this may be true, the reasoning provided lacks clarity. It '\n",
      "              'would be beneficial to explain why the specific capabilities of '\n",
      "              'Claude SONET 3.5 were not necessary for their project, rather '\n",
      "              'than simply stating it is the most intelligent.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is somewhat vague. The '\n",
      "              'candidate mentions writing several prompts to help the LLM '\n",
      "              'overlook grammatical errors, but it would be helpful to provide '\n",
      "              'specific examples of the types of prompts used or how they were '\n",
      "              'structured to address these issues.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate discusses the challenges of converting speech '\n",
      "              'to text and the resulting errors. While they mention prompting '\n",
      "              'the LLM to overlook these errors, it would strengthen the '\n",
      "              'response to elaborate on how this process was implemented and '\n",
      "              \"the effectiveness of the prompts in improving the model's \"\n",
      "              'understanding.\\n'\n",
      "              '\\n'\n",
      "              '5. The overall explanation of the integration of the RAG '\n",
      "              'pipeline with the LLM could benefit from more detail on how the '\n",
      "              \"RAG component specifically enhances the model's performance. \"\n",
      "              'For instance, discussing how the retrieved information is '\n",
      "              \"utilized in the context of the LLM's responses would provide a \"\n",
      "              'clearer picture of the integration process.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of the RAG pipeline and its application, additional clarity and '\n",
      "              'detail in certain areas would enhance the explanation.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 70%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is accurate in the context of fine-tuning but could be '\n",
      "              \"misleading. They should clarify that while they haven't \"\n",
      "              'fine-tuned a model, they have made decisions regarding model '\n",
      "              'selection based on performance and cost-effectiveness, which is '\n",
      "              'a form of optimization.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPT 4.0 Mini for accuracy '\n",
      "              'verification. However, they should provide more context on why '\n",
      "              'these specific models were chosen beyond their intelligence and '\n",
      "              'cost-effectiveness. For instance, discussing their performance '\n",
      "              'metrics or specific features that align with the project '\n",
      "              'requirements would strengthen their response.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of using Hume AI for sentiment analysis is '\n",
      "              'somewhat vague. The candidate should elaborate on how the '\n",
      "              'integration of this external service works within the overall '\n",
      "              'architecture of their project and how it contributes to the '\n",
      "              'final output.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions feeding the sentiment and accuracy '\n",
      "              'verifier scores back into Claude Sonnet 3.5 to make sense of '\n",
      "              \"the candidate's answers. While this is a valid approach, they \"\n",
      "              'should clarify how this process is executed and what specific '\n",
      "              'insights are derived from this integration.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's assertion that Claude 3.5 Sonnet is the \"\n",
      "              '\"most intelligent LLM till date\" is subjective and could be '\n",
      "              'better supported with specific comparisons or metrics. '\n",
      "              'Providing evidence or references to back this claim would '\n",
      "              'enhance the credibility of their statement.\\n'\n",
      "              '\\n'\n",
      "              '6. Overall, while the candidate demonstrates a solid '\n",
      "              'understanding of model selection and integration, additional '\n",
      "              'clarity and detail in certain areas would enhance the '\n",
      "              'explanation.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-0------------------------\n",
      "---------------------------1724629705-1------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.68310547, 'detected_language': 'fr'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'AllÃ´', 'position': {'begin': 0, 'end': 4}, 'time': {'begin': 0.2525, 'end': 0.7525}, 'confidence': 0.5239258, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020738372579216957}, {'name': 'Adoration', 'score': 0.01181262917816639}, {'name': 'Aesthetic Appreciation', 'score': 0.025624649599194527}, {'name': 'Amusement', 'score': 0.09691035747528076}, {'name': 'Anger', 'score': 0.0017122863791882992}, {'name': 'Annoyance', 'score': 0.033468905836343765}, {'name': 'Anxiety', 'score': 0.001537588657811284}, {'name': 'Awe', 'score': 0.014678704552352428}, {'name': 'Awkwardness', 'score': 0.09391114115715027}, {'name': 'Boredom', 'score': 0.08684036880731583}, {'name': 'Calmness', 'score': 0.17578108608722687}, {'name': 'Concentration', 'score': 0.01743236742913723}, {'name': 'Confusion', 'score': 0.07936257123947144}, {'name': 'Contemplation', 'score': 0.03057972341775894}, {'name': 'Contempt', 'score': 0.026898130774497986}, {'name': 'Contentment', 'score': 0.05857750028371811}, {'name': 'Craving', 'score': 0.001543792779557407}, {'name': 'Desire', 'score': 0.002783130621537566}, {'name': 'Determination', 'score': 0.0045044259168207645}, {'name': 'Disappointment', 'score': 0.007620147429406643}, {'name': 'Disapproval', 'score': 0.01939733326435089}, {'name': 'Disgust', 'score': 0.003605820704251528}, {'name': 'Distress', 'score': 0.0011619292199611664}, {'name': 'Doubt', 'score': 0.014647692441940308}, {'name': 'Ecstasy', 'score': 0.004119873512536287}, {'name': 'Embarrassment', 'score': 0.00807468593120575}, {'name': 'Empathic Pain', 'score': 0.00083870196249336}, {'name': 'Enthusiasm', 'score': 0.09402378648519516}, {'name': 'Entrancement', 'score': 0.013106844387948513}, {'name': 'Envy', 'score': 0.0023139817640185356}, {'name': 'Excitement', 'score': 0.04988173767924309}, {'name': 'Fear', 'score': 0.0004378239973448217}, {'name': 'Gratitude', 'score': 0.010108529590070248}, {'name': 'Guilt', 'score': 0.001002092962153256}, {'name': 'Horror', 'score': 0.0003048056969419122}, {'name': 'Interest', 'score': 0.13673515617847443}, {'name': 'Joy', 'score': 0.08501555025577545}, {'name': 'Love', 'score': 0.005728898104280233}, {'name': 'Nostalgia', 'score': 0.005715981591492891}, {'name': 'Pain', 'score': 0.0002003698464250192}, {'name': 'Pride', 'score': 0.005743168294429779}, {'name': 'Realization', 'score': 0.10372363775968552}, {'name': 'Relief', 'score': 0.007595688104629517}, {'name': 'Romance', 'score': 0.00347414193674922}, {'name': 'Sadness', 'score': 0.0006772489286959171}, {'name': 'Sarcasm', 'score': 0.05803794041275978}, {'name': 'Satisfaction', 'score': 0.052822887897491455}, {'name': 'Shame', 'score': 0.0032640795689076185}, {'name': 'Surprise (negative)', 'score': 0.036121830344200134}, {'name': 'Surprise (positive)', 'score': 0.21053987741470337}, {'name': 'Sympathy', 'score': 0.001816560048609972}, {'name': 'Tiredness', 'score': 0.005596870556473732}, {'name': 'Triumph', 'score': 0.009085817262530327}], 'sentiment': [{'name': '1', 'score': 0.0006381617859005928}, {'name': '2', 'score': 0.0009034622926265001}, {'name': '3', 'score': 0.0016342836897820234}, {'name': '4', 'score': 0.004970110487192869}, {'name': '5', 'score': 0.37046676874160767}, {'name': '6', 'score': 0.18902303278446198}, {'name': '7', 'score': 0.14644967019557953}, {'name': '8', 'score': 0.13382212817668915}, {'name': '9', 'score': 0.13396164774894714}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036815230268985033}, {'name': 'insult', 'score': 0.005257337354123592}, {'name': 'obscene', 'score': 0.0027760271914303303}, {'name': 'severe_toxic', 'score': 0.0018583473283797503}, {'name': 'threat', 'score': 0.0033218057360500097}, {'name': 'toxic', 'score': 0.06039322167634964}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882552, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9876133, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9886468, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1237}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline for accuracy verification in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in natural language processing tasks\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for language model integration\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"ChatGPD 4.0 Mini vs ChatGPD 4.0 comparison in terms of cost effectiveness and speed\"']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. **Clarity and Structure:** The response could benefit from '\n",
      "              'clearer structure and organization. The explanation of the RAG '\n",
      "              'pipeline and its components is somewhat convoluted, making it '\n",
      "              'difficult to follow. Breaking down the explanation into '\n",
      "              'distinct parts (e.g., introduction of the project, description '\n",
      "              'of the RAG pipeline, and its application) would enhance '\n",
      "              'clarity.\\n'\n",
      "              '\\n'\n",
      "              '2. **Technical Detail:** While the candidate mentions using a '\n",
      "              'RAG pipeline for an accuracy verifier, it would be helpful to '\n",
      "              'elaborate on how the sentiment analysis component interacts '\n",
      "              'with the RAG pipeline. The current explanation does not clearly '\n",
      "              'connect how sentiment analysis contributes to the overall '\n",
      "              'accuracy verification process.\\n'\n",
      "              '\\n'\n",
      "              '3. **Implementation Specifics:** The candidate states that the '\n",
      "              'RAG pipeline breaks down answers into \"searchable strings\" and '\n",
      "              'retrieves articles from Google. It would strengthen the answer '\n",
      "              'to specify how the retrieved content is integrated into the '\n",
      "              \"LLM's context. More detail on the mechanism of this integration \"\n",
      "              'would provide a clearer understanding of the RAG process.\\n'\n",
      "              '\\n'\n",
      "              '4. **Terminology Consistency:** The term \"automated screening '\n",
      "              'Interview agent\" should be consistently referred to as '\n",
      "              '\"automated screening interview agent\" for clarity. This is a '\n",
      "              'minor transcription issue and does not reflect on the '\n",
      "              \"candidate's understanding.\\n\"\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates relevant experience with '\n",
      "              'RAG pipelines, but the explanation could be improved for '\n",
      "              'clarity and depth.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. **Clarity and Structure:** The explanation of the RAG '\n",
      "              'pipeline could be clearer. While the candidate describes '\n",
      "              'breaking down answers into searchable strings, the overall flow '\n",
      "              'of the explanation is somewhat convoluted. A more structured '\n",
      "              'approach, such as clearly delineating the steps involved in the '\n",
      "              'RAG pipeline, would enhance understanding.\\n'\n",
      "              '\\n'\n",
      "              '2. **Technical Detail:** The candidate mentions using a '\n",
      "              'retrieval-augmented generation (RAG) pipeline for an accuracy '\n",
      "              'verifier but does not sufficiently explain how the sentiment '\n",
      "              'analysis component interacts with the RAG pipeline. Clarifying '\n",
      "              'this relationship would provide a more comprehensive '\n",
      "              \"understanding of the system's functionality.\\n\"\n",
      "              '\\n'\n",
      "              '3. **Integration of Retrieved Content:** The answer states that '\n",
      "              'the RAG pipeline retrieves content from Google and uses it to '\n",
      "              'provide context for the LLM. However, it lacks detail on how '\n",
      "              \"this retrieved content is integrated into the LLM's context. \"\n",
      "              'More specifics on this integration process would strengthen the '\n",
      "              'explanation.\\n'\n",
      "              '\\n'\n",
      "              '4. **Reference to Source Material:** The candidate references a '\n",
      "              'paper by Google on Query Decomposition but does not provide '\n",
      "              'enough context on how this technique specifically applies to '\n",
      "              'their implementation. A brief explanation of how this technique '\n",
      "              'was adapted for their project would enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates relevant experience '\n",
      "              'with RAG pipelines, the explanation could benefit from improved '\n",
      "              'clarity and depth in certain areas.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. **Model Selection:** The candidate mentions choosing \"JATGBD '\n",
      "              '4.0 mini\" for the RAG pipeline, but there is no widely '\n",
      "              'recognized model by that name in the context of large language '\n",
      "              'models. This could be a transcription error or a '\n",
      "              \"misunderstanding. The candidate should clarify the model's name \"\n",
      "              'and its relevance to the task.\\n'\n",
      "              '\\n'\n",
      "              '2. **Prompt Engineering:** The candidate describes the '\n",
      "              'challenges faced with grammatical and typographical errors in '\n",
      "              'the speech-to-text conversion. While this is a valid point, the '\n",
      "              'explanation lacks detail on how the prompts were specifically '\n",
      "              'designed to address these issues. More information on the types '\n",
      "              'of prompts used and their effectiveness would enhance the '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              '3. **Integration of Retrieved Content:** The answer mentions '\n",
      "              'that the RAG pipeline retrieves content from Google and uses it '\n",
      "              'to verify candidate answers. However, it does not clearly '\n",
      "              \"explain how this retrieved content is integrated into the LLM's \"\n",
      "              'context. Providing more specifics on the integration process '\n",
      "              'would strengthen the explanation.\\n'\n",
      "              '\\n'\n",
      "              '4. **Comparison with Other Models:** The candidate states that '\n",
      "              'they did not choose a more sophisticated model like \"Claude '\n",
      "              'SONET 3.5\" because it was not necessary. However, the reasoning '\n",
      "              'could be more robust. It would be beneficial to elaborate on '\n",
      "              'the specific requirements of the project that justified the '\n",
      "              'choice of a less powerful model.\\n'\n",
      "              '\\n'\n",
      "              '5. **Overall Clarity:** The response could benefit from '\n",
      "              'improved clarity and organization. The explanation of the '\n",
      "              'integration of the RAG pipeline with the LLM is somewhat '\n",
      "              'convoluted, making it difficult to follow. A more structured '\n",
      "              'approach would enhance understanding.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 70%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. **Experience with Optimization:** The candidate states that '\n",
      "              'they have not optimized any model, which directly answers the '\n",
      "              'question but lacks depth. It would be beneficial to elaborate '\n",
      "              'on any considerations or evaluations made regarding model '\n",
      "              'performance, even if no formal optimization was conducted.\\n'\n",
      "              '\\n'\n",
      "              '2. **Model Selection Justification:** The candidate mentions '\n",
      "              'choosing Claude 3.5 Sonnet and ChatGPT 4.0 Mini based on their '\n",
      "              'intelligence, cost-effectiveness, and speed. However, the '\n",
      "              'explanation could be improved by providing more context on how '\n",
      "              'these factors were specifically evaluated in relation to the '\n",
      "              'project requirements. For instance, discussing any performance '\n",
      "              'metrics or benchmarks that influenced the decision would '\n",
      "              'strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              '3. **Integration of Services:** The candidate describes using '\n",
      "              'Hume AI for sentiment analysis but does not clarify how this '\n",
      "              'service integrates with the overall system. More detail on how '\n",
      "              'the outputs from Hume AI are utilized in conjunction with the '\n",
      "              'LLMs would enhance the understanding of the workflow.\\n'\n",
      "              '\\n'\n",
      "              '4. **Final Verdict Process:** The explanation of how the final '\n",
      "              'verdict is derived from the outputs of the accuracy verifier '\n",
      "              'and sentiment analysis is somewhat vague. Providing more '\n",
      "              'specifics on the decision-making process or criteria used to '\n",
      "              'arrive at the final verdict would add clarity.\\n'\n",
      "              '\\n'\n",
      "              '5. **General Clarity and Structure:** The overall structure of '\n",
      "              'the response could be improved for better readability. Breaking '\n",
      "              'down the answer into distinct sections (e.g., model selection, '\n",
      "              'integration, and final verdict) would help convey the '\n",
      "              'information more clearly.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-1------------------------\n",
      "---------------------------1724629705-2------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9236462, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9541237, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882997, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98761624, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down. The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector, that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1319}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.989057, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last, iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1238}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline for accuracy verification in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in natural language processing applications\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Prompt engineering techniques for language model integration in speech to text conversion\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Score: 85%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentioned using a RAG pipeline for an '\n",
      "              'automated screening interview agent, which is a relevant '\n",
      "              'application of RAG. However, the explanation of how the RAG '\n",
      "              'pipeline is implemented could be clearer. The candidate states '\n",
      "              'that the answer is broken down into \"searchable strings\" and '\n",
      "              'then searched on Google. While this is a valid approach, it '\n",
      "              'would be beneficial to clarify how the retrieved articles are '\n",
      "              \"processed and integrated into the LLM's context. \\n\"\n",
      "              '\\n'\n",
      "              '2. The candidate refers to the \"accuracy verifier\" but does not '\n",
      "              'explain how the sentiment analysis component interacts with the '\n",
      "              'RAG pipeline. A brief mention of how sentiment analysis '\n",
      "              'contributes to the overall accuracy verification process would '\n",
      "              'enhance the response.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of retrieving the contents of the first two '\n",
      "              'articles could be more precise. It would be helpful to specify '\n",
      "              'how the information from these articles is utilized to assess '\n",
      "              \"the accuracy of the candidate's answers.\\n\"\n",
      "              '\\n'\n",
      "              \"4. The candidate's description of providing an accuracy \"\n",
      "              'percentage is relevant, but it lacks detail on the methodology '\n",
      "              'used to calculate this percentage. Including information on the '\n",
      "              'criteria or metrics used for this evaluation would strengthen '\n",
      "              'the answer.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in AI, but additional '\n",
      "              'clarity and detail in the explanation would improve the '\n",
      "              'response.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Score: 90%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's explanation of breaking down the answer into \"\n",
      "              '\"searchable strings\" is a valid approach, but it could benefit '\n",
      "              'from more clarity on how these strings are generated and how '\n",
      "              'they relate to the context of the original answer. The mention '\n",
      "              'of using Google for retrieval is appropriate, but elaborating '\n",
      "              'on how the information from the retrieved articles is processed '\n",
      "              \"and integrated into the LLM's context would enhance \"\n",
      "              'understanding.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discusses the retrieval of information from '\n",
      "              'the first two web pages but does not specify how this '\n",
      "              'information is utilized to assess the accuracy of the '\n",
      "              \"candidate's answers. Providing details on the methodology for \"\n",
      "              \"integrating this information into the LLM's context would \"\n",
      "              'strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentions using cosine similarity to find '\n",
      "              'relevant portions of the vector store, which is a good '\n",
      "              'technique. However, it would be beneficial to explain how this '\n",
      "              'process contributes to improving the quality of the answers '\n",
      "              'fetched. Clarifying the relationship between the retrieved '\n",
      "              'context and the accuracy verification process would provide a '\n",
      "              'more comprehensive view.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              'a strong point, but the candidate could elaborate on how this '\n",
      "              'technique specifically addresses the challenges faced in their '\n",
      "              'implementation. Providing examples or insights from the paper '\n",
      "              'would add depth to the explanation.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in AI, but additional '\n",
      "              'clarity and detail in the explanation would improve the '\n",
      "              'response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Score: 88%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's choice of model, JATGBD 4.0 mini, is \"\n",
      "              'justified based on its balance of intelligence, '\n",
      "              'cost-effectiveness, and speed. However, the mention of Claude '\n",
      "              'SONET 3.5 as the \"most intelligent LLM till now\" could be '\n",
      "              'misleading. While Claude SONET 3.5 is indeed a strong model, '\n",
      "              'the context of \"most intelligent\" can vary based on specific '\n",
      "              'tasks and metrics. It would be beneficial to clarify that the '\n",
      "              'choice of model should align with the specific requirements of '\n",
      "              'the task rather than solely on perceived intelligence.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discusses the challenges faced during prompt '\n",
      "              'engineering, particularly regarding grammatical and '\n",
      "              'typographical errors in the transcribed text. While this is a '\n",
      "              'valid point, it would enhance the response to provide specific '\n",
      "              'examples of the types of prompts used to address these issues. '\n",
      "              'This would give a clearer picture of the prompt engineering '\n",
      "              'process and its effectiveness.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of how the LLM was prompted to overlook '\n",
      "              'grammatical errors is relevant, but it lacks detail on the '\n",
      "              'strategies employed to ensure that the model still understood '\n",
      "              'the intended meaning. Providing insights into the types of '\n",
      "              \"prompts or adjustments made to the model's behavior would \"\n",
      "              'strengthen this part of the answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions that the LLM was prompted to make '\n",
      "              'sense of words that were not properly converted but were close '\n",
      "              'to the actual word. It would be helpful to elaborate on how '\n",
      "              'this was achieved, such as whether synonyms or context clues '\n",
      "              \"were used to guide the model's understanding.\\n\"\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges faced. However, additional clarity and detail in the '\n",
      "              'explanation would improve the response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Score: 80%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is accurate in the context of not having performed '\n",
      "              'fine-tuning. However, the question specifically asks about '\n",
      "              'optimizing model performance in terms of speed and cost '\n",
      "              'efficiency. The candidate could have discussed their '\n",
      "              'decision-making process in selecting models based on these '\n",
      "              'criteria, which they did to some extent, but it could have been '\n",
      "              'more explicit.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPT 4.0 Mini for accuracy '\n",
      "              'verification. While they provide reasoning for these choices, '\n",
      "              'they do not elaborate on how these models were optimized for '\n",
      "              'speed and cost efficiency beyond stating that 4.0 Mini has the '\n",
      "              'right balance. More detail on the specific optimizations or '\n",
      "              'considerations taken into account when selecting these models '\n",
      "              'would enhance the response.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate does not clarify how the '\n",
      "              'integration of this service impacts overall model performance '\n",
      "              'in terms of speed and cost. Discussing the trade-offs or '\n",
      "              'benefits of using an external service versus an in-house '\n",
      "              'solution could provide a more comprehensive view.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions feeding the results from the accuracy '\n",
      "              'verifier and sentiment analysis back into Claude Sonnet 3.5 for '\n",
      "              'final evaluation. However, they do not explain how this process '\n",
      "              'contributes to optimizing model performance. Clarifying how '\n",
      "              'this integration affects the overall efficiency and '\n",
      "              'effectiveness of the model would strengthen the answer.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's assertion that Claude 3.5 Sonnet is the \"\n",
      "              '\"most intelligent LLM till date\" could be misleading without '\n",
      "              'context. While it may be a strong model, the performance can '\n",
      "              'vary based on specific tasks and metrics. It would be '\n",
      "              'beneficial to clarify that the choice of model should align '\n",
      "              'with the specific requirements of the task rather than solely '\n",
      "              'on perceived intelligence.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of model selection and application, additional clarity and '\n",
      "              'detail regarding optimization for speed and cost efficiency '\n",
      "              'would improve the response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-2------------------------\n",
      "---------------------------1724629705-3------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9163557, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.95652884, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98824286, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.988279, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down the answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of these six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1329}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9886467, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1237}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Prompt engineering for language model integration in RAG pipeline\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and other language model models in terms of intelligence, speed, and cost effectiveness.', '2. Review of ChatGPD 4.0 Mini and its performance in accuracy verification compared to other similar models.']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentioned they are studying artificial '\n",
      "              'intelligence at the University of Kent and are working on an '\n",
      "              'automated screening interview agent for their dissertation. '\n",
      "              'This is relevant and shows their engagement with AI projects.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of the RAG pipeline is mostly accurate, but '\n",
      "              'there are some areas that could be clarified. The candidate '\n",
      "              'states that the RAG pipeline is used as an \"accuracy verifier.\" '\n",
      "              'While RAG can enhance the accuracy of responses by providing '\n",
      "              'up-to-date information, it might be more precise to describe it '\n",
      "              'as a method to augment the generation of responses rather than '\n",
      "              'solely as an accuracy verifier.\\n'\n",
      "              '\\n'\n",
      "              \"3. The candidate describes breaking down the candidate's \"\n",
      "              'answers into \"searchable strings\" and searching on Google for '\n",
      "              'articles. This is a practical application of RAG, but it would '\n",
      "              'be beneficial to mention how the retrieved articles are '\n",
      "              'processed or ranked before being used in the LLM context. The '\n",
      "              'mention of using the first two articles could imply a lack of a '\n",
      "              're-ranking mechanism, which is often a critical part of RAG '\n",
      "              'systems to ensure the most relevant information is '\n",
      "              'prioritized.\\n'\n",
      "              '\\n'\n",
      "              '4. The explanation of how the LLM uses the retrieved content to '\n",
      "              \"verify the candidate's answers is a good demonstration of \"\n",
      "              'understanding the RAG process. However, the candidate could '\n",
      "              'have elaborated on how the accuracy percentage is calculated '\n",
      "              'based on the retrieved information, which would provide a '\n",
      "              'clearer picture of the implementation.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a real-world project, '\n",
      "              'but some clarifications and additional details would enhance '\n",
      "              'the explanation.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's approach to improving context and retrieval \"\n",
      "              'quality by breaking down answers into searchable strings is a '\n",
      "              'valid strategy. However, the explanation could benefit from '\n",
      "              'more clarity on how the retrieved articles are processed or '\n",
      "              'ranked before being used in the LLM context. The candidate '\n",
      "              'mentions using the first two articles, which may imply a lack '\n",
      "              'of a re-ranking mechanism that is often critical in RAG systems '\n",
      "              'to ensure the most relevant information is prioritized.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that the LLM uses cosine similarity to '\n",
      "              'find relevant portions of the vector store. While this is a '\n",
      "              'common method, it would be helpful to elaborate on how the '\n",
      "              'cosine similarity is applied in practice and how it contributes '\n",
      "              'to the overall accuracy verification process.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of drawing context from the first two web pages '\n",
      "              'is a practical application, but it could be improved by '\n",
      "              'discussing how the information from these pages is integrated '\n",
      "              \"into the LLM's response generation. This would provide a \"\n",
      "              'clearer picture of the implementation and its effectiveness.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate references a paper by Google on Query '\n",
      "              'Decomposition, which is a good point. However, it would '\n",
      "              'strengthen the answer to briefly explain how this technique '\n",
      "              'specifically addresses the challenges faced in their '\n",
      "              'implementation, rather than just stating that it overcomes '\n",
      "              'shortcomings.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a real-world project, '\n",
      "              'but additional details and clarifications would enhance the '\n",
      "              'explanation.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions selecting \"JATGBD 4.0 mini\" for model '\n",
      "              'selection, which appears to be a transcription error. The '\n",
      "              'correct model name is likely \"GPT-4o mini.\" This should not be '\n",
      "              'considered a fault of the candidate, but it does affect the '\n",
      "              'clarity of the response.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that they chose GPT-4o mini for its '\n",
      "              '\"balance of intelligence and cost-effectiveness.\" While this is '\n",
      "              'a valid point, it would be beneficial to provide more context '\n",
      "              'on how this model specifically meets the needs of the RAG '\n",
      "              'pipeline compared to other models, such as Claude 3.5 Sonnet. '\n",
      "              'The mention of Claude SONET 3.5 being the \"most intelligent '\n",
      "              'LLM\" is subjective and could be clarified with more objective '\n",
      "              'comparisons.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate discusses challenges related to grammatical '\n",
      "              'and typographical errors in the transcribed text. While this is '\n",
      "              'a relevant point, it would enhance the answer to elaborate on '\n",
      "              'how these errors specifically impacted the performance of the '\n",
      "              'RAG pipeline and the strategies used to mitigate these issues.\\n'\n",
      "              '\\n'\n",
      "              '4. The explanation of prompt engineering is somewhat vague. The '\n",
      "              'candidate mentions writing several prompts but does not provide '\n",
      "              'specific examples or details on how these prompts were '\n",
      "              'structured to address the challenges faced. More detail on the '\n",
      "              'prompt engineering process would strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's mention of using prompts to overlook \"\n",
      "              'grammatical errors is a good point, but it could be improved by '\n",
      "              \"discussing how the model's responses were evaluated for \"\n",
      "              'accuracy after these adjustments were made.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM, but '\n",
      "              'additional details and clarifications would enhance the '\n",
      "              'explanation.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 70%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is accurate in the context of their experience. However, '\n",
      "              'they could have elaborated on the importance of model selection '\n",
      "              'and how it contributes to performance optimization in terms of '\n",
      "              'speed and cost efficiency.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPT 4.0 Mini for accuracy '\n",
      "              'verification. While they provide reasoning for their choices, '\n",
      "              'they could have included more specific details on how these '\n",
      "              'models were evaluated for speed and cost efficiency, '\n",
      "              'particularly in comparison to other models.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate does not discuss how the '\n",
      "              'integration of this service impacts overall model performance, '\n",
      "              'speed, or cost efficiency. This omission affects the '\n",
      "              'completeness of their answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions feeding the results from the accuracy '\n",
      "              'verifier and sentiment analysis back into Claude Sonnet 3.5 for '\n",
      "              'final evaluation. While this demonstrates a good understanding '\n",
      "              'of the workflow, they could have provided more insight into how '\n",
      "              'this process contributes to optimizing model performance.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's assertion that Claude 3.5 Sonnet is the \"\n",
      "              '\"most intelligent LLM\" is subjective and could be better '\n",
      "              'supported with objective comparisons or metrics, especially in '\n",
      "              'light of the additional context provided about the performance '\n",
      "              'of GPT-4o compared to Claude 3.5 Sonnet.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of model selection and its implications for performance, '\n",
      "              'additional details and clarifications regarding optimization '\n",
      "              'strategies, evaluation criteria, and the impact of their '\n",
      "              'choices on speed and cost efficiency would enhance the '\n",
      "              'response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "Error in chat: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n",
      "---------------------------1724629705-3------------------------\n",
      "---------------------------1724629705-4------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.91965765, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9576909, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98828125, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will be retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 898}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98759794, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down. The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector, that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1323}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9887826, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Minis. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1238}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Query Decomposition technique for improving retrieval quality in RAG pipeline\"', '2. \"Cosine similarity in LLM for accuracy verification in information retrieval\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentioned they are studying artificial '\n",
      "              'intelligence at the University of Kent and are working on an '\n",
      "              'automated screening interview agent for their dissertation. '\n",
      "              'This is relevant and shows their engagement with AI projects.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of the RAG pipeline is mostly accurate, but '\n",
      "              'there are some areas that could be clarified. The candidate '\n",
      "              'states that the RAG pipeline is used as an \"accuracy verifier.\" '\n",
      "              'While RAG can enhance the accuracy of responses by providing '\n",
      "              'up-to-date information, it might be more precise to say that it '\n",
      "              'augments the generation process rather than serving solely as '\n",
      "              'an accuracy verifier.\\n'\n",
      "              '\\n'\n",
      "              \"3. The candidate describes breaking down the candidate's \"\n",
      "              'answers into \"searchable strings\" and searching on Google for '\n",
      "              'articles. This is a practical application of RAG, but it would '\n",
      "              'be beneficial to mention how the retrieved articles are '\n",
      "              \"processed and integrated into the LLM's context. The \"\n",
      "              'explanation could be clearer regarding how the information from '\n",
      "              'the articles is utilized to assess the accuracy of the '\n",
      "              \"candidate's responses.\\n\"\n",
      "              '\\n'\n",
      "              '4. The candidate mentions providing an accuracy percentage '\n",
      "              'based on the verification process. While this is a good '\n",
      "              'application of the RAG pipeline, it would be helpful to '\n",
      "              'elaborate on how this percentage is calculated and what metrics '\n",
      "              'are used to determine accuracy.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in their project, but some '\n",
      "              'aspects of the explanation could be more detailed for clarity.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's approach to improving context and retrieval \"\n",
      "              'quality by breaking down answers into searchable strings is a '\n",
      "              'valid strategy. However, the explanation could be clearer '\n",
      "              'regarding how these strings are generated and how they relate '\n",
      "              'to the original answer.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions using Google to retrieve the first '\n",
      "              'two articles and drawing context from them. While this is a '\n",
      "              'practical method, it would be beneficial to clarify how the '\n",
      "              'information from these articles is processed and integrated '\n",
      "              \"into the LLM's context. The answer lacks detail on how the \"\n",
      "              'retrieved content is utilized to assess the accuracy of the '\n",
      "              \"candidate's responses.\\n\"\n",
      "              '\\n'\n",
      "              '3. The candidate states that the LLM uses cosine similarity to '\n",
      "              'find relevant portions of the vector store. While this is a '\n",
      "              'common technique, it would be helpful to elaborate on how this '\n",
      "              'process works in the context of their implementation and how it '\n",
      "              'contributes to improving answer quality.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of a paper by Google on Query Decomposition is a '\n",
      "              'good reference, but the candidate could have provided more '\n",
      "              'context on how this technique specifically addresses the '\n",
      "              'challenges they faced in their project.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's explanation of providing an accuracy \"\n",
      "              'percentage based on the verification process is interesting, '\n",
      "              'but it lacks detail on the metrics or methods used to calculate '\n",
      "              'this percentage. More information on how accuracy is quantified '\n",
      "              'would enhance the response.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in their project, but some '\n",
      "              'aspects of the explanation could be more detailed for clarity.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions selecting \"JATGBD 4.0 mini\" for the '\n",
      "              'RAG pipeline implementation. However, it appears there may be a '\n",
      "              'transcription error, as the context provided refers to \"GPT-4o '\n",
      "              'mini.\" This could lead to confusion regarding the model being '\n",
      "              \"discussed. It's important to clarify the correct model name to \"\n",
      "              'ensure accurate communication.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that they chose GPT-4o mini for its '\n",
      "              '\"balance of intelligence and cost-effectiveness.\" While this is '\n",
      "              'a valid point, it would be beneficial to provide more context '\n",
      "              'on how this balance was determined, especially in comparison to '\n",
      "              'Claude 3.5 Sonnet, which is noted as a more sophisticated '\n",
      "              'model. The reasoning behind the choice could be elaborated to '\n",
      "              'strengthen the argument.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is somewhat vague. The '\n",
      "              'candidate mentions writing several prompts to help the LLM '\n",
      "              'overlook grammatical errors and make sense of poorly converted '\n",
      "              'words. However, it would be helpful to provide specific '\n",
      "              'examples of these prompts or describe the process in more '\n",
      "              'detail to illustrate how they effectively addressed the '\n",
      "              'challenges faced.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate discusses the challenges of grammatical and '\n",
      "              'typographical errors in the transcribed text. While they '\n",
      "              'mention prompting the LLM to overlook these issues, it would be '\n",
      "              \"beneficial to elaborate on how the model's responses were \"\n",
      "              'adjusted or improved as a result of this prompting. More detail '\n",
      "              'on the effectiveness of this approach would enhance the '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's mention of the challenges faced during the \"\n",
      "              'integration of the RAG pipeline is relevant, but the '\n",
      "              'explanation could be clearer regarding how these challenges '\n",
      "              'were specifically addressed. Providing more detail on the '\n",
      "              'strategies used to overcome these challenges would improve the '\n",
      "              'overall clarity of the response. \\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of the integration of the RAG pipeline with the LLM, some areas '\n",
      "              'of the explanation could benefit from additional detail and '\n",
      "              'clarification.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 70%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is accurate in the context of their experience. However, '\n",
      "              'they could have elaborated on the importance of model selection '\n",
      "              'and how it relates to performance optimization, particularly in '\n",
      "              'terms of speed and cost efficiency.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPT 4.0 Mini for accuracy '\n",
      "              'verification. While they provide reasoning for these choices, '\n",
      "              'it would be beneficial to include specific metrics or '\n",
      "              'comparisons that led to these decisions, especially regarding '\n",
      "              'speed and cost efficiency.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate does not discuss how the '\n",
      "              'integration of this service impacts overall model performance '\n",
      "              'or cost. More detail on the implications of using external '\n",
      "              'services would enhance the response.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions feeding the results from the accuracy '\n",
      "              'verifier and sentiment analysis back into Claude Sonnet 3.5 to '\n",
      "              \"make sense of the candidate's answers. However, they do not \"\n",
      "              'clarify how this process contributes to optimizing model '\n",
      "              'performance or improving the final verdict. More detail on this '\n",
      "              'integration would strengthen the answer.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's mention of the models being chosen based on \"\n",
      "              'their intelligence and cost-effectiveness is valid, but they '\n",
      "              'could have provided more context on how they evaluated these '\n",
      "              'factors. For instance, discussing any specific metrics or '\n",
      "              'benchmarks used to assess speed and cost efficiency would '\n",
      "              'improve the clarity of their reasoning.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of model selection and its implications, some areas of the '\n",
      "              'explanation could benefit from additional detail and clarity '\n",
      "              'regarding performance optimization.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-4------------------------\n",
      "---------------------------1724629705-5------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9063586, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello.', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.95032483, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.01251170877367258}, {'name': 'Adoration', 'score': 0.008131410926580429}, {'name': 'Aesthetic Appreciation', 'score': 0.016512403264641762}, {'name': 'Amusement', 'score': 0.0907270684838295}, {'name': 'Anger', 'score': 0.0005581923178397119}, {'name': 'Annoyance', 'score': 0.014846810139715672}, {'name': 'Anxiety', 'score': 0.0011913252528756857}, {'name': 'Awe', 'score': 0.008423087187111378}, {'name': 'Awkwardness', 'score': 0.06354702264070511}, {'name': 'Boredom', 'score': 0.10851363092660904}, {'name': 'Calmness', 'score': 0.31933408975601196}, {'name': 'Concentration', 'score': 0.016252553090453148}, {'name': 'Confusion', 'score': 0.029891543090343475}, {'name': 'Contemplation', 'score': 0.02187623828649521}, {'name': 'Contempt', 'score': 0.018504859879612923}, {'name': 'Contentment', 'score': 0.08336649090051651}, {'name': 'Craving', 'score': 0.0006236955523490906}, {'name': 'Desire', 'score': 0.000760009977966547}, {'name': 'Determination', 'score': 0.0022584907710552216}, {'name': 'Disappointment', 'score': 0.0025702782440930605}, {'name': 'Disapproval', 'score': 0.007185309659689665}, {'name': 'Disgust', 'score': 0.0015889910282567143}, {'name': 'Distress', 'score': 0.0006113385898061097}, {'name': 'Doubt', 'score': 0.00666783144697547}, {'name': 'Ecstasy', 'score': 0.0021341179963201284}, {'name': 'Embarrassment', 'score': 0.0033599617891013622}, {'name': 'Empathic Pain', 'score': 0.0008289636461995542}, {'name': 'Enthusiasm', 'score': 0.09442762285470963}, {'name': 'Entrancement', 'score': 0.006641392130404711}, {'name': 'Envy', 'score': 0.0008029191521927714}, {'name': 'Excitement', 'score': 0.05306636542081833}, {'name': 'Fear', 'score': 0.00027591444086283445}, {'name': 'Gratitude', 'score': 0.010404795408248901}, {'name': 'Guilt', 'score': 0.00038117836811579764}, {'name': 'Horror', 'score': 0.00014820862270426005}, {'name': 'Interest', 'score': 0.22310522198677063}, {'name': 'Joy', 'score': 0.08868356049060822}, {'name': 'Love', 'score': 0.005399726331233978}, {'name': 'Nostalgia', 'score': 0.004958468955010176}, {'name': 'Pain', 'score': 0.00010659849067451432}, {'name': 'Pride', 'score': 0.0036682302597910166}, {'name': 'Realization', 'score': 0.053931184113025665}, {'name': 'Relief', 'score': 0.0077752480283379555}, {'name': 'Romance', 'score': 0.001546808984130621}, {'name': 'Sadness', 'score': 0.0003105304786004126}, {'name': 'Sarcasm', 'score': 0.025866815820336342}, {'name': 'Satisfaction', 'score': 0.05627468600869179}, {'name': 'Shame', 'score': 0.0010352996177971363}, {'name': 'Surprise (negative)', 'score': 0.017590997740626335}, {'name': 'Surprise (positive)', 'score': 0.1599823534488678}, {'name': 'Sympathy', 'score': 0.002042335458099842}, {'name': 'Tiredness', 'score': 0.004060816951096058}, {'name': 'Triumph', 'score': 0.00503922114148736}], 'sentiment': [{'name': '1', 'score': 0.0005626916536130011}, {'name': '2', 'score': 0.0006312259938567877}, {'name': '3', 'score': 0.0011379551142454147}, {'name': '4', 'score': 0.0038008831907063723}, {'name': '5', 'score': 0.36965522170066833}, {'name': '6', 'score': 0.2254335731267929}, {'name': '7', 'score': 0.13478495180606842}, {'name': '8', 'score': 0.11552125215530396}, {'name': '9', 'score': 0.11766091734170914}], 'toxicity': [{'name': 'identity_hate', 'score': 0.002974089467898011}, {'name': 'insult', 'score': 0.0027754041366279125}, {'name': 'obscene', 'score': 0.002369341906160116}, {'name': 'severe_toxic', 'score': 0.0023268223740160465}, {'name': 'threat', 'score': 0.002879881300032139}, {'name': 'toxic', 'score': 0.006132675334811211}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882707, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9880586, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down the answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9888114, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1237}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"improving context retrieval quality RAG pipeline\"', '2. \"Google Query Decomposition paper accuracy verification\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"Comparison between ChatGPD 4.0 and ChatGPD 4.0 Mini LLM models\"']\n",
      "\n",
      "\n",
      "documents added to retriever\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions using a RAG pipeline for an automated '\n",
      "              'screening interview agent, which is relevant. However, the '\n",
      "              'explanation lacks clarity in how the RAG pipeline specifically '\n",
      "              'enhances the accuracy verification process. It would be '\n",
      "              'beneficial to elaborate on the role of the RAG pipeline in the '\n",
      "              'context of the overall system architecture.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that the accuracy verifier breaks down '\n",
      "              'the answer into \"searchable strings\" and searches on Google. '\n",
      "              'While this is a valid approach, it would be more accurate to '\n",
      "              'specify that the retrieval component of the RAG pipeline '\n",
      "              'typically involves querying a knowledge base or document store '\n",
      "              'rather than a general search engine like Google. This could '\n",
      "              'lead to confusion about the intended use of RAG in a structured '\n",
      "              'environment.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of how the retrieved articles are used to '\n",
      "              'provide context for the LLM is somewhat vague. It would '\n",
      "              'strengthen the answer to clarify how the information from the '\n",
      "              \"articles is integrated into the LLM's processing to assess the \"\n",
      "              \"candidate's answer.\\n\"\n",
      "              '\\n'\n",
      "              '4. The candidate mentions providing an accuracy percentage '\n",
      "              'based on the verification process, which is a good point. '\n",
      "              'However, it would be helpful to explain how this percentage is '\n",
      "              'calculated and what metrics are used to determine accuracy.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates relevant experience '\n",
      "              'with RAG pipelines, the answer could benefit from more precise '\n",
      "              'language and a clearer explanation of the processes involved.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              \"1. The candidate's explanation of breaking down the answer into \"\n",
      "              '\"searchable strings\" is a valid approach, but it could be more '\n",
      "              'precise. The retrieval component of a RAG pipeline typically '\n",
      "              'involves querying a structured knowledge base or document store '\n",
      "              'rather than a general search engine like Google. This could '\n",
      "              'lead to confusion about the intended use of RAG in a structured '\n",
      "              'environment.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions retrieving content from the first two '\n",
      "              'articles found on Google. While this method can provide '\n",
      "              'context, it would be beneficial to clarify how the information '\n",
      "              \"from these articles is integrated into the LLM's processing. \"\n",
      "              'The answer could be strengthened by explaining how the '\n",
      "              'retrieved information is used to assess the accuracy of the '\n",
      "              \"candidate's response.\\n\"\n",
      "              '\\n'\n",
      "              '3. The candidate states that the LLM uses cosine similarity to '\n",
      "              'find relevant portions of the vector store. This is a good '\n",
      "              'point, but it would enhance the answer to elaborate on how this '\n",
      "              'process works in practice and how it contributes to improving '\n",
      "              'the quality of the answers fetched.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of an accuracy percentage based on the '\n",
      "              'verification process is a positive aspect. However, the '\n",
      "              'candidate should explain how this percentage is calculated and '\n",
      "              'what specific metrics are used to determine accuracy. This '\n",
      "              'would provide a clearer understanding of the evaluation '\n",
      "              'process.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates relevant experience with '\n",
      "              'RAG pipelines, but the answer could benefit from more precise '\n",
      "              'language and a clearer explanation of the processes involved.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 78%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions selecting \"JATGBD 4.0 mini\" for the '\n",
      "              'RAG pipeline, which appears to be a transcription error. It is '\n",
      "              'likely intended to refer to \"GPT-4o mini.\" This could lead to '\n",
      "              \"confusion regarding the model's capabilities and comparisons \"\n",
      "              'with other models like Claude 3.5 Sonnet. \\n'\n",
      "              '\\n'\n",
      "              '2. The candidate states that they chose GPT-4o mini for its '\n",
      "              '\"balance of intelligence and cost-effectiveness.\" While this is '\n",
      "              'a valid point, it would be beneficial to provide more context '\n",
      "              'on how this balance specifically impacts the performance of the '\n",
      "              'RAG pipeline in terms of accuracy verification.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation regarding prompt engineering is somewhat '\n",
      "              'vague. The candidate mentions writing several prompts to help '\n",
      "              'the LLM overlook grammatical errors in the transcribed text. It '\n",
      "              'would strengthen the answer to elaborate on the types of '\n",
      "              'prompts used and how they were structured to effectively guide '\n",
      "              'the model in understanding the context despite the errors.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate discusses the challenges faced with '\n",
      "              'grammatical and typographical errors in the transcribed text. '\n",
      "              'While they mention prompting the LLM to overlook these issues, '\n",
      "              'it would be helpful to clarify how successful this approach was '\n",
      "              'and whether any specific strategies were employed to improve '\n",
      "              \"the model's understanding of the intended meaning.\\n\"\n",
      "              '\\n'\n",
      "              \"5. The candidate's mention of not needing a more sophisticated \"\n",
      "              'model like Claude 3.5 Sonnet is valid, but it would enhance the '\n",
      "              'answer to briefly explain why the capabilities of GPT-4o mini '\n",
      "              'were deemed sufficient for the task at hand, particularly in '\n",
      "              \"the context of the RAG pipeline's requirements.\\n\"\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates relevant experience '\n",
      "              'and understanding of the RAG pipeline and model selection, the '\n",
      "              'answer could benefit from clearer explanations and more '\n",
      "              'specific details regarding the processes involved.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 70%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is a direct response to the question. However, the '\n",
      "              'question specifically asks about experience with optimizing '\n",
      "              'model performance in terms of speed and cost efficiency. The '\n",
      "              'candidate could have elaborated on how the selection of models '\n",
      "              '(Claude 3.5 Sonnet and ChatGPT 4.0 Mini) was influenced by '\n",
      "              'considerations of speed and cost efficiency, even if they did '\n",
      "              'not perform optimization themselves.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPT 4.0 Mini for accuracy '\n",
      "              'verification, citing their intelligence and cost-effectiveness. '\n",
      "              'While this is relevant, it would strengthen the answer to '\n",
      "              'provide specific examples of how these choices impacted the '\n",
      "              'overall performance of the project in terms of speed and cost '\n",
      "              'efficiency.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation regarding the use of Hume AI for sentiment '\n",
      "              'analysis lacks detail on how this choice contributes to '\n",
      "              'optimizing model performance. The candidate could have '\n",
      "              'discussed the trade-offs involved in using an external service '\n",
      "              'versus an in-house solution, particularly in terms of cost and '\n",
      "              'speed.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentions feeding the results from the accuracy '\n",
      "              'verifier and sentiment analysis back into Claude 3.5 Sonnet for '\n",
      "              'final evaluation. While this is a good point, it would be '\n",
      "              'beneficial to clarify how this integration affects the overall '\n",
      "              \"efficiency and effectiveness of the model's performance.\\n\"\n",
      "              '\\n'\n",
      "              \"5. The candidate's response could benefit from a more \"\n",
      "              'structured approach to discussing the optimization of model '\n",
      "              'performance, including specific metrics or outcomes that '\n",
      "              'demonstrate the effectiveness of their choices in terms of '\n",
      "              'speed and cost efficiency. \\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate provides relevant information '\n",
      "              'about model selection and integration, the answer lacks depth '\n",
      "              'in discussing optimization strategies and their impact on '\n",
      "              'performance.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-5------------------------\n",
      "---------------------------1724629705-6------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9236462, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9541237, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882818, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98833185, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.988415, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1237}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"Comparison between ChatGPD 4.0 and ChatGPD 4.0 Mini LLM\"']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline was implemented as '\n",
      "              'an accuracy verifier is mostly clear. However, the candidate '\n",
      "              'could have elaborated more on the specific methods or '\n",
      "              'technologies used in the RAG pipeline, such as the type of '\n",
      "              'language model (LM) employed or the specific algorithms for '\n",
      "              'sentiment analysis. This would have strengthened their '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              '3. The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more clearly articulated. It would be '\n",
      "              'beneficial to specify how the answers are processed before '\n",
      "              'being sent for retrieval, as this is a critical part of the RAG '\n",
      "              'pipeline.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of using Google to retrieve articles is '\n",
      "              'relevant, but it would be helpful to clarify how the retrieved '\n",
      "              \"content is integrated into the LLM's context. More detail on \"\n",
      "              'this integration process would enhance the understanding of the '\n",
      "              \"RAG pipeline's functionality.\\n\"\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'additional details and clarity in certain areas would improve '\n",
      "              'the response.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate effectively described the process of breaking '\n",
      "              'down answers into searchable strings, which is a key aspect of '\n",
      "              'improving retrieval quality in a RAG pipeline. However, the '\n",
      "              'explanation could benefit from more clarity on how the answers '\n",
      "              'are processed before being transformed into query strings. This '\n",
      "              'is a critical step that was not fully elaborated upon.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentioned using Google to retrieve articles, '\n",
      "              'which is relevant. However, they could have provided more '\n",
      "              'detail on how the retrieved content is integrated into the '\n",
      "              \"LLM's context. Specifically, explaining how the information \"\n",
      "              'from the 12 web pages is synthesized and utilized by the LLM '\n",
      "              \"would enhance the understanding of the RAG pipeline's \"\n",
      "              'functionality.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate referenced a paper by Google on Query '\n",
      "              'Decomposition, which adds credibility to their approach. '\n",
      "              'However, it would be beneficial to briefly explain how this '\n",
      "              'technique specifically addresses the challenges faced in their '\n",
      "              'implementation, as this would provide a clearer connection '\n",
      "              'between the theory and their practical application.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the RAG pipeline and its application in their project, but '\n",
      "              'additional details and clarity in certain areas would improve '\n",
      "              'the response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a clear rationale for selecting the '\n",
      "              'JATGBD 4.0 mini model, emphasizing its balance of intelligence, '\n",
      "              'cost-effectiveness, and speed. However, the mention of \"GPT 4.0 '\n",
      "              'mini\" seems to be a transcription error, as it should likely '\n",
      "              'refer to \"JATGBD 4.0 mini\" throughout. This could lead to '\n",
      "              'confusion regarding the model selection.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate stated that they did not require a more '\n",
      "              'sophisticated model like Claude SONET 3.5, which is a valid '\n",
      "              'point. However, it would have been beneficial to elaborate on '\n",
      "              'the specific criteria used to determine the sufficiency of the '\n",
      "              'JATGBD 4.0 mini model for their needs.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering was somewhat vague. '\n",
      "              'While the candidate mentioned writing several prompts to '\n",
      "              'address grammatical and typographical errors in the transcribed '\n",
      "              'text, they could have provided more detail on the types of '\n",
      "              'prompts used and how they specifically guided the LLM to handle '\n",
      "              'these errors. This would enhance the understanding of the '\n",
      "              'challenges faced in prompt engineering.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's mention of the challenges related to \"\n",
      "              'speech-to-text conversion is relevant, but they could have '\n",
      "              'elaborated on how these challenges impacted the overall '\n",
      "              'performance of the RAG pipeline. Providing specific examples of '\n",
      "              'how the errors affected the output would strengthen their '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges faced during implementation, but additional details '\n",
      "              'and clarity in certain areas would improve the response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which directly addresses the question about experience '\n",
      "              'with optimizing model performance. However, this could be seen '\n",
      "              'as a limitation in their experience, as they did not provide '\n",
      "              'any examples of optimization efforts, even if they were not '\n",
      "              'directly involved in fine-tuning.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate interpreted \"optimizing\" as \"fine-tuning,\" '\n",
      "              'which may not encompass all aspects of optimization, such as '\n",
      "              'improving speed and cost efficiency. They could have elaborated '\n",
      "              'on any considerations they made regarding model selection based '\n",
      "              'on these factors, even if they did not perform optimizations '\n",
      "              'themselves.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned selecting Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPD 4.0 Mini for the accuracy '\n",
      "              'verifier, but they did not explicitly discuss how these choices '\n",
      "              'contributed to optimizing performance in terms of speed and '\n",
      "              'cost efficiency. More detail on the decision-making process '\n",
      "              'regarding these aspects would enhance the response.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate did not explain how this choice '\n",
      "              'impacted the overall performance of the system in terms of '\n",
      "              'speed and cost. Providing insights into the cost-effectiveness '\n",
      "              'of using an external service versus an in-house solution would '\n",
      "              'have strengthened their answer.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's explanation of feeding the sentiment and \"\n",
      "              'accuracy verifier scores back into Claude Sonnet 3.5 is a good '\n",
      "              'point, but they did not clarify how this integration affected '\n",
      "              'the overall performance or efficiency of the model. More detail '\n",
      "              'on the implications of this integration would improve the '\n",
      "              'understanding of their approach to optimizing performance.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate provided relevant information '\n",
      "              'about model selection, they did not sufficiently address the '\n",
      "              'specific aspects of optimizing model performance, particularly '\n",
      "              'in terms of speed and cost efficiency. Additional details and '\n",
      "              'clarity in these areas would enhance their response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-6------------------------\n",
      "---------------------------1724629705-7------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.91568226, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9571514, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882246, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98795193, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down. The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector, that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1323}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9890401, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1224}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"Comparison between JATGBD 4.0 mini and Claude SONET 3.5 in terms of intelligence and cost effectiveness\"', '2. \"Challenges in prompt engineering for large language models in speech to text conversion\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be improved for '\n",
      "              'clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more clearly articulated. It would be '\n",
      "              'beneficial to specify how the answers are broken down and what '\n",
      "              'criteria are used for creating these searchable strings.\\n'\n",
      "              '   - The mention of \"the first two articles it will retrieve '\n",
      "              'the contents of the first two articles\" is somewhat repetitive '\n",
      "              'and could be streamlined for better readability.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate did not explicitly mention the specific '\n",
      "              'technologies or frameworks used to implement the RAG pipeline, '\n",
      "              'which could enhance the answer. Including this information '\n",
      "              'would provide a clearer picture of their technical skills and '\n",
      "              'experience.\\n'\n",
      "              '\\n'\n",
      "              '4. The explanation of how the LLM (Language Model) uses the '\n",
      "              \"retrieved content to verify the candidate's answers is a strong \"\n",
      "              'point, but it could benefit from a more detailed description of '\n",
      "              'the process and how the accuracy percentage is calculated.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'there are opportunities for improvement in clarity and detail.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant and detailed explanation '\n",
      "              'of how they improved the context and retrieval quality of the '\n",
      "              \"RAG pipeline. They described breaking down the candidate's \"\n",
      "              'answers into searchable strings, which is a key aspect of '\n",
      "              'enhancing retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentioned using six query strings to search '\n",
      "              'Google and retrieving context from the first two web pages, '\n",
      "              'which is a solid approach. However, it would be beneficial to '\n",
      "              'clarify how they determined the six query strings and the '\n",
      "              'criteria used for their creation. This would enhance the '\n",
      "              'understanding of their methodology.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity to find relevant '\n",
      "              'portions is a strong point. However, the candidate could '\n",
      "              'elaborate on how cosine similarity is calculated and how it '\n",
      "              'contributes to the accuracy verification process.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a good inclusion, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would be helpful to briefly '\n",
      "              'explain how this technique specifically addressed the '\n",
      "              'challenges they faced in their implementation.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the RAG pipeline and effectively communicated their approach to '\n",
      "              'improving retrieval quality and context relevance. There are '\n",
      "              'minor areas for improvement in clarity and detail, but the core '\n",
      "              'concepts were well articulated.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant explanation regarding '\n",
      "              'model selection, stating that they chose JATGBD 4.0 mini due to '\n",
      "              'its balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion about the model '\n",
      "              'being discussed.\\n'\n",
      "              '\\n'\n",
      "              \"2. The candidate's rationale for not selecting a more \"\n",
      "              'sophisticated model like Claude SONET 3.5 is valid, but it '\n",
      "              'could benefit from further elaboration on the specific '\n",
      "              'requirements of their project that made JATGBD 4.0 mini a '\n",
      "              'better fit. This would provide more context for their '\n",
      "              'decision-making process.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is somewhat vague. '\n",
      "              'While the candidate mentions writing several prompts to address '\n",
      "              'grammatical and typographical errors in the speech-to-text '\n",
      "              'conversion, it would be helpful to provide specific examples of '\n",
      "              'the types of prompts used and how they effectively addressed '\n",
      "              'these issues. This would enhance the clarity of their '\n",
      "              'approach.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's mention of challenges faced during the \"\n",
      "              'integration of the RAG pipeline with the LLM is relevant, but '\n",
      "              'it could be more detailed. For instance, discussing how they '\n",
      "              'evaluated the effectiveness of their prompts or any iterative '\n",
      "              'processes they went through would provide a deeper insight into '\n",
      "              'their problem-solving strategies.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges involved, but there are areas where additional '\n",
      "              'detail and clarity could improve the response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated that they have not optimized any model, '\n",
      "              'which is accurate in the context of their specific project. '\n",
      "              'However, they could have elaborated on the importance of model '\n",
      "              'selection in optimizing performance, particularly in terms of '\n",
      "              'speed and cost efficiency. While they mentioned choosing models '\n",
      "              'based on intelligence, cost-effectiveness, and speed, a more '\n",
      "              'explicit connection to optimization strategies would strengthen '\n",
      "              'their response.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discussed the selection of Claude 3.5 Sonnet '\n",
      "              'and ChatGPD 4.0 Mini, highlighting their considerations for '\n",
      "              'model choice. However, they did not provide specific examples '\n",
      "              'of how these models were optimized for speed and cost '\n",
      "              'efficiency in their implementation. Including details about any '\n",
      "              'techniques or practices they employed to ensure efficient '\n",
      "              'performance would enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of Hume AI for sentiment analysis is relevant, '\n",
      "              'but the candidate did not discuss how the integration of this '\n",
      "              'external service impacted overall model performance in terms of '\n",
      "              'speed and cost. Providing insights into the trade-offs or '\n",
      "              'benefits of using an external service versus an in-house '\n",
      "              'solution would add depth to their response.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's explanation of feeding the sentiment and \"\n",
      "              'accuracy verifier scores back into Claude Sonnet 3.5 is a good '\n",
      "              'point, but they could have elaborated on how this process '\n",
      "              'contributes to optimizing the overall model performance. '\n",
      "              'Discussing any iterative improvements or adjustments made based '\n",
      "              'on the outputs would provide a clearer picture of their '\n",
      "              'optimization approach.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrated a solid understanding '\n",
      "              'of model selection and its implications for performance, there '\n",
      "              'are areas where additional detail and clarity regarding '\n",
      "              'optimization strategies could improve the response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-7------------------------\n",
      "---------------------------1724629705-8------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9233082, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9590267, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882978, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98819315, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9888263, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Minis. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1238}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_31892\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in LLM\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. How does Hume AI perform sentiment analysis directly from audio and video feed?']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Score: 85%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate mentions working on an automated screening '\n",
      "              'interview agent, which is relevant to the question about RAG '\n",
      "              'pipelines. However, the explanation of how the RAG pipeline is '\n",
      "              'implemented could be clearer. The candidate states that the RAG '\n",
      "              'pipeline is used as an \"accuracy verifier,\" but does not '\n",
      "              'elaborate on how the retrieval-augmented generation '\n",
      "              'specifically enhances the accuracy verification process beyond '\n",
      "              'breaking down answers into searchable strings.\\n'\n",
      "              '\\n'\n",
      "              '2. The description of the process involving sentiment analysis '\n",
      "              'and the retrieval of articles from Google is somewhat '\n",
      "              'convoluted. While the candidate indicates that the first two '\n",
      "              'articles are retrieved and used to provide context for the '\n",
      "              'language model (LM), the explanation lacks clarity on how this '\n",
      "              'directly contributes to verifying the accuracy of the '\n",
      "              \"candidate's answers.\\n\"\n",
      "              '\\n'\n",
      "              '3. The candidate does not explicitly mention any specific '\n",
      "              'challenges faced during the implementation of the RAG pipeline '\n",
      "              'or any results achieved, which could have strengthened their '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates relevant experience '\n",
      "              'with RAG pipelines, the explanation could benefit from more '\n",
      "              'clarity and detail regarding the implementation and its impact '\n",
      "              'on the project.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Score: 90%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provides a more detailed explanation of the '\n",
      "              'RAG pipeline implementation compared to the previous answer. '\n",
      "              \"They describe breaking down the candidate's answer into six \"\n",
      "              'query strings and using these to search Google, which enhances '\n",
      "              'the retrieval quality. This is a positive addition that '\n",
      "              'clarifies the process.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions storing the retrieved information in '\n",
      "              'a vector store and using cosine similarity to find relevant '\n",
      "              'portions, which adds depth to the explanation. However, they '\n",
      "              'could further elaborate on how this process specifically '\n",
      "              'improves the accuracy verification beyond just stating that it '\n",
      "              '\"vastly improves the quality of the answers fetched.\"\\n'\n",
      "              '\\n'\n",
      "              '3. While the candidate references a paper by Google on Query '\n",
      "              'Decomposition, they do not explicitly mention any specific '\n",
      "              'challenges faced during the implementation of the RAG pipeline. '\n",
      "              'Including such challenges and how they were addressed would '\n",
      "              'strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              '4. The answer could benefit from a clearer connection between '\n",
      "              'the retrieval process and the accuracy verification of the '\n",
      "              \"candidate's answers. While the candidate explains the retrieval \"\n",
      "              'process well, they could clarify how this directly impacts the '\n",
      "              'accuracy assessment.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and retrieval quality improvement, but '\n",
      "              'additional details on challenges faced and the direct impact on '\n",
      "              'accuracy verification would enhance the response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Score: 88%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provides a clear rationale for selecting the '\n",
      "              'JATGBD 4.0 mini model, emphasizing its balance of intelligence, '\n",
      "              'cost-effectiveness, and speed. However, the mention of Claude '\n",
      "              'SONET 3.5 as a more sophisticated model could be misleading '\n",
      "              'without context on why it was not chosen. It would be '\n",
      "              'beneficial to clarify that the choice was based on project '\n",
      "              'requirements rather than a direct comparison of capabilities.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of prompt engineering is relevant, but it '\n",
      "              'could be more concise. The candidate discusses the need to '\n",
      "              'prompt the LLM to overlook grammatical and typographical '\n",
      "              'errors, which is a valid challenge. However, they could enhance '\n",
      "              'this section by providing specific examples of the types of '\n",
      "              'prompts used or how they were structured to address these '\n",
      "              'issues.\\n'\n",
      "              '\\n'\n",
      "              '3. While the candidate mentions challenges related to '\n",
      "              'speech-to-text conversion errors, they do not elaborate on how '\n",
      "              'these challenges specifically impacted the integration of the '\n",
      "              'RAG pipeline with the LLM. Providing more detail on the '\n",
      "              'implications of these errors on the overall process would '\n",
      "              'strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              '4. The answer could benefit from a clearer connection between '\n",
      "              'the model selection and the integration with the RAG pipeline. '\n",
      "              \"While the candidate discusses the model's suitability, they do \"\n",
      "              'not explicitly state how it interacts with the RAG pipeline or '\n",
      "              'the specific benefits it brings to the integration process.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'model selection and prompt engineering in the context of a RAG '\n",
      "              'pipeline, but additional clarity and detail in certain areas '\n",
      "              'would enhance the response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Score: 75%**\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states, \"So far I have not optimized any '\n",
      "              'model,\" which directly addresses the question about experience '\n",
      "              'with optimizing model performance. However, this response lacks '\n",
      "              'depth, as it does not provide any insights into potential '\n",
      "              'strategies or considerations for optimization, even if they '\n",
      "              \"haven't personally implemented them.\\n\"\n",
      "              '\\n'\n",
      "              '2. The candidate mentions that they had to determine which '\n",
      "              'model best suited the project area but does not elaborate on '\n",
      "              'how this decision-making process relates to optimizing model '\n",
      "              'performance in terms of speed and cost efficiency. This '\n",
      "              'connection is crucial to the question and should have been '\n",
      "              'addressed.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate discusses the selection of Claude 3.5 Sonnet '\n",
      "              'and ChatGPT 4.0 Mini, highlighting their intelligence, '\n",
      "              'cost-effectiveness, and speed. However, the explanation lacks '\n",
      "              'specific details on how these models were optimized for '\n",
      "              'performance in the context of the project. For instance, they '\n",
      "              'could have mentioned any techniques or methodologies used to '\n",
      "              \"evaluate or compare the models' performance.\\n\"\n",
      "              '\\n'\n",
      "              '4. The mention of Hume AI for sentiment analysis is relevant, '\n",
      "              'but the candidate does not discuss how the integration of this '\n",
      "              'external service impacts overall model performance, speed, or '\n",
      "              'cost efficiency. This omission weakens the response, as it '\n",
      "              'misses an opportunity to discuss the optimization of the '\n",
      "              'overall system.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's statement about not knowing the specific \"\n",
      "              'implementation of the Hume AI service could be seen as a '\n",
      "              \"limitation. While it's understandable that they may not have \"\n",
      "              'direct knowledge, discussing the implications of using an '\n",
      "              'external service on performance and cost would have added value '\n",
      "              'to the answer.\\n'\n",
      "              '\\n'\n",
      "              '6. The overall structure of the response could be improved for '\n",
      "              'clarity. The candidate jumps between different models and '\n",
      "              'services without clearly linking them back to the question of '\n",
      "              'optimization, which may confuse the reader.\\n'\n",
      "              '\\n'\n",
      "              'In summary, while the candidate provides some relevant '\n",
      "              'information about model selection, they do not adequately '\n",
      "              'address the specific aspects of optimizing model performance, '\n",
      "              'particularly in terms of speed and cost efficiency. More '\n",
      "              'detailed explanations and connections to the question would '\n",
      "              'enhance the response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-8------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Flow ---\n",
    "for i in range(9):\n",
    "    print(f'---------------------------{timestamp}-{i}------------------------')\n",
    "    chatlog_chat = reformat_chatlog(chatlog)  # Process the loaded chatlog\n",
    "\n",
    "    chatlog_chat = process_sentiments(chatlog_chat, timestamp) # Analyze sentiments\n",
    "\n",
    "    #chatlog_chat_copy = chatlog_chat[1]\n",
    "    evaluation = evaluate_candidate(chatlog_chat) # Get the final evaluation\n",
    "\n",
    "    \n",
    "    chatlog_file_path = f\"data/interviews/{timestamp}-{i}/outcome/\"\n",
    "\n",
    "    if not os.path.exists(chatlog_file_path):\n",
    "        os.makedirs(chatlog_file_path)\n",
    "\n",
    "    with open(chatlog_file_path+\"chatlog.json\", \"w\") as file:\n",
    "        json.dump(chatlog_chat, file, indent=4)\n",
    "    evaluation_file_path = f\"data/interviews/{timestamp}-{i}/outcome/\"\n",
    "    if not os.path.exists(chatlog_file_path):\n",
    "        os.makedirs(chatlog_file_path)\n",
    "    with open(evaluation_file_path+\"evaluation.txt\", \"w\") as file:\n",
    "        file.write(str(evaluation).replace(\"\\\\n\", \"\\n\"))\n",
    "    print(f'---------------------------{timestamp}-{i}------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
