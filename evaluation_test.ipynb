{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages if you don't have them:\n",
    "# !pip install python-dotenv joblib anthropic hume-api\n",
    "\n",
    "# Import Statements\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from joblib import load\n",
    "from pprint import pprint\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "# Custom Modules (Assuming these are in your project structure)\n",
    "from src.utils.anthropicwrapper import ClaudeChat, ClaudeChatAssess\n",
    "from src.utils.humewrapper import HumeSentimentAnalyzer\n",
    "from src.modules import ConversationVerifier\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Environment Variables ---\n",
    "load_dotenv(os.path.join(os.path.dirname(os.getcwd()), \".env\"))\n",
    "\n",
    "# --- Timestamp Input ---\n",
    "timestamp = 1724629705  # Example timestamp, replace with your input \n",
    "\n",
    "# --- Load Data ---\n",
    "directory = f'data/interviews/{timestamp}/'\n",
    "chatlog = load(os.path.join(directory, \"joblib/conversation.joblib\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing PDF: no such file: 'data/cvs/cv-deb.pdf'\n"
     ]
    }
   ],
   "source": [
    "# --- Model Setup --- \n",
    "try:\n",
    "    sentiment_analyser = HumeSentimentAnalyzer(api_key=os.getenv(\"HUME_API_KEY\"))\n",
    "\n",
    "    sentiment_system_prompt = \"\"\"You are a skilled emotions analyst provided with a detailed breakdown of sentiment analysis scores from Hume.ai, for a single response in an interview to a question from the interviewer. The scores are split into 3 sections. All numbers are from 0 to 1, linearly scaling, with 1 being a very strong representation of the indicator in question.\n",
    "\n",
    "    First, Emotions. This contains several human emotions with a numerical value indicating the strength of the corresponding emotion.\n",
    "    Second, Sentiments. This contains a scale from 1 to 9, each containing a numerical value indicating the magnitude of the sentiment of the topic of the conversation. A negative topic such as murder will have a high value lower in the scale, such as 1 or 2, and a positive topic will have a high value from 0 to 1 higher in the scale such as 8 or 9.\n",
    "    Third, Toxicity. This contains several toxic representations such as hate, insult, etc, with a value from 0 to 1 for each representation identified in the audio.\n",
    "\n",
    "    Your job is to provide a concise detailed one sentence breakdown of how the individual was feeling for the particular scores provided. You must be highly objective as your job is to discern whether or not a candidate was exhibiting traits which would or would not be fitting for a successful interview. \n",
    "    Model your answer beginning with something along the lines of \"For this particular response, the candidate...\"\"\"\n",
    "    \n",
    "    sentiment_summariser = ClaudeChat(\"claude-3-5-sonnet-20240620\", sentiment_system_prompt)\n",
    "\n",
    "    evaluation_system_prompt = \"\"\"You are a highly skilled interviewer currently tasked with reviewing a phone screening interview candidate to decide whether they are to pass on to the next stage of the interview process. There is a high volume of candidates and as such only 47% of candidates should be allowed to pass.\n",
    "    You will be provided with the following information to aid your decision:\n",
    "    1. A copy of the job description.\n",
    "    2. Transcript of the phone interview between the interviewer and candidate.\n",
    "    3. Sentiment analysis summary to provide you with insight into the tone of the candidate in the call\n",
    "    4. A summary of any factual inaccuracies the candidate may have made during discussions in the call\n",
    "    5. A copy of the candidate's CV, which will be provided next.\n",
    "\n",
    "    You are to evaluate the candidate, primarily on the transcript, and use the additional information provided to identify any potential red-flags. Your response should include a detailed breakdown of why the candidate is chosen to continue onwards to further interviewing. You must end the breakdown with a simple one word response on a new line, \"pass\" or \"fail\".\"\"\"\n",
    "\n",
    "    candidate_evaluator = ClaudeChatAssess(\"claude-3-5-sonnet-20240620\", evaluation_system_prompt, \"data/cvs/cv-deb.pdf\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during model setup: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reformat Chatlog ---\n",
    "def reformat_chatlog(chatlog):\n",
    "    dropped_context = chatlog[3:]\n",
    "    outputchatlog = []\n",
    "\n",
    "    for i in range(0, len(dropped_context), 2):\n",
    "        if i + 1 < len(dropped_context):\n",
    "            tempdict = {\n",
    "                'interviewer': dropped_context[i]['content'],\n",
    "                'candidate': dropped_context[i+1]['content']\n",
    "            }\n",
    "            outputchatlog.append(tempdict)\n",
    "        else:\n",
    "            break \n",
    "            \n",
    "    return outputchatlog\n",
    "\n",
    "# --- Process Sentiments ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiments(chatlog_chat, timestamp):\n",
    "    filepath = os.path.join('data', 'interviews', str(timestamp), 'audio')\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Full audio directory path: {os.path.abspath(filepath)}\")\n",
    "\n",
    "    files = [f for f in os.listdir(filepath) if os.path.isfile(os.path.join(filepath, f))]\n",
    "    \n",
    "    if len(chatlog_chat) < len(files):\n",
    "        files = files[:len(chatlog_chat)]\n",
    "\n",
    "    for f in files:\n",
    "        print(f\"File found: {f}\")\n",
    "\n",
    "    sentiments = []\n",
    "    \n",
    "    for count, file in enumerate(files, 1):\n",
    "        full_file_path = os.path.join(filepath, file)\n",
    "        print(f\"Processing file: {full_file_path}\")\n",
    "        \n",
    "        # Add a small delay and re-check file existence\n",
    "        time.sleep(0.1)\n",
    "        if not os.path.exists(full_file_path):\n",
    "            print(f\"File not found (after delay): {full_file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = sentiment_analyser.analyze_audio(full_file_path)\n",
    "            sentiment_summary = sentiment_summariser.chat(str(result))\n",
    "            sentiments.append((result, sentiment_summary))\n",
    "            chatlog_chat[count-1]['sentiment'] = sentiment_summary\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {full_file_path}: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    return chatlog_chat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate Candidate --- \n",
    "def evaluate_candidate(chatlog_chat):\n",
    "    ConversationVerifier.process_qa_pair(chatlog_chat)\n",
    "    print(\"The Feedback JSON from the sentiment analyser and accuracy verifier: \\n\")\n",
    "    pprint(chatlog_chat)\n",
    "    evaluation = candidate_evaluator.chat(str(chatlog_chat))\n",
    "    return evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------1724629705-0------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.917409, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.957554, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98839974, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making a automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 897}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882056, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.722984}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9892666, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1221}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition paper on improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be improved for '\n",
      "              'clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more clearly articulated. It would be '\n",
      "              'beneficial to specify how the answers are processed before '\n",
      "              'being searched.\\n'\n",
      "              '   - The mention of \"the first two articles\" could be '\n",
      "              'confusing. It would be clearer to specify that the system '\n",
      "              'retrieves the contents of the first two articles from the '\n",
      "              'search results to provide context for the language model (LM).\\n'\n",
      "              '\\n'\n",
      "              \"3. The candidate's description of using sentiment analysis \"\n",
      "              'alongside the accuracy verifier is a good addition, but it '\n",
      "              'would be helpful to briefly explain how sentiment analysis '\n",
      "              'contributes to the overall process.\\n'\n",
      "              '\\n'\n",
      "              '4. The overall structure of the answer is somewhat convoluted, '\n",
      "              'making it a bit difficult to follow. Encouraging the candidate '\n",
      "              'to organize their thoughts more clearly could enhance their '\n",
      "              'communication skills.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates relevant experience with '\n",
      "              'RAG pipelines, but clarity and organization in their '\n",
      "              'explanation could be improved.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant and detailed explanation '\n",
      "              'of the challenges faced while implementing the RAG pipeline for '\n",
      "              'the accuracy verifier. They effectively described the process '\n",
      "              'of breaking down answers into searchable strings and using '\n",
      "              'multiple web pages to enhance retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieve context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to specify how the candidate '\n",
      "              'determined which web pages to select and how they ensured the '\n",
      "              'relevance of the retrieved content.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity for relevance, which '\n",
      "              'is a good approach. However, they could elaborate on how they '\n",
      "              'evaluated the effectiveness of this method in improving '\n",
      "              'retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              'a strong point, but the candidate could enhance their answer by '\n",
      "              'briefly summarizing the key insights from the paper that they '\n",
      "              'applied to their project.\\n'\n",
      "              '\\n'\n",
      "              '5. Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the RAG pipeline and the specific challenges they faced. The '\n",
      "              'clarity of their explanation has improved compared to the '\n",
      "              'previous answer, but further elaboration on certain points '\n",
      "              'could enhance the overall quality of their response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant explanation regarding '\n",
      "              'model selection, stating that they chose JATGBD 4.0 mini due to '\n",
      "              'its balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout the response. This could lead to confusion '\n",
      "              'about the model being discussed.\\n'\n",
      "              '\\n'\n",
      "              \"2. The candidate's rationale for not selecting a more \"\n",
      "              'sophisticated model like Claude SONET 3.5 is valid, but it '\n",
      "              'could be enhanced by briefly explaining the specific '\n",
      "              'requirements of their project that made a less powerful model '\n",
      "              'sufficient.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is somewhat vague. '\n",
      "              'While the candidate mentions writing several prompts to address '\n",
      "              'grammatical and typographical errors in the speech-to-text '\n",
      "              'conversion, they could provide more detail on the types of '\n",
      "              'prompts used and how they specifically guided the LLM to handle '\n",
      "              'these errors.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's description of the challenges faced during \"\n",
      "              'prompt engineering is relevant, but it lacks depth. They could '\n",
      "              'elaborate on how they assessed the effectiveness of their '\n",
      "              \"prompts in improving the model's performance and accuracy.\\n\"\n",
      "              '\\n'\n",
      "              '5. Overall, while the candidate demonstrates a good '\n",
      "              'understanding of the integration of the RAG pipeline with the '\n",
      "              'LLM, the clarity and depth of their explanation could be '\n",
      "              'improved in certain areas, particularly regarding prompt '\n",
      "              'engineering and the rationale behind model selection.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which directly addresses the question about experience '\n",
      "              'with optimizing model performance. However, this could be '\n",
      "              'misleading as they later discuss model selection and '\n",
      "              'considerations for speed and cost efficiency, which are aspects '\n",
      "              'of optimization. It would be beneficial for the candidate to '\n",
      "              \"clarify that while they haven't performed optimization in the \"\n",
      "              'traditional sense, they have made decisions that impact '\n",
      "              'performance.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPD 4.0 Mini for the accuracy '\n",
      "              'verifier. While they provide reasoning for these choices, they '\n",
      "              'could enhance their answer by explicitly discussing how these '\n",
      "              'selections contribute to speed and cost efficiency. For '\n",
      "              'instance, they could elaborate on the trade-offs made between '\n",
      "              'model complexity and operational costs.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of Hume AI for sentiment analysis is relevant, '\n",
      "              'but the candidate does not explain how this choice impacts the '\n",
      "              'overall performance in terms of speed and cost. Providing '\n",
      "              'insight into the cost structure of using Hume AI compared to '\n",
      "              'other potential solutions would strengthen their response.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate states, \"we don\\'t know the specific '\n",
      "              'implementation of the service because we are paying to use the '\n",
      "              'service.\" This could be perceived as a lack of engagement with '\n",
      "              'the optimization process. It would be more effective if the '\n",
      "              'candidate acknowledged the importance of understanding the '\n",
      "              'services they utilize, even if they are external, to make '\n",
      "              'informed decisions about performance and cost.\\n'\n",
      "              '\\n'\n",
      "              '5. Overall, while the candidate demonstrates an understanding '\n",
      "              'of model selection and its implications for performance, the '\n",
      "              'answer lacks depth in discussing specific optimization '\n",
      "              'strategies or metrics used to evaluate speed and cost '\n",
      "              'efficiency. More elaboration on these points would improve the '\n",
      "              'clarity and relevance of their response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-0------------------------\n",
      "---------------------------1724629705-1------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9185851, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9564517, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98831725, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making a automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 897}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98833185, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98933417, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1224}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing accuracy verifier using retrieval-augmented generation in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition paper on improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be improved for '\n",
      "              'clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more precise. It would be beneficial to '\n",
      "              'clarify how the breakdown process works and what criteria are '\n",
      "              'used for creating these strings.\\n'\n",
      "              '   - The mention of \"searching on Google\" may imply a lack of '\n",
      "              'control over the sources being retrieved. It would be more '\n",
      "              'effective to specify if there are any constraints or filters '\n",
      "              'applied to ensure the reliability of the retrieved content.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentions using sentiment analysis alongside '\n",
      "              'the accuracy verifier, but does not elaborate on how sentiment '\n",
      "              'analysis contributes to the overall process. Providing a brief '\n",
      "              'explanation of its role would enhance the completeness of the '\n",
      "              'answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The overall structure of the response could benefit from '\n",
      "              'clearer segmentation of ideas. Breaking the answer into '\n",
      "              'distinct parts (e.g., introduction of the project, explanation '\n",
      "              'of the RAG pipeline, and the role of sentiment analysis) would '\n",
      "              'improve readability and comprehension.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'could enhance their explanation for clarity and depth.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate effectively elaborated on the challenges faced '\n",
      "              'while implementing the RAG pipeline, specifically addressing '\n",
      "              'retrieval quality and context relevance. They described '\n",
      "              'breaking down answers into searchable strings, which is a '\n",
      "              'relevant approach to enhance retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieving context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to clarify how the candidate '\n",
      "              'determines the relevance of the retrieved web pages and if '\n",
      "              'there are any filters or criteria applied to ensure the '\n",
      "              'reliability of the sources.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved information in '\n",
      "              'a vector store and using cosine similarity for relevance, which '\n",
      "              'is a solid approach. However, they could have provided more '\n",
      "              'detail on how cosine similarity is applied in practice and how '\n",
      "              'it contributes to improving the accuracy verification process.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a strong point, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would enhance the answer if they '\n",
      "              'briefly explained how this technique specifically addresses the '\n",
      "              'challenges they faced.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a strong understanding of '\n",
      "              'the RAG pipeline and effectively addresses the challenges '\n",
      "              'related to retrieval quality and context relevance, but could '\n",
      "              'improve clarity and depth in certain areas.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a clear rationale for model '\n",
      "              'selection, stating that they chose JATGBD 4.0 mini due to its '\n",
      "              'balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This does not detract from the overall '\n",
      "              'understanding but could lead to confusion.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate effectively described the challenges faced in '\n",
      "              'prompt engineering, particularly regarding grammatical and '\n",
      "              'typographical errors in the speech-to-text conversion. They '\n",
      "              'explained how they prompted the LLM to overlook these errors, '\n",
      "              'which is a relevant and insightful point.\\n'\n",
      "              '\\n'\n",
      "              '3. While the candidate mentioned the need for several prompts '\n",
      "              'to guide the LLM, they could have elaborated on the specific '\n",
      "              'types of prompts used or provided examples. This would enhance '\n",
      "              'the clarity of their approach to prompt engineering.\\n'\n",
      "              '\\n'\n",
      "              '4. The answer could benefit from a more structured '\n",
      "              'presentation. For instance, separating the discussion of model '\n",
      "              'selection from the challenges in prompt engineering would '\n",
      "              'improve readability and comprehension.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and '\n",
      "              'effectively addresses the challenges faced, but could improve '\n",
      "              'clarity and depth in certain areas.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states, \"So far I have not optimized any '\n",
      "              'model,\" which directly addresses the question about experience '\n",
      "              'with optimizing model performance. However, this could be seen '\n",
      "              'as a limitation since the question specifically asks for '\n",
      "              'experience in optimizing model performance, particularly in '\n",
      "              'speed and cost efficiency. The candidate could have elaborated '\n",
      "              'on any considerations or strategies they might have thought '\n",
      "              \"about for future optimization, even if they haven't implemented \"\n",
      "              'them yet.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions that they had to determine which '\n",
      "              'model best suited the specific area of their project, which is '\n",
      "              'relevant. However, the explanation lacks depth regarding how '\n",
      "              'they evaluated the models in terms of speed and cost '\n",
      "              'efficiency. Providing specific criteria or metrics used for '\n",
      "              'evaluation would enhance the response.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate discusses the selection of Claude 3.5 Sonnet '\n",
      "              'and ChatGPD 4.0 Mini, but does not provide concrete examples of '\n",
      "              'how these choices impacted speed and cost efficiency in their '\n",
      "              'project. More detail on the decision-making process regarding '\n",
      "              'these factors would strengthen the answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of Hume AI for sentiment analysis is relevant, '\n",
      "              'but the candidate states, \"we don\\'t know the specific '\n",
      "              'implementation of the service because we are paying to use the '\n",
      "              'service.\" This could be perceived as a lack of engagement with '\n",
      "              'the optimization aspect of the service. It would be beneficial '\n",
      "              'to discuss any known performance metrics or how they assessed '\n",
      "              \"the service's effectiveness in relation to cost.\\n\"\n",
      "              '\\n'\n",
      "              '5. The overall structure of the answer could be improved for '\n",
      "              'clarity. The candidate jumps between different models and their '\n",
      "              'purposes without a clear transition, making it harder to follow '\n",
      "              'the logic of their choices. A more organized approach, perhaps '\n",
      "              'by categorizing the models based on their roles (e.g., '\n",
      "              'conversation generation, accuracy verification, sentiment '\n",
      "              'analysis), would enhance readability.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-1------------------------\n",
      "---------------------------1724629705-2------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.90743434, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello.', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.95309263, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.01251170877367258}, {'name': 'Adoration', 'score': 0.008131410926580429}, {'name': 'Aesthetic Appreciation', 'score': 0.016512403264641762}, {'name': 'Amusement', 'score': 0.0907270684838295}, {'name': 'Anger', 'score': 0.0005581923178397119}, {'name': 'Annoyance', 'score': 0.014846810139715672}, {'name': 'Anxiety', 'score': 0.0011913252528756857}, {'name': 'Awe', 'score': 0.008423087187111378}, {'name': 'Awkwardness', 'score': 0.06354702264070511}, {'name': 'Boredom', 'score': 0.10851363092660904}, {'name': 'Calmness', 'score': 0.31933408975601196}, {'name': 'Concentration', 'score': 0.016252553090453148}, {'name': 'Confusion', 'score': 0.029891543090343475}, {'name': 'Contemplation', 'score': 0.02187623828649521}, {'name': 'Contempt', 'score': 0.018504859879612923}, {'name': 'Contentment', 'score': 0.08336649090051651}, {'name': 'Craving', 'score': 0.0006236955523490906}, {'name': 'Desire', 'score': 0.000760009977966547}, {'name': 'Determination', 'score': 0.0022584907710552216}, {'name': 'Disappointment', 'score': 0.0025702782440930605}, {'name': 'Disapproval', 'score': 0.007185309659689665}, {'name': 'Disgust', 'score': 0.0015889910282567143}, {'name': 'Distress', 'score': 0.0006113385898061097}, {'name': 'Doubt', 'score': 0.00666783144697547}, {'name': 'Ecstasy', 'score': 0.0021341179963201284}, {'name': 'Embarrassment', 'score': 0.0033599617891013622}, {'name': 'Empathic Pain', 'score': 0.0008289636461995542}, {'name': 'Enthusiasm', 'score': 0.09442762285470963}, {'name': 'Entrancement', 'score': 0.006641392130404711}, {'name': 'Envy', 'score': 0.0008029191521927714}, {'name': 'Excitement', 'score': 0.05306636542081833}, {'name': 'Fear', 'score': 0.00027591444086283445}, {'name': 'Gratitude', 'score': 0.010404795408248901}, {'name': 'Guilt', 'score': 0.00038117836811579764}, {'name': 'Horror', 'score': 0.00014820862270426005}, {'name': 'Interest', 'score': 0.22310522198677063}, {'name': 'Joy', 'score': 0.08868356049060822}, {'name': 'Love', 'score': 0.005399726331233978}, {'name': 'Nostalgia', 'score': 0.004958468955010176}, {'name': 'Pain', 'score': 0.00010659849067451432}, {'name': 'Pride', 'score': 0.0036682302597910166}, {'name': 'Realization', 'score': 0.053931184113025665}, {'name': 'Relief', 'score': 0.0077752480283379555}, {'name': 'Romance', 'score': 0.001546808984130621}, {'name': 'Sadness', 'score': 0.0003105304786004126}, {'name': 'Sarcasm', 'score': 0.025866815820336342}, {'name': 'Satisfaction', 'score': 0.05627468600869179}, {'name': 'Shame', 'score': 0.0010352996177971363}, {'name': 'Surprise (negative)', 'score': 0.017590997740626335}, {'name': 'Surprise (positive)', 'score': 0.1599823534488678}, {'name': 'Sympathy', 'score': 0.002042335458099842}, {'name': 'Tiredness', 'score': 0.004060816951096058}, {'name': 'Triumph', 'score': 0.00503922114148736}], 'sentiment': [{'name': '1', 'score': 0.0005626916536130011}, {'name': '2', 'score': 0.0006312259938567877}, {'name': '3', 'score': 0.0011379551142454147}, {'name': '4', 'score': 0.0038008831907063723}, {'name': '5', 'score': 0.36965522170066833}, {'name': '6', 'score': 0.2254335731267929}, {'name': '7', 'score': 0.13478495180606842}, {'name': '8', 'score': 0.11552125215530396}, {'name': '9', 'score': 0.11766091734170914}], 'toxicity': [{'name': 'identity_hate', 'score': 0.002974089467898011}, {'name': 'insult', 'score': 0.0027754041366279125}, {'name': 'obscene', 'score': 0.002369341906160116}, {'name': 'severe_toxic', 'score': 0.0023268223740160465}, {'name': 'threat', 'score': 0.002879881300032139}, {'name': 'toxic', 'score': 0.006132675334811211}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9881942, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9888159, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9889891, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Minis. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some these were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1222}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004012571182101965}, {'name': 'Adoration', 'score': 0.0006829452468082309}, {'name': 'Aesthetic Appreciation', 'score': 0.009531065821647644}, {'name': 'Amusement', 'score': 0.017768558114767075}, {'name': 'Anger', 'score': 0.021377960219979286}, {'name': 'Annoyance', 'score': 0.290971577167511}, {'name': 'Anxiety', 'score': 0.006301478948444128}, {'name': 'Awe', 'score': 0.0018564180936664343}, {'name': 'Awkwardness', 'score': 0.02713104523718357}, {'name': 'Boredom', 'score': 0.01682097092270851}, {'name': 'Calmness', 'score': 0.057823993265628815}, {'name': 'Concentration', 'score': 0.24914142489433289}, {'name': 'Confusion', 'score': 0.23553943634033203}, {'name': 'Contemplation', 'score': 0.23761515319347382}, {'name': 'Contempt', 'score': 0.11283509433269501}, {'name': 'Contentment', 'score': 0.00917212013155222}, {'name': 'Craving', 'score': 0.0004715125251095742}, {'name': 'Desire', 'score': 0.0004467264807317406}, {'name': 'Determination', 'score': 0.13106904923915863}, {'name': 'Disappointment', 'score': 0.1638752520084381}, {'name': 'Disapproval', 'score': 0.2616901695728302}, {'name': 'Disgust', 'score': 0.01599179208278656}, {'name': 'Distress', 'score': 0.012161052785813808}, {'name': 'Doubt', 'score': 0.11882956326007843}, {'name': 'Ecstasy', 'score': 0.00018029283091891557}, {'name': 'Embarrassment', 'score': 0.012859308160841465}, {'name': 'Empathic Pain', 'score': 0.004992311354726553}, {'name': 'Enthusiasm', 'score': 0.016190679743885994}, {'name': 'Entrancement', 'score': 0.005481095984578133}, {'name': 'Envy', 'score': 0.0009175182785838842}, {'name': 'Excitement', 'score': 0.0019570139702409506}, {'name': 'Fear', 'score': 0.0029000556096434593}, {'name': 'Gratitude', 'score': 0.002121470170095563}, {'name': 'Guilt', 'score': 0.005176608916372061}, {'name': 'Horror', 'score': 0.0012084299232810736}, {'name': 'Interest', 'score': 0.12445180118083954}, {'name': 'Joy', 'score': 0.0007418046006932855}, {'name': 'Love', 'score': 0.0004286622570361942}, {'name': 'Nostalgia', 'score': 0.002683773636817932}, {'name': 'Pain', 'score': 0.0019978354685008526}, {'name': 'Pride', 'score': 0.014628242701292038}, {'name': 'Realization', 'score': 0.109249547123909}, {'name': 'Relief', 'score': 0.0012261600932106376}, {'name': 'Romance', 'score': 0.00022957369219511747}, {'name': 'Sadness', 'score': 0.004294019192457199}, {'name': 'Sarcasm', 'score': 0.050484832376241684}, {'name': 'Satisfaction', 'score': 0.017589660361409187}, {'name': 'Shame', 'score': 0.015066226944327354}, {'name': 'Surprise (negative)', 'score': 0.035315126180648804}, {'name': 'Surprise (positive)', 'score': 0.0037249226588755846}, {'name': 'Sympathy', 'score': 0.0052593122236430645}, {'name': 'Tiredness', 'score': 0.008244472555816174}, {'name': 'Triumph', 'score': 0.013056766241788864}], 'sentiment': [{'name': '1', 'score': 0.10035957396030426}, {'name': '2', 'score': 0.2912595868110657}, {'name': '3', 'score': 0.28277504444122314}, {'name': '4', 'score': 0.20105718076229095}, {'name': '5', 'score': 0.0871923565864563}, {'name': '6', 'score': 0.015252696350216866}, {'name': '7', 'score': 0.0145176462829113}, {'name': '8', 'score': 0.007305674720555544}, {'name': '9', 'score': 0.0033061157446354628}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003395714797079563}, {'name': 'insult', 'score': 0.0018982250476256013}, {'name': 'obscene', 'score': 0.0019057746976613998}, {'name': 'severe_toxic', 'score': 0.0025856047868728638}, {'name': 'threat', 'score': 0.0030888679903000593}, {'name': 'toxic', 'score': 0.004099810495972633}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition paper on improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet, ChatGPD 4.0 Mini, and Hume AI for model optimization in terms of speed, cost efficiency, and accuracy.', '2. Benefits of using external services like Hume AI for sentiment analysis in model optimization.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of their answer. They described using RAG for '\n",
      "              'an automated screening interview agent, which aligns well with '\n",
      "              'the question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be more precise. For '\n",
      "              'instance, the candidate mentions breaking down the answer into '\n",
      "              '\"separate searchable strings\" and searching on Google. While '\n",
      "              'this is a valid approach, it would be beneficial to clarify how '\n",
      "              'the retrieved articles are processed and integrated into the '\n",
      "              \"LLM's context. The explanation could be more structured to \"\n",
      "              'enhance clarity.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate states that the RAG pipeline is used as an '\n",
      "              '\"accuracy verifier.\" While this is a valid use case, it would '\n",
      "              'be helpful to elaborate on how the accuracy percentage is '\n",
      "              'calculated and what criteria are used to determine the accuracy '\n",
      "              \"of the candidate's answers.\\n\"\n",
      "              '\\n'\n",
      "              '4. The phrase \"the first two articles it will retrieve the '\n",
      "              'contents of the first two articles\" is somewhat repetitive and '\n",
      "              'could be streamlined for better readability.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'additional clarity and detail in certain areas would strengthen '\n",
      "              'their response.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate effectively elaborated on the challenges faced '\n",
      "              'while implementing the RAG pipeline, specifically addressing '\n",
      "              'retrieval quality and context relevance. They described '\n",
      "              \"breaking down the candidate's answer into searchable strings, \"\n",
      "              'which is a valid approach to enhance retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieving context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to clarify how the information '\n",
      "              \"from these web pages is processed and integrated into the LLM's \"\n",
      "              'context. This would provide a more comprehensive understanding '\n",
      "              'of the retrieval process.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity to find relevant '\n",
      "              'portions. This is a strong point, but further elaboration on '\n",
      "              'how cosine similarity is applied in practice would enhance '\n",
      "              'clarity.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a good addition, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would be helpful to briefly '\n",
      "              'explain how this technique specifically addresses the '\n",
      "              'challenges they faced.\\n'\n",
      "              '\\n'\n",
      "              '5. The phrase \"this I think is plenty of information to feed '\n",
      "              'the LLM\" could be more assertive. Instead of using \"I think,\" a '\n",
      "              'more confident statement would strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and effectively addresses the challenges '\n",
      "              'related to retrieval quality and context relevance. A bit more '\n",
      "              'detail in certain areas would further enhance the clarity and '\n",
      "              'depth of their response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a clear rationale for model '\n",
      "              'selection, stating that they chose JATGBD 4.0 mini due to its '\n",
      "              'balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion regarding the '\n",
      "              'model being discussed.\\n'\n",
      "              '\\n'\n",
      "              \"2. The candidate's explanation of the integration of the RAG \"\n",
      "              'pipeline with the LLM is mostly coherent, but it could benefit '\n",
      "              'from more detail on how the outputs from the RAG pipeline are '\n",
      "              'specifically utilized by the LLM. For instance, elaborating on '\n",
      "              'how the searchable strings are generated and how they are '\n",
      "              'processed by the LLM would enhance clarity.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned challenges related to grammatical '\n",
      "              'and typographical errors in the speech-to-text conversion. '\n",
      "              'While they addressed how they prompted the LLM to overlook '\n",
      "              'these issues, it would be helpful to provide specific examples '\n",
      "              'of the types of prompts used. This would give a clearer picture '\n",
      "              'of their prompt engineering process.\\n'\n",
      "              '\\n'\n",
      "              '4. The phrase \"we just needed a cost-effective LLM to just '\n",
      "              'verify the answer\" is somewhat repetitive. Streamlining this '\n",
      "              'statement could improve readability.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's assertion that they did not need a more \"\n",
      "              'sophisticated model like Claude SONET 3.5 is valid, but it '\n",
      "              'would strengthen their argument to briefly mention the specific '\n",
      "              'capabilities or features of JATGBD 4.0 mini that made it '\n",
      "              'suitable for their needs.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges faced during prompt engineering. However, additional '\n",
      "              'clarity and detail in certain areas would enhance the overall '\n",
      "              'quality of their response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate begins by stating that they have not optimized '\n",
      "              'any model, which directly addresses the question about '\n",
      "              'experience with optimizing model performance. However, they '\n",
      "              'seem to confuse \"optimizing\" with \"fine-tuning,\" which may '\n",
      "              'indicate a lack of understanding of the broader concept of '\n",
      "              'optimization in model performance. This could be clarified to '\n",
      "              'improve the response.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discusses the selection of models for '\n",
      "              'different tasks, which is relevant. However, they do not '\n",
      "              'provide specific examples of how they optimized for speed and '\n",
      "              'cost efficiency in their choices. While they mention that '\n",
      "              'Claude 3.5 Sonnet is the \"most intelligent LLM\" and that '\n",
      "              'ChatGPD 4.0 Mini has a \"right balance of intelligence and cost '\n",
      "              'effectiveness,\" they do not elaborate on the criteria or '\n",
      "              'methods used to evaluate these aspects. This lack of detail on '\n",
      "              'optimization strategies affects the overall accuracy of the '\n",
      "              'response.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate states they do not know the '\n",
      "              'specific implementation of the service. This could be seen as a '\n",
      "              'limitation in their understanding of the tools they are using, '\n",
      "              'which detracts from their credibility in discussing '\n",
      "              'optimization.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's explanation of how they integrated the \"\n",
      "              'outputs from the sentiment analysis and accuracy verifier into '\n",
      "              'Claude Sonnet 3.5 is somewhat vague. They mention feeding the '\n",
      "              'scores into Claude but do not explain how this process '\n",
      "              'contributes to optimizing model performance in terms of speed '\n",
      "              'and cost efficiency.\\n'\n",
      "              '\\n'\n",
      "              '5. The overall structure of the answer could be improved for '\n",
      "              'clarity. The candidate jumps between different models and their '\n",
      "              'purposes without a clear connection to the question about '\n",
      "              'optimization. A more organized approach would enhance the '\n",
      "              'readability and effectiveness of their response. \\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates some understanding of '\n",
      "              'model selection and application, they need to provide more '\n",
      "              'specific examples and clarity regarding optimization strategies '\n",
      "              'to fully address the question.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-2------------------------\n",
      "---------------------------1724629705-3------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9212194, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.95657223, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98820305, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98817444, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.722984}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98872584, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1224}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline for accuracy verification in AI screening interviews\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in automated screening interview agents\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"improving context retrieval quality RAG pipeline\"', '2. \"Google Query Decomposition paper accuracy verification\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Challenges in prompt engineering for large language model integration\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be improved for '\n",
      "              'clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more precise. It would be beneficial to '\n",
      "              'clarify how the breakdown process works and what criteria are '\n",
      "              'used for creating these strings.\\n'\n",
      "              '   - The mention of \"searching on Google\" may imply a lack of '\n",
      "              'control over the sources being retrieved. It would be more '\n",
      "              'effective to specify if there are any filters or criteria for '\n",
      "              'selecting the articles to ensure the reliability of the '\n",
      "              'information.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentions using sentiment analysis alongside '\n",
      "              'the accuracy verifier, but does not elaborate on how sentiment '\n",
      "              'analysis contributes to the overall process. Providing a brief '\n",
      "              'explanation of its role would enhance the understanding of the '\n",
      "              'project.\\n'\n",
      "              '\\n'\n",
      "              '4. The overall structure of the answer could be improved for '\n",
      "              'better readability. Breaking the response into shorter '\n",
      "              'sentences or using bullet points for key components of the '\n",
      "              'project could help convey the information more effectively.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'some areas could benefit from additional detail and clarity.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant and detailed explanation '\n",
      "              'of how they implemented the RAG pipeline for their accuracy '\n",
      "              \"verifier. They described breaking down the candidate's answers \"\n",
      "              'into searchable strings, which is a key aspect of improving '\n",
      "              'retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieving context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to specify if there are any '\n",
      "              'criteria or filters applied to ensure the reliability of the '\n",
      "              'sources being retrieved. This would address potential concerns '\n",
      "              'about the quality of information sourced from Google.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved information in '\n",
      "              'a vector store and using cosine similarity for relevance, which '\n",
      "              'is a good approach. However, they could have elaborated on how '\n",
      "              'the cosine similarity is calculated and how it contributes to '\n",
      "              'determining the relevance of the retrieved information.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a strong point, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would enhance the answer if they '\n",
      "              'briefly explained how this technique specifically improved '\n",
      "              'their implementation compared to previous methods.\\n'\n",
      "              '\\n'\n",
      "              '5. The candidate did not elaborate on the role of sentiment '\n",
      "              'analysis in the overall process, which was mentioned in the '\n",
      "              'previous answer. Providing a brief explanation of how sentiment '\n",
      "              'analysis interacts with the accuracy verification process would '\n",
      "              'improve clarity.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and its application in their project, with '\n",
      "              'only minor areas for improvement in clarity and detail.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant explanation regarding '\n",
      "              'model selection, stating that they chose JATGBD 4.0 mini due to '\n",
      "              'its balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion about the model '\n",
      "              'being discussed.\\n'\n",
      "              '\\n'\n",
      "              \"2. The candidate's rationale for not selecting a more \"\n",
      "              'sophisticated model like Claude SONET 3.5 is valid, but it '\n",
      "              'could be enhanced by briefly explaining the specific '\n",
      "              'requirements of their project that made JATGBD 4.0 mini '\n",
      "              'sufficient.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is generally clear, '\n",
      "              'but it would benefit from more detail on the types of prompts '\n",
      "              'used and how they specifically addressed the challenges of '\n",
      "              'grammatical and typographical errors in the transcribed text. '\n",
      "              \"This would provide a clearer picture of the candidate's \"\n",
      "              'approach to overcoming these challenges.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate mentioned prompting the LLM to overlook '\n",
      "              'grammatical errors, but it would be helpful to elaborate on how '\n",
      "              'this was achieved in practice. For instance, did they use '\n",
      "              \"specific prompt structures or techniques to guide the model's \"\n",
      "              'understanding?\\n'\n",
      "              '\\n'\n",
      "              '5. Overall, while the candidate demonstrates a solid '\n",
      "              'understanding of the integration of the RAG pipeline with the '\n",
      "              'LLM, providing more specific examples and clarifications would '\n",
      "              'enhance the clarity and depth of their response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate states that they have not optimized any model, '\n",
      "              'which is accurate in the context of their specific project. '\n",
      "              'However, they could have elaborated on any considerations or '\n",
      "              'strategies they might have used to ensure speed and cost '\n",
      "              'efficiency when selecting models, even if they did not perform '\n",
      "              'optimization themselves.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions choosing Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPD 4.0 Mini for the accuracy '\n",
      "              'verifier. While they provide reasoning for these choices, they '\n",
      "              'could have included more detail on how these models '\n",
      "              'specifically contributed to speed and cost efficiency in their '\n",
      "              'project. For instance, discussing any performance metrics or '\n",
      "              'comparisons would strengthen their response.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of Hume AI for sentiment analysis is relevant, '\n",
      "              'but the candidate does not explain how the integration of this '\n",
      "              'service impacts overall model performance in terms of speed and '\n",
      "              'cost. Providing insight into the trade-offs or benefits of '\n",
      "              'using an external service would enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate states that they do not know the specific '\n",
      "              'implementation of the Hume AI service because they are paying '\n",
      "              'to use it. While this is understandable, it would be beneficial '\n",
      "              'to discuss any considerations they made regarding the '\n",
      "              'reliability or efficiency of using an external service versus '\n",
      "              'an in-house solution.\\n'\n",
      "              '\\n'\n",
      "              '5. The overall structure of the answer could be improved for '\n",
      "              'clarity. Breaking down the response into clearer sections or '\n",
      "              'bullet points for each model and its specific contributions to '\n",
      "              'speed and cost efficiency would help convey the information '\n",
      "              'more effectively. \\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate demonstrates a solid understanding '\n",
      "              'of model selection and its implications, there are areas where '\n",
      "              'additional detail and clarity could improve the response.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-3------------------------\n",
      "---------------------------1724629705-4------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.91965765, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9576909, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98839426, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9882196, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9884829, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1237}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition paper on improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of their answer. They described using RAG for '\n",
      "              'an automated screening interview agent, which aligns well with '\n",
      "              'the question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be more precise. For '\n",
      "              'instance, the candidate mentions breaking down the answer into '\n",
      "              '\"separate searchable strings\" and searching on Google. While '\n",
      "              'this is a valid approach, it would be beneficial to clarify how '\n",
      "              'the retrieved articles are processed and integrated into the '\n",
      "              \"LLM's context. The explanation could be more structured to \"\n",
      "              'enhance clarity.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate states that the RAG pipeline is used as an '\n",
      "              '\"accuracy verifier.\" While this is a valid use case, it would '\n",
      "              'be helpful to elaborate on how the accuracy percentage is '\n",
      "              'calculated and what criteria are used to determine the accuracy '\n",
      "              \"of the candidate's answers.\\n\"\n",
      "              '\\n'\n",
      "              '4. The phrase \"the first two articles it will retrieve the '\n",
      "              'contents of the first two articles\" is somewhat redundant and '\n",
      "              'could be streamlined for better clarity.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'there is room for improvement in clarity and detail regarding '\n",
      "              'the implementation and evaluation aspects.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate effectively elaborated on the challenges faced '\n",
      "              'while implementing the RAG pipeline, specifically addressing '\n",
      "              'retrieval quality and context relevance. They described '\n",
      "              \"breaking down the candidate's answer into searchable strings, \"\n",
      "              'which is a valid approach to enhance retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieving context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to clarify how the information '\n",
      "              \"from these web pages is processed and integrated into the LLM's \"\n",
      "              'context. This detail would enhance the understanding of the '\n",
      "              'retrieval process.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity to find relevant '\n",
      "              'portions. This is a good approach, but it would be helpful to '\n",
      "              'elaborate on how the cosine similarity is calculated and how it '\n",
      "              'contributes to improving the accuracy verification process.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a strong point, as it shows the candidate's engagement with \"\n",
      "              'relevant literature. However, it would be beneficial to briefly '\n",
      "              'explain how this technique specifically addresses the '\n",
      "              'shortcomings of searching only two or three websites.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and its application in their project, with '\n",
      "              'only minor areas for improvement in clarity and detail '\n",
      "              'regarding the integration and evaluation processes.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a clear rationale for model '\n",
      "              'selection, stating that they chose JATGBD 4.0 mini due to its '\n",
      "              'balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion regarding the '\n",
      "              'model being discussed.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate effectively explained the challenges faced in '\n",
      "              'prompt engineering, particularly regarding grammatical and '\n",
      "              'typographical errors in the speech-to-text conversion. They '\n",
      "              'described how they prompted the LLM to overlook these errors, '\n",
      "              'which is a valid approach. However, it would be beneficial to '\n",
      "              'provide more detail on the specific types of prompts used and '\n",
      "              'how they were structured to address these issues.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned that they did not require a more '\n",
      "              'sophisticated model like Claude SONET 3.5, which is a '\n",
      "              'reasonable decision based on their project needs. However, it '\n",
      "              'would enhance the answer to briefly explain why the chosen '\n",
      "              'model was sufficient for their specific use case, particularly '\n",
      "              'in terms of the tasks it needed to perform.\\n'\n",
      "              '\\n'\n",
      "              '4. The explanation of the integration of the RAG pipeline with '\n",
      "              'the LLM could be more structured. While the candidate mentioned '\n",
      "              'the process of converting speech to text and the subsequent '\n",
      "              'challenges, a clearer outline of how the RAG pipeline interacts '\n",
      "              'with the LLM during the verification process would improve '\n",
      "              'clarity.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM, but there are '\n",
      "              'areas for improvement in clarity and detail regarding the '\n",
      "              'prompt engineering and the rationale behind model selection.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which indicates a lack of direct experience with model '\n",
      "              'optimization. While they interpreted \"optimizing\" as '\n",
      "              '\"fine-tuning,\" it would have been beneficial to clarify that '\n",
      "              'optimization can also include other aspects such as model '\n",
      "              'selection, cost efficiency, and speed, which they did discuss '\n",
      "              'later in their answer.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentioned selecting Claude 3.5 Sonnet for '\n",
      "              'real-time conversation and ChatGPD 4.0 Mini for the accuracy '\n",
      "              'verifier. However, they did not elaborate on how these choices '\n",
      "              'specifically contributed to optimizing model performance in '\n",
      "              'terms of speed and cost efficiency. More detail on the '\n",
      "              'decision-making process regarding these models and their '\n",
      "              'performance metrics would enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate stated that \"4.0 Mini has the right balance of '\n",
      "              'intelligence and cost effectiveness and also speed.\" While this '\n",
      "              'is a valid point, they did not provide specific examples or '\n",
      "              'metrics to support this claim. Including such details would '\n",
      "              'strengthen their argument regarding cost efficiency and speed.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate did not explain how this choice '\n",
      "              'impacted the overall optimization of the model performance. A '\n",
      "              'brief discussion on the cost and speed implications of using an '\n",
      "              'external service would have added depth to their answer.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's explanation of feeding the sentiment and \"\n",
      "              'accuracy verifier scores back into Claude Sonnet 3.5 to derive '\n",
      "              'a final verdict is a good point. However, they could have '\n",
      "              'elaborated on how this process contributes to optimizing the '\n",
      "              'overall model performance, particularly in terms of efficiency '\n",
      "              'and accuracy.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate provided relevant information '\n",
      "              'regarding model selection and considerations for speed and cost '\n",
      "              'efficiency, they could improve their answer by providing more '\n",
      "              'specific examples, metrics, and a clearer connection between '\n",
      "              'their choices and the optimization of model performance.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-4------------------------\n",
      "---------------------------1724629705-5------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9187515, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.955974, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98806256, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9883294, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98864156, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last, iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo geographical errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1238}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in sentiment analysis and accuracy verification\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in automated screening interview agent\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition technique for improving answer accuracy\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini vs Claude SONET 3.5 for language model selection\"', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. Comparison between Claude 3.5 Sonnet and ChatGPD 4.0 Mini in terms of intelligence, cost effectiveness, and speed.', '2. Information on Hume AI for sentiment analysis directly from audio and video feed.']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline was implemented as '\n",
      "              'an accuracy verifier is mostly clear, but there are some areas '\n",
      "              'that could be improved for clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more clearly articulated. It would be '\n",
      "              'beneficial to specify how the answers are processed before '\n",
      "              'being searched.\\n'\n",
      "              '   - The explanation of retrieving the contents of the first '\n",
      "              'two articles could be more concise. The candidate could clarify '\n",
      "              'how the information from these articles is integrated into the '\n",
      "              'context of the language model (LM).\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned using sentiment analysis alongside '\n",
      "              'the RAG pipeline, which is an interesting addition. However, '\n",
      "              'they did not elaborate on how sentiment analysis contributes to '\n",
      "              'the overall functionality of the project. Providing a brief '\n",
      "              'explanation of its role could enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's description of the process of verifying the \"\n",
      "              \"accuracy of the candidate's answers is generally accurate, but \"\n",
      "              'the phrasing could be refined for better readability. For '\n",
      "              'example, the sentence structure is somewhat convoluted, which '\n",
      "              'may hinder understanding.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'some areas could benefit from clearer articulation and '\n",
      "              'additional detail.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant and detailed explanation '\n",
      "              'of how they implemented the RAG pipeline for the accuracy '\n",
      "              'verifier. They effectively described the process of breaking '\n",
      "              \"down the candidate's answers into searchable strings and using \"\n",
      "              'those to retrieve information from multiple web pages.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentioned using six query strings to search '\n",
      "              'Google, which is a good approach to enhance retrieval quality. '\n",
      "              'However, they could clarify how they determined the number of '\n",
      "              'query strings and the criteria for selecting them. This would '\n",
      "              'provide more insight into their decision-making process.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of retrieving context from the first two web '\n",
      "              'pages is clear, but the candidate could improve the '\n",
      "              'articulation of how this information is integrated into the '\n",
      "              \"LLM's context. A more explicit description of the integration \"\n",
      "              'process would enhance understanding.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate referenced a paper by Google on Query '\n",
      "              'Decomposition, which adds credibility to their approach. '\n",
      "              'However, they could briefly explain how this technique '\n",
      "              'specifically addresses the challenges they faced in their '\n",
      "              'project. This would provide a clearer connection between the '\n",
      "              'technique and its application in their work.\\n'\n",
      "              '\\n'\n",
      "              '5. The mention of using cosine similarity to find relevant '\n",
      "              'portions of the vector store is a strong point, but the '\n",
      "              'candidate could elaborate on how they ensured the quality of '\n",
      "              'the retrieved information. Discussing any validation or '\n",
      "              'filtering steps would strengthen their response.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and its application in their project, with '\n",
      "              'only minor areas for improvement in clarity and detail.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant explanation regarding '\n",
      "              'model selection, stating that they chose JATGBD 4.0 mini due to '\n",
      "              'its balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This does not detract from the overall '\n",
      "              'understanding of their model selection process.\\n'\n",
      "              '\\n'\n",
      "              \"2. The candidate's rationale for not selecting a more \"\n",
      "              'sophisticated model like Claude SONET 3.5 is clear, but they '\n",
      "              'could have elaborated on the specific criteria they used to '\n",
      "              'evaluate the balance of intelligence, cost, and speed. '\n",
      "              'Providing more detail on this decision-making process would '\n",
      "              'enhance the answer.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of prompt engineering is somewhat vague. The '\n",
      "              'candidate mentions writing several prompts to help the LLM '\n",
      "              'verify answers, but they could provide more specific examples '\n",
      "              'of the types of prompts they used or how they structured them '\n",
      "              'to address the challenges faced. This would give a clearer '\n",
      "              'picture of their approach to prompt engineering.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate discusses challenges related to grammatical '\n",
      "              'and typographical errors in the speech-to-text conversion '\n",
      "              'process. While they mention prompting the LLM to overlook these '\n",
      "              'errors, they could elaborate on how effective this approach was '\n",
      "              'and whether they implemented any additional strategies to '\n",
      "              'improve the accuracy of the text input.\\n'\n",
      "              '\\n'\n",
      "              '5. Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM, but the '\n",
      "              'answer could benefit from more specific examples and a deeper '\n",
      "              'exploration of their decision-making process and the '\n",
      "              'effectiveness of their strategies.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which indicates a lack of direct experience in '\n",
      "              'optimizing model performance. While they clarify that they '\n",
      "              'interpreted \"optimizing\" as \"fine-tuning,\" it would be '\n",
      "              'beneficial to explicitly mention any indirect experiences or '\n",
      "              'considerations they had regarding optimization, even if it was '\n",
      "              'not part of their specific project.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discusses the selection of models based on '\n",
      "              'their suitability for specific tasks, which is relevant. '\n",
      "              'However, they could have provided more detail on how they '\n",
      "              'evaluated the trade-offs between speed, cost, and performance '\n",
      "              'when selecting Claude 3.5 Sonnet and ChatGPT 4.0 Mini. This '\n",
      "              'would enhance the understanding of their decision-making '\n",
      "              'process.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate does not elaborate on how this '\n",
      "              'choice impacts the overall model performance in terms of speed '\n",
      "              'and cost efficiency. Providing insights into the cost '\n",
      "              'implications or performance metrics of using Hume AI would '\n",
      "              'strengthen the answer.\\n'\n",
      "              '\\n'\n",
      "              '4. The candidate states, \"we don\\'t know the specific '\n",
      "              'implementation of the service because we are paying to use the '\n",
      "              'service.\" While this is understandable, it would be helpful to '\n",
      "              'mention any considerations they had regarding the reliability '\n",
      "              'or performance of the external service, as this could relate to '\n",
      "              'overall optimization.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's explanation of how they integrated the \"\n",
      "              'outputs from the sentiment analysis and accuracy verifier into '\n",
      "              'Claude Sonnet 3.5 is somewhat vague. More clarity on how these '\n",
      "              'outputs were utilized to enhance the final decision-making '\n",
      "              'process would improve the response.\\n'\n",
      "              '\\n'\n",
      "              '6. Overall, while the candidate demonstrates an understanding '\n",
      "              'of model selection and considerations for performance, the '\n",
      "              'answer lacks depth in discussing optimization strategies and '\n",
      "              'the implications of their choices on speed and cost efficiency.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-5------------------------\n",
      "---------------------------1724629705-6------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9117972, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello.', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9554481, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.01251170877367258}, {'name': 'Adoration', 'score': 0.008131410926580429}, {'name': 'Aesthetic Appreciation', 'score': 0.016512403264641762}, {'name': 'Amusement', 'score': 0.0907270684838295}, {'name': 'Anger', 'score': 0.0005581923178397119}, {'name': 'Annoyance', 'score': 0.014846810139715672}, {'name': 'Anxiety', 'score': 0.0011913252528756857}, {'name': 'Awe', 'score': 0.008423087187111378}, {'name': 'Awkwardness', 'score': 0.06354702264070511}, {'name': 'Boredom', 'score': 0.10851363092660904}, {'name': 'Calmness', 'score': 0.31933408975601196}, {'name': 'Concentration', 'score': 0.016252553090453148}, {'name': 'Confusion', 'score': 0.029891543090343475}, {'name': 'Contemplation', 'score': 0.02187623828649521}, {'name': 'Contempt', 'score': 0.018504859879612923}, {'name': 'Contentment', 'score': 0.08336649090051651}, {'name': 'Craving', 'score': 0.0006236955523490906}, {'name': 'Desire', 'score': 0.000760009977966547}, {'name': 'Determination', 'score': 0.0022584907710552216}, {'name': 'Disappointment', 'score': 0.0025702782440930605}, {'name': 'Disapproval', 'score': 0.007185309659689665}, {'name': 'Disgust', 'score': 0.0015889910282567143}, {'name': 'Distress', 'score': 0.0006113385898061097}, {'name': 'Doubt', 'score': 0.00666783144697547}, {'name': 'Ecstasy', 'score': 0.0021341179963201284}, {'name': 'Embarrassment', 'score': 0.0033599617891013622}, {'name': 'Empathic Pain', 'score': 0.0008289636461995542}, {'name': 'Enthusiasm', 'score': 0.09442762285470963}, {'name': 'Entrancement', 'score': 0.006641392130404711}, {'name': 'Envy', 'score': 0.0008029191521927714}, {'name': 'Excitement', 'score': 0.05306636542081833}, {'name': 'Fear', 'score': 0.00027591444086283445}, {'name': 'Gratitude', 'score': 0.010404795408248901}, {'name': 'Guilt', 'score': 0.00038117836811579764}, {'name': 'Horror', 'score': 0.00014820862270426005}, {'name': 'Interest', 'score': 0.22310522198677063}, {'name': 'Joy', 'score': 0.08868356049060822}, {'name': 'Love', 'score': 0.005399726331233978}, {'name': 'Nostalgia', 'score': 0.004958468955010176}, {'name': 'Pain', 'score': 0.00010659849067451432}, {'name': 'Pride', 'score': 0.0036682302597910166}, {'name': 'Realization', 'score': 0.053931184113025665}, {'name': 'Relief', 'score': 0.0077752480283379555}, {'name': 'Romance', 'score': 0.001546808984130621}, {'name': 'Sadness', 'score': 0.0003105304786004126}, {'name': 'Sarcasm', 'score': 0.025866815820336342}, {'name': 'Satisfaction', 'score': 0.05627468600869179}, {'name': 'Shame', 'score': 0.0010352996177971363}, {'name': 'Surprise (negative)', 'score': 0.017590997740626335}, {'name': 'Surprise (positive)', 'score': 0.1599823534488678}, {'name': 'Sympathy', 'score': 0.002042335458099842}, {'name': 'Tiredness', 'score': 0.004060816951096058}, {'name': 'Triumph', 'score': 0.00503922114148736}], 'sentiment': [{'name': '1', 'score': 0.0005626916536130011}, {'name': '2', 'score': 0.0006312259938567877}, {'name': '3', 'score': 0.0011379551142454147}, {'name': '4', 'score': 0.0038008831907063723}, {'name': '5', 'score': 0.36965522170066833}, {'name': '6', 'score': 0.2254335731267929}, {'name': '7', 'score': 0.13478495180606842}, {'name': '8', 'score': 0.11552125215530396}, {'name': '9', 'score': 0.11766091734170914}], 'toxicity': [{'name': 'identity_hate', 'score': 0.002974089467898011}, {'name': 'insult', 'score': 0.0027754041366279125}, {'name': 'obscene', 'score': 0.002369341906160116}, {'name': 'severe_toxic', 'score': 0.0023268223740160465}, {'name': 'threat', 'score': 0.002879881300032139}, {'name': 'toxic', 'score': 0.006132675334811211}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9883465, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98850524, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98858786, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Minis. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1225}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in AI projects\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context retrieval quality in RAG pipeline\"', '2. \"Google Query Decomposition paper\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"Comparison between ChatGPD 4.0 and ChatGPD 4.0 Mini LLM\"']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described using RAG for an '\n",
      "              'automated screening interview agent, which aligns well with the '\n",
      "              'question.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline functions is mostly '\n",
      "              'clear, but there are some areas that could be improved for '\n",
      "              'clarity:\\n'\n",
      "              '   - The phrase \"break down the answer into separate Searchable '\n",
      "              'strings\" could be more clearly articulated. It would be '\n",
      "              'beneficial to specify how the answers are broken down and what '\n",
      "              'criteria are used for creating these searchable strings.\\n'\n",
      "              '   - The mention of \"the first two articles it will retrieve\" '\n",
      "              'could be confusing. It would be clearer to specify that the '\n",
      "              'system retrieves the contents of the first two articles from '\n",
      "              'Google and uses that information for verification.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate did not explicitly mention the specific '\n",
      "              'technologies or frameworks used in the implementation of the '\n",
      "              'RAG pipeline, which could enhance the answer. Including this '\n",
      "              'information would provide a more comprehensive view of their '\n",
      "              'technical skills.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'RAG pipelines and their application in a practical project, but '\n",
      "              'some areas could benefit from additional detail and clarity.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant and detailed explanation '\n",
      "              'of how they improved the context and retrieval quality of the '\n",
      "              \"RAG pipeline. They described breaking down the candidate's \"\n",
      "              'answer into six query strings, which is a clear and logical '\n",
      "              'approach to enhancing retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using Google to search for the first two '\n",
      "              'articles and drawing context from them is well-articulated. '\n",
      "              'However, it would be beneficial to clarify that the system '\n",
      "              'retrieves the contents of these articles for verification '\n",
      "              'purposes, as this could enhance understanding.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the information in a vector '\n",
      "              'store and using cosine similarity for relevance, which '\n",
      "              'demonstrates a solid understanding of the technical aspects of '\n",
      "              'the RAG pipeline. However, they could have elaborated on how '\n",
      "              'they determined the relevance of the retrieved information to '\n",
      "              \"the candidate's answer.\\n\"\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a strong point, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would be helpful to briefly '\n",
      "              'explain how this technique specifically addressed the '\n",
      "              'challenges they faced in their implementation.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a strong grasp of the RAG '\n",
      "              'pipeline and its application, with only minor areas for '\n",
      "              'improvement in clarity and detail.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 88%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant explanation regarding '\n",
      "              'model selection, stating that they chose JATGBD 4.0 mini due to '\n",
      "              'its balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion about the model '\n",
      "              'being discussed.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate effectively described the challenges faced in '\n",
      "              'prompt engineering, particularly regarding grammatical and '\n",
      "              'typographical errors in the speech-to-text conversion. They '\n",
      "              'explained how they prompted the LLM to overlook these errors, '\n",
      "              'which is a valid approach. However, it would be beneficial to '\n",
      "              'elaborate on the specific types of prompts used and how they '\n",
      "              'were structured to address these issues.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned not needing a more sophisticated '\n",
      "              'model like Claude SONET 3.5, which is a reasonable '\n",
      "              'justification for their choice of model. However, they could '\n",
      "              'have provided more context on how they assessed the sufficiency '\n",
      "              'of JATGBD 4.0 mini for their specific use case, which would '\n",
      "              'enhance the understanding of their decision-making process.\\n'\n",
      "              '\\n'\n",
      "              '4. While the candidate discussed the integration of the RAG '\n",
      "              'pipeline with the LLM, they could have provided more detail on '\n",
      "              'how the RAG pipeline was specifically implemented within the '\n",
      "              'context of the LLM, such as any particular techniques or '\n",
      "              'frameworks used beyond Langchain.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrated a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges faced, but there are areas where additional detail '\n",
      "              'and clarity could improve the response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 75%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which directly addresses the question about experience '\n",
      "              'with optimizing model performance. However, this could be seen '\n",
      "              'as a limitation since the question specifically asks for '\n",
      "              'experience in optimizing models, and the candidate does not '\n",
      "              'provide any examples or insights into potential optimization '\n",
      "              'strategies they might have considered.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate mentions that they had to determine which '\n",
      "              'model best suited the project area, which is relevant. However, '\n",
      "              'they do not elaborate on any specific optimization techniques '\n",
      "              'they might have used to enhance speed or cost efficiency. While '\n",
      "              'they discuss model selection, they do not connect this to '\n",
      "              'optimization in terms of performance metrics, which is a key '\n",
      "              'aspect of the question.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate discusses the selection of Claude 3.5 Sonnet '\n",
      "              'and ChatGPD 4.0 Mini based on intelligence, cost-effectiveness, '\n",
      "              'and speed. While this is relevant, it lacks a direct connection '\n",
      "              'to optimization strategies. They could have provided insights '\n",
      "              'into how they evaluated these models in terms of performance '\n",
      "              'metrics or any adjustments made to improve their efficiency.\\n'\n",
      "              '\\n'\n",
      "              '4. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate does not discuss any optimization '\n",
      "              'considerations related to this external service. It would have '\n",
      "              'been beneficial to mention how they assessed the '\n",
      "              'cost-effectiveness or speed of this service in relation to '\n",
      "              'their project needs.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's explanation of feeding the sentiment and \"\n",
      "              'accuracy verifier scores back into Claude Sonnet 3.5 is '\n",
      "              'relevant, but again, it lacks a discussion of how this process '\n",
      "              'was optimized for performance or cost efficiency.\\n'\n",
      "              '\\n'\n",
      "              'Overall, while the candidate provides some relevant information '\n",
      "              'regarding model selection and the context of their project, '\n",
      "              'they do not adequately address the specific aspects of '\n",
      "              'optimizing model performance in terms of speed and cost '\n",
      "              'efficiency, leading to a lower accuracy score.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-6------------------------\n",
      "---------------------------1724629705-7------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9236462, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello?', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.9541237, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.004917457699775696}, {'name': 'Adoration', 'score': 0.004174551926553249}, {'name': 'Aesthetic Appreciation', 'score': 0.005793654825538397}, {'name': 'Amusement', 'score': 0.013763082213699818}, {'name': 'Anger', 'score': 0.001238273223862052}, {'name': 'Annoyance', 'score': 0.009719441644847393}, {'name': 'Anxiety', 'score': 0.05329950153827667}, {'name': 'Awe', 'score': 0.005215151701122522}, {'name': 'Awkwardness', 'score': 0.15866424143314362}, {'name': 'Boredom', 'score': 0.016056111082434654}, {'name': 'Calmness', 'score': 0.09533677995204926}, {'name': 'Concentration', 'score': 0.048347994685173035}, {'name': 'Confusion', 'score': 0.33356761932373047}, {'name': 'Contemplation', 'score': 0.11072219163179398}, {'name': 'Contempt', 'score': 0.007205050904303789}, {'name': 'Contentment', 'score': 0.01255878061056137}, {'name': 'Craving', 'score': 0.003111861413344741}, {'name': 'Desire', 'score': 0.004330466967076063}, {'name': 'Determination', 'score': 0.009868061169981956}, {'name': 'Disappointment', 'score': 0.002099820179864764}, {'name': 'Disapproval', 'score': 0.0043081543408334255}, {'name': 'Disgust', 'score': 0.0014217490097507834}, {'name': 'Distress', 'score': 0.007642513141036034}, {'name': 'Doubt', 'score': 0.14205265045166016}, {'name': 'Ecstasy', 'score': 0.0016397946747019887}, {'name': 'Embarrassment', 'score': 0.007673530839383602}, {'name': 'Empathic Pain', 'score': 0.004410985391587019}, {'name': 'Enthusiasm', 'score': 0.05461122840642929}, {'name': 'Entrancement', 'score': 0.010877513326704502}, {'name': 'Envy', 'score': 0.0010307441698387265}, {'name': 'Excitement', 'score': 0.048237673938274384}, {'name': 'Fear', 'score': 0.013723790645599365}, {'name': 'Gratitude', 'score': 0.005805298686027527}, {'name': 'Guilt', 'score': 0.0027865080628544092}, {'name': 'Horror', 'score': 0.000945321167819202}, {'name': 'Interest', 'score': 0.511585533618927}, {'name': 'Joy', 'score': 0.013462582603096962}, {'name': 'Love', 'score': 0.005053339526057243}, {'name': 'Nostalgia', 'score': 0.0022508178371936083}, {'name': 'Pain', 'score': 0.0005023297271691263}, {'name': 'Pride', 'score': 0.002255357103422284}, {'name': 'Realization', 'score': 0.03419802337884903}, {'name': 'Relief', 'score': 0.004991704598069191}, {'name': 'Romance', 'score': 0.00645497627556324}, {'name': 'Sadness', 'score': 0.0007819914608262479}, {'name': 'Sarcasm', 'score': 0.006686879321932793}, {'name': 'Satisfaction', 'score': 0.010099216364324093}, {'name': 'Shame', 'score': 0.0032989843748509884}, {'name': 'Surprise (negative)', 'score': 0.030789455398917198}, {'name': 'Surprise (positive)', 'score': 0.09750684350728989}, {'name': 'Sympathy', 'score': 0.0067804595455527306}, {'name': 'Tiredness', 'score': 0.0020965617150068283}, {'name': 'Triumph', 'score': 0.002277676248922944}], 'sentiment': [{'name': '1', 'score': 0.0006536049768328667}, {'name': '2', 'score': 0.0008608695352450013}, {'name': '3', 'score': 0.0019260875415056944}, {'name': '4', 'score': 0.010997419245541096}, {'name': '5', 'score': 0.6381703019142151}, {'name': '6', 'score': 0.22112508118152618}, {'name': '7', 'score': 0.05380028858780861}, {'name': '8', 'score': 0.031817760318517685}, {'name': '9', 'score': 0.023089038208127022}], 'toxicity': [{'name': 'identity_hate', 'score': 0.003185395384207368}, {'name': 'insult', 'score': 0.002416277304291725}, {'name': 'obscene', 'score': 0.002234936458989978}, {'name': 'severe_toxic', 'score': 0.0025471309199929237}, {'name': 'threat', 'score': 0.002981527242809534}, {'name': 'toxic', 'score': 0.00495997816324234}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98820305, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9883817, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector stall that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1327}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9890872, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Minis. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1225}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"retrieval-augmented generation (RAG) pipeline for accuracy verification in automated screening interview agent\"', '2. \"implementing retrieval-augmented generation for accuracy verification in natural language processing\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's an interesting project. Can you elaborate on the specific challenges you faced while implementing the RAG pipeline for your accuracy verifier? How did you address issues like retrieval quality or context relevance?\n",
      "\n",
      "queries are: ['1. \"Improving context and retrieval quality in RAG pipeline\"', '2. \"Query Decomposition technique for accuracy verification in information retrieval\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: That's a sophisticated approach. How did you handle the integration of this RAG pipeline with the large language model? Were there any specific challenges in terms of prompt engineering or model selection?\n",
      "\n",
      "queries are: ['1. \"JATGBD 4.0 mini model for language processing\" ', '2. \"Challenges in prompt engineering for large language models\"']\n",
      "\n",
      "\n",
      "search queries generated for answer to question: Thank you for sharing those details. Can you discuss any experience you have with optimizing model performance, particularly in terms of speed and cost efficiency?\n",
      "\n",
      "queries are: ['1. \"Claude 3.5 Sonnet LLM features and capabilities\"', '2. \"ChatGPD 4.0 Mini LLM cost effectiveness and speed comparison\"']\n",
      "\n",
      "\n",
      "Warning: No documents were split. The retriever will be empty.\n",
      "No documents retrieved from webscrapping \n",
      "\n",
      "Feedback is being returned from accuracy verifier\n",
      "The Feedback JSON from the sentiment analyser and accuracy verifier: \n",
      "\n",
      "[{'candidate': \" I'm sure so I'm studying artificial intelligence at the \"\n",
      "               'University of Kent currently and for my final dissertation. '\n",
      "               \"I'm working on making a automated screening Interview agent \"\n",
      "               'and to implement this I have used a rag pipeline mainly as the '\n",
      "               'accuracy verifier so what happens is when the Candidate '\n",
      "               'answers their questions it goes through two pipelines one is '\n",
      "               'the sentiment analysis and one is the accuracy verifier For '\n",
      "               'the accuracy verifier I have implemented a retrieval augmented '\n",
      "               'generation, which would basically break down the answer into '\n",
      "               'separate Searchable strings which will then be searched on '\n",
      "               'Google and The first two articles it will retrieve the '\n",
      "               'contents of the first two articles and input that in the '\n",
      "               'context of the LLM So the LM has more up-to-date information '\n",
      "               'to verify with the whether the answer from the candidate is '\n",
      "               'accurate or not and to give an accuracy percentage',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a relevant project involving a '\n",
      "              'retrieval-augmented generation (RAG) pipeline, which is a '\n",
      "              'positive aspect of the answer. They described their work on an '\n",
      "              'automated screening interview agent and how they implemented a '\n",
      "              'RAG pipeline for accuracy verification.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of how the RAG pipeline works is mostly '\n",
      "              'clear, but there are some areas that could be more precise. For '\n",
      "              'instance, the candidate mentions breaking down the answer into '\n",
      "              '\"separate searchable strings\" and searching on Google. While '\n",
      "              'this is a valid approach, it would be beneficial to clarify how '\n",
      "              'the retrieved articles are processed and integrated into the '\n",
      "              'context of the language model (LM) for accuracy verification.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate states that the LM uses the contents of the '\n",
      "              \"first two articles to verify the candidate's answer. It would \"\n",
      "              'enhance the answer to explain how the LM determines the '\n",
      "              'accuracy percentage based on this information, as this is a '\n",
      "              'critical part of the RAG process.\\n'\n",
      "              '\\n'\n",
      "              '4. The phrase \"to give an accuracy percentage\" at the end of '\n",
      "              'the answer could be elaborated upon. It would be helpful to '\n",
      "              'understand the methodology used to calculate this percentage, '\n",
      "              \"as it is a key aspect of the project's functionality.\\n\"\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'RAG pipelines and their application in their project, but '\n",
      "              'additional clarity and detail in certain areas would strengthen '\n",
      "              'their response.',\n",
      "  'interviewer': 'Hello! Thank you for taking the time to speak with me today '\n",
      "                 \"about the Entry-Level RAG AI Engineer role. I'd like to \"\n",
      "                 'start by asking you a few questions about your experience '\n",
      "                 \"and skills. Could you tell me about any projects you've \"\n",
      "                 'worked on involving retrieval-augmented generation (RAG) '\n",
      "                 'pipelines?'},\n",
      " {'candidate': ' Yes, so to improve the context or the retrieval quality of '\n",
      "               'the rag pipeline, I had to break down the answer from the '\n",
      "               'candidate into searchable strings with the help of an LLM. So '\n",
      "               \"let's say an answer can be broken down into six query strings. \"\n",
      "               'Each of these six query strings would then be used to search '\n",
      "               'in Google and we would draw the context from the first two web '\n",
      "               'pages. So in a total we would get the information from a total '\n",
      "               'of 12 web pages for one answer. So this I think is plenty of '\n",
      "               'information to feed the LLM. This answer, this document would '\n",
      "               'then be stored in a vector store and when the LLM would be '\n",
      "               'queried on a specific topic or like when the LLM wanted to '\n",
      "               'verify the accuracy of a certain answer it would then use a '\n",
      "               'cosine similarity to find out the relevant portions of the '\n",
      "               'vector store that are relevant to the answer. And doing this, '\n",
      "               'it would vastly improve the quality of the answers fetched. I '\n",
      "               'got this from a paper written by Google called Query '\n",
      "               'Decomposition. This was the technique they used and this '\n",
      "               'overcame the shortcomings of just searching for two or three '\n",
      "               'websites instead of getting a more holistic picture of the '\n",
      "               'entire topics being discussed in the answer.',\n",
      "  'feedback': '**Accuracy Percentage:** 90%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate effectively elaborated on the challenges faced '\n",
      "              'while implementing the RAG pipeline, specifically addressing '\n",
      "              'retrieval quality and context relevance. They described '\n",
      "              \"breaking down the candidate's answer into searchable strings, \"\n",
      "              'which is a valid approach to enhance retrieval quality.\\n'\n",
      "              '\\n'\n",
      "              '2. The explanation of using six query strings to search Google '\n",
      "              'and retrieving context from the first two web pages is clear. '\n",
      "              'However, it would be beneficial to clarify how the information '\n",
      "              'from these web pages is processed and integrated into the '\n",
      "              'context of the language model (LLM) for accuracy verification. '\n",
      "              'This detail is crucial for understanding the overall '\n",
      "              'effectiveness of the RAG pipeline.\\n'\n",
      "              '\\n'\n",
      "              '3. The candidate mentioned storing the retrieved documents in a '\n",
      "              'vector store and using cosine similarity to find relevant '\n",
      "              'portions. This is a strong point, but it would enhance the '\n",
      "              'answer to explain how the LLM utilizes this information to '\n",
      "              \"determine the accuracy of the candidate's answer. Providing \"\n",
      "              'insight into the methodology for calculating the accuracy '\n",
      "              'percentage would also strengthen the response.\\n'\n",
      "              '\\n'\n",
      "              '4. The reference to the Google paper on Query Decomposition is '\n",
      "              \"a good addition, as it shows the candidate's engagement with \"\n",
      "              'existing research. However, it would be helpful to briefly '\n",
      "              'explain how this technique specifically addresses the '\n",
      "              'shortcomings of searching only two or three websites, as this '\n",
      "              'would provide a clearer understanding of its application in '\n",
      "              'their project.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the RAG pipeline and its implementation challenges, but '\n",
      "              'additional clarity and detail in certain areas would further '\n",
      "              'enhance their response.',\n",
      "  'interviewer': \"That's an interesting project. Can you elaborate on the \"\n",
      "                 'specific challenges you faced while implementing the RAG '\n",
      "                 'pipeline for your accuracy verifier? How did you address '\n",
      "                 'issues like retrieval quality or context relevance?'},\n",
      " {'candidate': ' For model selection, we chose JATGBD 4.0 mini mainly because '\n",
      "               'we used Langchain to implement the rank pipeline and GPT 4.0 '\n",
      "               'mini had the perfect balance of intelligence and cost '\n",
      "               'effectiveness and also speed that we had to manage. And this '\n",
      "               'was just to verify the answer. So we did not go for a more '\n",
      "               'sophisticated model such as Claude SONET 3.5 which is '\n",
      "               'considered the most intelligent LLM till now. We did not need '\n",
      "               'such a high powered LLM, we just needed a cost effective LLM '\n",
      "               'to just verify the answer and make searchable strings and '\n",
      "               'JATGBD 4.0 mini was perfect for the job. Apart from this, for '\n",
      "               'prompt engineering, yes, I had to write several prompts to '\n",
      "               'give the last rank pipeline to verify the answer. So what '\n",
      "               'would happen is when we converted speech to text from the '\n",
      "               'interview, some of the text had grammatical errors or '\n",
      "               'typographical errors which is common for most text translation '\n",
      "               'apps. So to overcome this, I had to prompt the LLM to '\n",
      "               'specifically overlook grammatical errors or to make sense of '\n",
      "               'words that were not properly converted but were close to the '\n",
      "               'actual word that the candidate was trying to explain. So these '\n",
      "               'were some of the challenges that I faced.',\n",
      "  'feedback': '**Accuracy Percentage:** 85%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate provided a clear rationale for their model '\n",
      "              'selection, stating that they chose JATGBD 4.0 mini due to its '\n",
      "              'balance of intelligence, cost-effectiveness, and speed. '\n",
      "              'However, the mention of \"GPT 4.0 mini\" seems to be a '\n",
      "              'transcription error, as it should likely refer to \"JATGBD 4.0 '\n",
      "              'mini\" throughout. This could lead to confusion regarding the '\n",
      "              'model being discussed.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate effectively described the challenges faced in '\n",
      "              'prompt engineering, particularly in dealing with grammatical '\n",
      "              'and typographical errors in the transcribed text. They '\n",
      "              'mentioned prompting the LLM to overlook these errors, which is '\n",
      "              'a valid approach. However, it would be beneficial to elaborate '\n",
      "              'on how these prompts were structured to ensure the LLM could '\n",
      "              'effectively interpret the intended meaning.\\n'\n",
      "              '\\n'\n",
      "              '3. The explanation of the integration of the RAG pipeline with '\n",
      "              'the LLM is mostly clear, but it lacks detail on how the outputs '\n",
      "              'from the RAG pipeline were specifically utilized by the LLM to '\n",
      "              \"verify the accuracy of the candidate's answers. Providing more \"\n",
      "              'insight into this process would enhance the understanding of '\n",
      "              'the overall workflow.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's mention of not needing a more sophisticated \"\n",
      "              'model like Claude SONET 3.5 is appropriate, but it would be '\n",
      "              'helpful to briefly explain why the chosen model was sufficient '\n",
      "              'for their specific use case. This would provide additional '\n",
      "              'context for their decision-making process.\\n'\n",
      "              '\\n'\n",
      "              'Overall, the candidate demonstrates a solid understanding of '\n",
      "              'the integration of the RAG pipeline with the LLM and the '\n",
      "              'challenges faced during implementation, but additional clarity '\n",
      "              'and detail in certain areas would strengthen their response.',\n",
      "  'interviewer': \"That's a sophisticated approach. How did you handle the \"\n",
      "                 'integration of this RAG pipeline with the large language '\n",
      "                 'model? Were there any specific challenges in terms of prompt '\n",
      "                 'engineering or model selection?'},\n",
      " {'candidate': ' So far I have not optimized any model. By optimizing I am '\n",
      "               'thinking you mean fine tuning model. So for the specific '\n",
      "               'project fine tuning was not necessary. However, we had to '\n",
      "               'determine which model best suited the specific area of our '\n",
      "               'project. So for example, for the real time conversation where '\n",
      "               'the LLM had to generate questions and interact with the '\n",
      "               'candidate, we went with Claude 3.5 Sonnet which is the most '\n",
      "               'intelligent LLM till date as preferred by most developers. And '\n",
      "               'again for the accuracy verifier we went with ChatGPD 4.0 Mini '\n",
      "               'which is a cut down version of ChatGPD 4.0 which itself is a '\n",
      "               'very powerful LLM. However, 4.0 Mini has the right balance of '\n",
      "               'intelligence and cost effectiveness and also speed. Then for '\n",
      "               'the sentiment analysis we went with Hume AI which is an '\n",
      "               'external service that does the sentiment analysis directly '\n",
      "               \"from audio and video feed. So the service, we don't know the \"\n",
      "               'specific implementation of the service because we are paying '\n",
      "               'to use the service. And after that getting the sentiment and '\n",
      "               'accuracy verifier score we then feed it into Claude Sonnet 3.5 '\n",
      "               'again to make sense of the answers that the candidate made '\n",
      "               'from both the accuracy verifier and from the sentiment '\n",
      "               'analysis and to give the final verdict of the candidate. So '\n",
      "               'these are the main considerations we made when choosing an '\n",
      "               'LLM.',\n",
      "  'feedback': '**Accuracy Percentage:** 80%\\n'\n",
      "              '\\n'\n",
      "              '**Feedback:**\\n'\n",
      "              '\\n'\n",
      "              '1. The candidate stated, \"So far I have not optimized any '\n",
      "              'model,\" which indicates a lack of direct experience in '\n",
      "              'optimizing model performance. While they clarified that they '\n",
      "              'interpreted \"optimizing\" as \"fine-tuning,\" it would have been '\n",
      "              'beneficial to mention any indirect experiences or '\n",
      "              'considerations they had regarding optimization, even if it was '\n",
      "              'not part of their specific project.\\n'\n",
      "              '\\n'\n",
      "              '2. The candidate discussed the selection of models based on '\n",
      "              'their suitability for specific tasks, which is relevant. '\n",
      "              'However, they did not provide specific examples of how they '\n",
      "              'considered speed and cost efficiency in their decision-making '\n",
      "              'process. While they mentioned that Claude 3.5 Sonnet is '\n",
      "              'preferred by developers, they could have elaborated on how they '\n",
      "              'assessed the trade-offs between model performance and cost in '\n",
      "              'their project.\\n'\n",
      "              '\\n'\n",
      "              '3. The mention of using Hume AI for sentiment analysis is '\n",
      "              'relevant, but the candidate did not explain how the integration '\n",
      "              'of this external service impacted the overall performance or '\n",
      "              'cost efficiency of the system. Providing insight into this '\n",
      "              'aspect would have strengthened their response.\\n'\n",
      "              '\\n'\n",
      "              \"4. The candidate's explanation of feeding the sentiment and \"\n",
      "              'accuracy verifier scores back into Claude Sonnet 3.5 is a good '\n",
      "              'point, but they did not clarify how this process contributes to '\n",
      "              'optimizing model performance in terms of speed and cost '\n",
      "              'efficiency. More detail on this integration would enhance the '\n",
      "              'understanding of their approach.\\n'\n",
      "              '\\n'\n",
      "              \"5. The candidate's response could benefit from a more explicit \"\n",
      "              'connection between their model selection and the optimization '\n",
      "              'of performance, particularly in terms of speed and cost '\n",
      "              'efficiency. They should have highlighted any specific '\n",
      "              'strategies or metrics they used to evaluate these factors '\n",
      "              'during their project.',\n",
      "  'interviewer': 'Thank you for sharing those details. Can you discuss any '\n",
      "                 'experience you have with optimizing model performance, '\n",
      "                 'particularly in terms of speed and cost efficiency?'}]\n",
      "---------------------------1724629705-7------------------------\n",
      "---------------------------1724629705-8------------------------\n",
      "Current working directory: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\n",
      "Full audio directory path: d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\data\\interviews\\1724629705\\audio\n",
      "File found: audio_1_1724629705.wav\n",
      "File found: audio_2_1724629705.wav\n",
      "File found: audio_3_1724629705.wav\n",
      "File found: audio_4_1724629705.wav\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_1_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '0bff7efa6ac3802fbe9bf25f5b4220c7'}, 'results': {'predictions': [{'file': 'audio_1_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.9169881, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'Hello.', 'position': {'begin': 0, 'end': 6}, 'time': {'begin': 0.116538465, 'end': 0.4273077}, 'confidence': 0.95389336, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.01251170877367258}, {'name': 'Adoration', 'score': 0.008131410926580429}, {'name': 'Aesthetic Appreciation', 'score': 0.016512403264641762}, {'name': 'Amusement', 'score': 0.0907270684838295}, {'name': 'Anger', 'score': 0.0005581923178397119}, {'name': 'Annoyance', 'score': 0.014846810139715672}, {'name': 'Anxiety', 'score': 0.0011913252528756857}, {'name': 'Awe', 'score': 0.008423087187111378}, {'name': 'Awkwardness', 'score': 0.06354702264070511}, {'name': 'Boredom', 'score': 0.10851363092660904}, {'name': 'Calmness', 'score': 0.31933408975601196}, {'name': 'Concentration', 'score': 0.016252553090453148}, {'name': 'Confusion', 'score': 0.029891543090343475}, {'name': 'Contemplation', 'score': 0.02187623828649521}, {'name': 'Contempt', 'score': 0.018504859879612923}, {'name': 'Contentment', 'score': 0.08336649090051651}, {'name': 'Craving', 'score': 0.0006236955523490906}, {'name': 'Desire', 'score': 0.000760009977966547}, {'name': 'Determination', 'score': 0.0022584907710552216}, {'name': 'Disappointment', 'score': 0.0025702782440930605}, {'name': 'Disapproval', 'score': 0.007185309659689665}, {'name': 'Disgust', 'score': 0.0015889910282567143}, {'name': 'Distress', 'score': 0.0006113385898061097}, {'name': 'Doubt', 'score': 0.00666783144697547}, {'name': 'Ecstasy', 'score': 0.0021341179963201284}, {'name': 'Embarrassment', 'score': 0.0033599617891013622}, {'name': 'Empathic Pain', 'score': 0.0008289636461995542}, {'name': 'Enthusiasm', 'score': 0.09442762285470963}, {'name': 'Entrancement', 'score': 0.006641392130404711}, {'name': 'Envy', 'score': 0.0008029191521927714}, {'name': 'Excitement', 'score': 0.05306636542081833}, {'name': 'Fear', 'score': 0.00027591444086283445}, {'name': 'Gratitude', 'score': 0.010404795408248901}, {'name': 'Guilt', 'score': 0.00038117836811579764}, {'name': 'Horror', 'score': 0.00014820862270426005}, {'name': 'Interest', 'score': 0.22310522198677063}, {'name': 'Joy', 'score': 0.08868356049060822}, {'name': 'Love', 'score': 0.005399726331233978}, {'name': 'Nostalgia', 'score': 0.004958468955010176}, {'name': 'Pain', 'score': 0.00010659849067451432}, {'name': 'Pride', 'score': 0.0036682302597910166}, {'name': 'Realization', 'score': 0.053931184113025665}, {'name': 'Relief', 'score': 0.0077752480283379555}, {'name': 'Romance', 'score': 0.001546808984130621}, {'name': 'Sadness', 'score': 0.0003105304786004126}, {'name': 'Sarcasm', 'score': 0.025866815820336342}, {'name': 'Satisfaction', 'score': 0.05627468600869179}, {'name': 'Shame', 'score': 0.0010352996177971363}, {'name': 'Surprise (negative)', 'score': 0.017590997740626335}, {'name': 'Surprise (positive)', 'score': 0.1599823534488678}, {'name': 'Sympathy', 'score': 0.002042335458099842}, {'name': 'Tiredness', 'score': 0.004060816951096058}, {'name': 'Triumph', 'score': 0.00503922114148736}], 'sentiment': [{'name': '1', 'score': 0.0005626916536130011}, {'name': '2', 'score': 0.0006312259938567877}, {'name': '3', 'score': 0.0011379551142454147}, {'name': '4', 'score': 0.0038008831907063723}, {'name': '5', 'score': 0.36965522170066833}, {'name': '6', 'score': 0.2254335731267929}, {'name': '7', 'score': 0.13478495180606842}, {'name': '8', 'score': 0.11552125215530396}, {'name': '9', 'score': 0.11766091734170914}], 'toxicity': [{'name': 'identity_hate', 'score': 0.002974089467898011}, {'name': 'insult', 'score': 0.0027754041366279125}, {'name': 'obscene', 'score': 0.002369341906160116}, {'name': 'severe_toxic', 'score': 0.0023268223740160465}, {'name': 'threat', 'score': 0.002879881300032139}, {'name': 'toxic', 'score': 0.006132675334811211}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_1_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_2_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '70a9526b605ea35c351a038ec2156ba8'}, 'results': {'predictions': [{'file': 'audio_2_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98829633, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"I'm sure. So I'm studying artificial intelligence of the university of Kent currently and for my final dis edition. I'm working on making automated screening interview agent. To implement this, I have used a rack pipeline, Mainly as the accuracy verifies verify. So what happens is when the candidate answers their questions. It goes through two pipelines. One is the sentiment analysis and one is the accuracy verify. For the accuracy verify, I have implemented our retrieval augmented generation, which would basically break down the answer into separate searchable strings, which will then be searched on Google. And the first two articles, it will retrieve the contents of the first two articles and input that in the context of the L. So the L has more up to date information to verify whether the whether the answer from the candidate is accurate or not and to give an accuracy percentage.\", 'position': {'begin': 0, 'end': 895}, 'time': {'begin': 0.5175472, 'end': 56.166195}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020867476239800453}, {'name': 'Adoration', 'score': 0.0015661435900256038}, {'name': 'Aesthetic Appreciation', 'score': 0.032758116722106934}, {'name': 'Amusement', 'score': 0.0052788956090807915}, {'name': 'Anger', 'score': 0.0020649421494454145}, {'name': 'Annoyance', 'score': 0.032586049288511276}, {'name': 'Anxiety', 'score': 0.008538252674043179}, {'name': 'Awe', 'score': 0.0050153546035289764}, {'name': 'Awkwardness', 'score': 0.004757682792842388}, {'name': 'Boredom', 'score': 0.022854255512356758}, {'name': 'Calmness', 'score': 0.11964289098978043}, {'name': 'Concentration', 'score': 0.8382731080055237}, {'name': 'Confusion', 'score': 0.03996102511882782}, {'name': 'Contemplation', 'score': 0.36052405834198}, {'name': 'Contempt', 'score': 0.04031214490532875}, {'name': 'Contentment', 'score': 0.0699230283498764}, {'name': 'Craving', 'score': 0.0018265082035213709}, {'name': 'Desire', 'score': 0.0002731457934714854}, {'name': 'Determination', 'score': 0.25358590483665466}, {'name': 'Disappointment', 'score': 0.008718395605683327}, {'name': 'Disapproval', 'score': 0.016138896346092224}, {'name': 'Disgust', 'score': 0.0032667110208421946}, {'name': 'Distress', 'score': 0.005003053229302168}, {'name': 'Doubt', 'score': 0.04147962108254433}, {'name': 'Ecstasy', 'score': 0.0008104772423394024}, {'name': 'Embarrassment', 'score': 0.0012096711434423923}, {'name': 'Empathic Pain', 'score': 0.002473619068041444}, {'name': 'Enthusiasm', 'score': 0.07662298530340195}, {'name': 'Entrancement', 'score': 0.012313921004533768}, {'name': 'Envy', 'score': 0.0004491372383199632}, {'name': 'Excitement', 'score': 0.010765568353235722}, {'name': 'Fear', 'score': 0.002865805523470044}, {'name': 'Gratitude', 'score': 0.05530688911676407}, {'name': 'Guilt', 'score': 0.0005613525281660259}, {'name': 'Horror', 'score': 0.0004426266241353005}, {'name': 'Interest', 'score': 0.2875368595123291}, {'name': 'Joy', 'score': 0.0025231139734387398}, {'name': 'Love', 'score': 0.0004412215785123408}, {'name': 'Nostalgia', 'score': 0.0025047531817108393}, {'name': 'Pain', 'score': 0.0008867710712365806}, {'name': 'Pride', 'score': 0.018499240279197693}, {'name': 'Realization', 'score': 0.19202357530593872}, {'name': 'Relief', 'score': 0.03130050748586655}, {'name': 'Romance', 'score': 0.00011835309123853222}, {'name': 'Sadness', 'score': 0.0007202433189377189}, {'name': 'Sarcasm', 'score': 0.01115118246525526}, {'name': 'Satisfaction', 'score': 0.2261616289615631}, {'name': 'Shame', 'score': 0.0012533975532278419}, {'name': 'Surprise (negative)', 'score': 0.005670612677931786}, {'name': 'Surprise (positive)', 'score': 0.018636632710695267}, {'name': 'Sympathy', 'score': 0.002606848953291774}, {'name': 'Tiredness', 'score': 0.007680388167500496}, {'name': 'Triumph', 'score': 0.0629279762506485}], 'sentiment': [{'name': '1', 'score': 0.001135596539825201}, {'name': '2', 'score': 0.0015475869877263904}, {'name': '3', 'score': 0.0016265560407191515}, {'name': '4', 'score': 0.0032312602270394564}, {'name': '5', 'score': 0.7084700465202332}, {'name': '6', 'score': 0.08665025234222412}, {'name': '7', 'score': 0.05303305760025978}, {'name': '8', 'score': 0.0630059465765953}, {'name': '9', 'score': 0.07855338603258133}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0041016447357833385}, {'name': 'insult', 'score': 0.001723858411423862}, {'name': 'obscene', 'score': 0.001753958873450756}, {'name': 'severe_toxic', 'score': 0.003110885852947831}, {'name': 'threat', 'score': 0.003645808668807149}, {'name': 'toxic', 'score': 0.002875712001696229}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_2_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_3_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '5705a3916e86facf2b6202b9fa12c165'}, 'results': {'predictions': [{'file': 'audio_3_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98814404, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': \"Yes. So to improve the context of the retrieval quality of the rag pipeline, I had to break down. The answer from the candidate and the searchable strings with the help of an. So let's say an answer can be broken down into six query strings. Each of the six query strings would then go on to... Each of... Sorry. Each of these six query strings would then be used to search in Google, and we would draw the context from the first two web pages. So in a total, we would get the information from a total of twelve web pages for one answer. So this, I think is plenty of information to feed the L. This answer, this documents would then be stored in a vector stored and when the L would be que on a specific topic or, like, one to... An l wanted you to verify the accuracy of a certain, it would then use a cosign sign similarity to find out the relevant portions of the vector, that are relevant to the answer, and doing this, it would vastly improve the quality of the answers fetched. I got this from paper, develop not developed. I... Got this from people written by Google called Quality composition. This was the technique they used, and this over... Uber overcame the shortcomings so just searching for two or three websites instead of getting a more holistic picture of the entire topics being discussed in the answer.\", 'position': {'begin': 0, 'end': 1323}, 'time': {'begin': 5.3199997, 'end': 100.6439}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.020808063447475433}, {'name': 'Adoration', 'score': 0.001254769042134285}, {'name': 'Aesthetic Appreciation', 'score': 0.021469533443450928}, {'name': 'Amusement', 'score': 0.01876199059188366}, {'name': 'Anger', 'score': 0.015144769102334976}, {'name': 'Annoyance', 'score': 0.23666991293430328}, {'name': 'Anxiety', 'score': 0.0015264194225892425}, {'name': 'Awe', 'score': 0.010324472561478615}, {'name': 'Awkwardness', 'score': 0.008019606582820415}, {'name': 'Boredom', 'score': 0.03264814615249634}, {'name': 'Calmness', 'score': 0.09112094342708588}, {'name': 'Concentration', 'score': 0.19919000566005707}, {'name': 'Confusion', 'score': 0.044877514243125916}, {'name': 'Contemplation', 'score': 0.15646770596504211}, {'name': 'Contempt', 'score': 0.14124396443367004}, {'name': 'Contentment', 'score': 0.04971728101372719}, {'name': 'Craving', 'score': 0.0004181693366263062}, {'name': 'Desire', 'score': 0.0005121738067828119}, {'name': 'Determination', 'score': 0.08143174648284912}, {'name': 'Disappointment', 'score': 0.09353584051132202}, {'name': 'Disapproval', 'score': 0.23318269848823547}, {'name': 'Disgust', 'score': 0.027429314330220222}, {'name': 'Distress', 'score': 0.003368454286828637}, {'name': 'Doubt', 'score': 0.025086307898163795}, {'name': 'Ecstasy', 'score': 0.0007996402564458549}, {'name': 'Embarrassment', 'score': 0.003262067912146449}, {'name': 'Empathic Pain', 'score': 0.003630182472988963}, {'name': 'Enthusiasm', 'score': 0.06152394786477089}, {'name': 'Entrancement', 'score': 0.007395419757813215}, {'name': 'Envy', 'score': 0.0022861084435135126}, {'name': 'Excitement', 'score': 0.012594147585332394}, {'name': 'Fear', 'score': 0.0005040622199885547}, {'name': 'Gratitude', 'score': 0.009668882936239243}, {'name': 'Guilt', 'score': 0.0008871213649399579}, {'name': 'Horror', 'score': 0.00078134163049981}, {'name': 'Interest', 'score': 0.19914337992668152}, {'name': 'Joy', 'score': 0.0029839242342859507}, {'name': 'Love', 'score': 0.0002288547984790057}, {'name': 'Nostalgia', 'score': 0.0020122856367379427}, {'name': 'Pain', 'score': 0.00070614751894027}, {'name': 'Pride', 'score': 0.022655297070741653}, {'name': 'Realization', 'score': 0.2237192690372467}, {'name': 'Relief', 'score': 0.02439228817820549}, {'name': 'Romance', 'score': 9.119707101490349e-05}, {'name': 'Sadness', 'score': 0.0014377792831510305}, {'name': 'Sarcasm', 'score': 0.052469972521066666}, {'name': 'Satisfaction', 'score': 0.17914029955863953}, {'name': 'Shame', 'score': 0.004415595903992653}, {'name': 'Surprise (negative)', 'score': 0.08195500075817108}, {'name': 'Surprise (positive)', 'score': 0.06367845833301544}, {'name': 'Sympathy', 'score': 0.004424653947353363}, {'name': 'Tiredness', 'score': 0.010416434146463871}, {'name': 'Triumph', 'score': 0.06883395463228226}], 'sentiment': [{'name': '1', 'score': 0.003974802326411009}, {'name': '2', 'score': 0.023127347230911255}, {'name': '3', 'score': 0.034780971705913544}, {'name': '4', 'score': 0.09737690538167953}, {'name': '5', 'score': 0.27668139338493347}, {'name': '6', 'score': 0.24386049807071686}, {'name': '7', 'score': 0.17257361114025116}, {'name': '8', 'score': 0.0691724643111229}, {'name': '9', 'score': 0.020153382793068886}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0036596707068383694}, {'name': 'insult', 'score': 0.0017336253076791763}, {'name': 'obscene', 'score': 0.001898844842799008}, {'name': 'severe_toxic', 'score': 0.0025924695655703545}, {'name': 'threat', 'score': 0.0033337101340293884}, {'name': 'toxic', 'score': 0.003517822828143835}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_3_1724629705.wav: Failed to process predictions\n",
      "Processing file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing audio file: data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav\n",
      "File exists: True\n",
      "Job submitted successfully\n",
      "Job completed\n",
      "[{'source': {'type': 'file', 'filename': 'audio_4_1724629705.wav', 'content_type': 'audio/wav', 'md5sum': '3475dd174cc4b9dc225984655c1d2eb2'}, 'results': {'predictions': [{'file': 'audio_4_1724629705.wav', 'file_type': 'audio', 'models': {'language': {'metadata': {'confidence': 0.98853487, 'detected_language': 'en'}, 'grouped_predictions': [{'id': 'unknown', 'predictions': [{'text': 'For model selection, we choose Gb four opinion, mainly because we use Lan to implement the right pipeline and Gp four o mini, had the perfect balance of intelligence and cost effectiveness. And also speed that we had to manage, and this was just to verify the answers. So we did not go for a most sophisticated model such as claude on it, three point five which by all... Which considered the most intelligent element l till now. We did not need such a a high powered L. We just needed a cost effective L to just verify the answer and make surgical strings and four. For Mini. Sorry. Was perfect for the job. Apart from this, for prompt engineering, yes, I had to write several prompts to give the last iraq pipeline to verify the answer. So what would happen is when we converted speech to text from the interview. Some of the text had grammatical errors or typo errors, which is common for, most text translation apps. So to overcome this, I had to prompt the and to specifically loop grammatical errors or to make sense of words that were not properly properly converted, but were close to the actual word that the candidate was trying to explain. So these were the some... These were some of the challenges that I faced.', 'position': {'begin': 0, 'end': 1224}, 'time': {'begin': 0.67829996, 'end': 97.533}, 'confidence': None, 'speaker_confidence': None, 'emotions': [{'name': 'Admiration', 'score': 0.006040629930794239}, {'name': 'Adoration', 'score': 0.0008832465973682702}, {'name': 'Aesthetic Appreciation', 'score': 0.014337360858917236}, {'name': 'Amusement', 'score': 0.027520691975951195}, {'name': 'Anger', 'score': 0.012150214053690434}, {'name': 'Annoyance', 'score': 0.1695852279663086}, {'name': 'Anxiety', 'score': 0.004828206729143858}, {'name': 'Awe', 'score': 0.0027052657678723335}, {'name': 'Awkwardness', 'score': 0.01664806343615055}, {'name': 'Boredom', 'score': 0.02688404731452465}, {'name': 'Calmness', 'score': 0.06000637635588646}, {'name': 'Concentration', 'score': 0.4357732832431793}, {'name': 'Confusion', 'score': 0.17373624444007874}, {'name': 'Contemplation', 'score': 0.26360902190208435}, {'name': 'Contempt', 'score': 0.08602551370859146}, {'name': 'Contentment', 'score': 0.016172541305422783}, {'name': 'Craving', 'score': 0.0009966546203941107}, {'name': 'Desire', 'score': 0.000548546202480793}, {'name': 'Determination', 'score': 0.2924180328845978}, {'name': 'Disappointment', 'score': 0.05303318053483963}, {'name': 'Disapproval', 'score': 0.11453741043806076}, {'name': 'Disgust', 'score': 0.008890906348824501}, {'name': 'Distress', 'score': 0.007310130633413792}, {'name': 'Doubt', 'score': 0.08330558985471725}, {'name': 'Ecstasy', 'score': 0.0004945023683831096}, {'name': 'Embarrassment', 'score': 0.0048986272886395454}, {'name': 'Empathic Pain', 'score': 0.0027902228757739067}, {'name': 'Enthusiasm', 'score': 0.0521029531955719}, {'name': 'Entrancement', 'score': 0.008253814652562141}, {'name': 'Envy', 'score': 0.0010283008450642228}, {'name': 'Excitement', 'score': 0.0070776850916445255}, {'name': 'Fear', 'score': 0.0022058424074202776}, {'name': 'Gratitude', 'score': 0.002507929690182209}, {'name': 'Guilt', 'score': 0.001834267401136458}, {'name': 'Horror', 'score': 0.0007345890044234693}, {'name': 'Interest', 'score': 0.16865162551403046}, {'name': 'Joy', 'score': 0.0018692347221076488}, {'name': 'Love', 'score': 0.00041259374120272696}, {'name': 'Nostalgia', 'score': 0.004078308120369911}, {'name': 'Pain', 'score': 0.0014112676726654172}, {'name': 'Pride', 'score': 0.029000241309404373}, {'name': 'Realization', 'score': 0.11491047590970993}, {'name': 'Relief', 'score': 0.002135586692020297}, {'name': 'Romance', 'score': 0.00020485413551796228}, {'name': 'Sadness', 'score': 0.0020301672630012035}, {'name': 'Sarcasm', 'score': 0.05700303241610527}, {'name': 'Satisfaction', 'score': 0.03777981176972389}, {'name': 'Shame', 'score': 0.005828468594700098}, {'name': 'Surprise (negative)', 'score': 0.01781364157795906}, {'name': 'Surprise (positive)', 'score': 0.006903736852109432}, {'name': 'Sympathy', 'score': 0.0027478619012981653}, {'name': 'Tiredness', 'score': 0.011161400005221367}, {'name': 'Triumph', 'score': 0.04218485951423645}], 'sentiment': [{'name': '1', 'score': 0.07941222190856934}, {'name': '2', 'score': 0.2081184983253479}, {'name': '3', 'score': 0.21649974584579468}, {'name': '4', 'score': 0.20860977470874786}, {'name': '5', 'score': 0.19275544583797455}, {'name': '6', 'score': 0.041113562881946564}, {'name': '7', 'score': 0.03723526746034622}, {'name': '8', 'score': 0.019725065678358078}, {'name': '9', 'score': 0.008710280060768127}], 'toxicity': [{'name': 'identity_hate', 'score': 0.0032493839971721172}, {'name': 'insult', 'score': 0.0020311397965997458}, {'name': 'obscene', 'score': 0.0018525373889133334}, {'name': 'severe_toxic', 'score': 0.0022576849441975355}, {'name': 'threat', 'score': 0.002867637202143669}, {'name': 'toxic', 'score': 0.0053838323801755905}]}]}]}}}], 'errors': []}}]\n",
      "Error analyzing audio: Failed to process predictions\n",
      "Error processing file data\\interviews\\1724629705\\audio\\audio_4_1724629705.wav: Failed to process predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\debo1\\AppData\\Local\\Temp\\ipykernel_27980\\1867170296.py\", line 27, in process_sentiments\n",
      "    result = sentiment_analyser.analyze_audio(full_file_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 77, in analyze_audio\n",
      "    return self._process_predictions(predictions)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\humewrapper.py\", line 94, in _process_predictions\n",
      "    raise Exception(\"Failed to process predictions\")\n",
      "Exception: Failed to process predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to generate searc queries from answers\n",
      "search queries generated for answer to question: Hello! Thank you for taking the time to speak with me today about the Entry-Level RAG AI Engineer role. I'd like to start by asking you a few questions about your experience and skills. Could you tell me about any projects you've worked on involving retrieval-augmented generation (RAG) pipelines?\n",
      "\n",
      "queries are: ['1. \"Retrieval-augmented generation (RAG) pipeline for accuracy verification in automated screening interview agent\"', '2. \"Implementing retrieval-augmented generation for accuracy verification in natural language processing projects\"']\n",
      "\n",
      "\n",
      "Environment setup error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m chatlog_chat \u001b[38;5;241m=\u001b[39m process_sentiments(chatlog_chat, timestamp) \u001b[38;5;66;03m# Analyze sentiments\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#chatlog_chat_copy = chatlog_chat[1]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchatlog_chat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Get the final evaluation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m chatlog_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/interviews/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/outcome/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(chatlog_file_path):\n",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m, in \u001b[0;36mevaluate_candidate\u001b[1;34m(chatlog_chat)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_candidate\u001b[39m(chatlog_chat):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mConversationVerifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_qa_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchatlog_chat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Feedback JSON from the sentiment analyser and accuracy verifier: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     pprint(chatlog_chat)\n",
      "File \u001b[1;32md:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\modules\\ConversationVerifier.py:67\u001b[0m, in \u001b[0;36mprocess_qa_pair\u001b[1;34m(chat_log)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m     66\u001b[0m         scraper \u001b[38;5;241m=\u001b[39m WebScraper(query, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m         documents\u001b[38;5;241m.\u001b[39mextend(\u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scraped_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Process and store documents\u001b[39;00m\n\u001b[0;32m     69\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter\u001b[38;5;241m.\u001b[39mfrom_tiktoken_encoder(\n\u001b[0;32m     70\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, \n\u001b[0;32m     71\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     72\u001b[0m )\n",
      "File \u001b[1;32md:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\webscraper.py:61\u001b[0m, in \u001b[0;36mWebScraper.get_scraped_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_scraped_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Get the scraped data from the search results.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m        list: A list of documents containing the scraped content.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_and_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Kent\\University Of Kent UK\\Projects\\Disso\\Screening-LLM\\src\\utils\\webscraper.py:34\u001b[0m, in \u001b[0;36mWebScraper.search_and_scrape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m search_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com/search?q=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m links \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myuRUbf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\debo1\\miniconda3\\envs\\disso\\Lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Flow ---\n",
    "for i in range(9):\n",
    "    print(f'---------------------------{timestamp}-{i}------------------------')\n",
    "    chatlog_chat = reformat_chatlog(chatlog)  # Process the loaded chatlog\n",
    "\n",
    "    chatlog_chat = process_sentiments(chatlog_chat, timestamp) # Analyze sentiments\n",
    "\n",
    "    #chatlog_chat_copy = chatlog_chat[1]\n",
    "    evaluation = evaluate_candidate(chatlog_chat) # Get the final evaluation\n",
    "\n",
    "    \n",
    "    chatlog_file_path = f\"data/interviews/{timestamp}-{i}/outcome/\"\n",
    "\n",
    "    if not os.path.exists(chatlog_file_path):\n",
    "        os.makedirs(chatlog_file_path)\n",
    "\n",
    "    with open(chatlog_file_path+\"chatlog.json\", \"w\") as file:\n",
    "        json.dump(chatlog_chat, file, indent=4)\n",
    "    evaluation_file_path = f\"data/interviews/{timestamp}-{i}/outcome/\"\n",
    "    if not os.path.exists(chatlog_file_path):\n",
    "        os.makedirs(chatlog_file_path)\n",
    "    with open(evaluation_file_path+\"evaluation.txt\", \"w\") as file:\n",
    "        file.write(str(evaluation).replace(\"\\\\n\", \"\\n\"))\n",
    "    print(f'---------------------------{timestamp}-{i}------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
